{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e395c1-d2b9-44b7-9dc3-9c4e9fd3ab33",
   "metadata": {},
   "source": [
    "# Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    "# Date; 3. 7. 2022 - 3. . 2022\n",
    "# Title: Korean_NER\n",
    "# Professor: Seung-Hoon Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a98df6f6-cbfe-41bc-88fb-829834b90249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, FlaubertModel\n",
    "# from transformers import BertModel\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73d75b32-3eb3-4ec9-9628-8dc8814d2586",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DataPreprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-fd58cda20ae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mDataPreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData_NER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mner_collate_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodeling_ner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBert_NER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'DataPreprocessing'"
     ]
    }
   ],
   "source": [
    "from DataPreprocessing import Data_NER, ner_collate_fn\n",
    "from modeling_ner import Bert_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f36c5933-32b5-40e5-a3c2-233c89f2e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_collate_fn import collate_fn_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "52d3b709-febb-4ebe-8eac-5852a887f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "bert = BertModel.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "# flaubert = FlaubertModel.from_pretrained(language_model_dir, output_loading_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dfe44-0ada-4b96-bbb2-0c9efccb6647",
   "metadata": {},
   "source": [
    "# BertTokenizer\n",
    " - return tensors = \"pt\" : Finally, you want the tokenizer to return the actual tensors that are fed to the model.\n",
    " \n",
    "# BertModel\n",
    " - Attention_mask : 1 where you care and 0 where you don't care.\n",
    " - Input_ids : the IDs of the sentence morpheme.\n",
    " - Token_type_ids : for the question problem, but it's enough to set it to zero now.\n",
    "\n",
    "reference: https://huggingface.co/docs/transformers/model_doc/bert\n",
    "\n",
    "# Example"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf314a1e-8ad7-48d1-ab93-cce57fed0941",
   "metadata": {},
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "=> {'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a12bf3-7745-4de2-a44b-edbf46b7f2fa",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b6e865-968d-4e76-a18e-3eac832a8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_dir = 'C:\\\\Users\\\\LG\\\\Desktop\\\\github\\\\JBNU-2022-SPRING\\\\English world class tagging & Korean_Named Entity Recognition\\\\Ko_En_NER_POStag_data\\Ko_NER_POS'\n",
    "PATH_ko_train = os.path.join(PATH_dir, 'prepro_train.json')\n",
    "PATH_ko_test = os.path.join(PATH_dir, 'prepro_test.json')\n",
    "PATH_ko_dev = os.path.join(PATH_dir, 'prepro_dev.json')\n",
    "total_tag = os.path.join(PATH_dir, 'total_tag.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3365558a-01cf-4c1c-a200-5f7771c31422",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_ko_train) as f: \n",
    "    dataset_train = json.load(f)\n",
    "with open(PATH_ko_test) as f: \n",
    "    dataset_test = json.load(f)\n",
    "with open(PATH_ko_dev) as f: \n",
    "    dataset_dev = json.load(f)\n",
    "with open(total_tag, 'r') as f:\n",
    "    tag_converter = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4112652e-ac9d-4c17-b3f0-cfb31324658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4250\n",
      "test 500\n",
      "dev 250\n"
     ]
    }
   ],
   "source": [
    "print('train', len(dataset_train))\n",
    "print('test', len(dataset_test))\n",
    "print('dev', len(dataset_dev))\n",
    "# tag_converter.id_to_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f4ea3-a170-4249-b309-b3aa9c3dc2d6",
   "metadata": {},
   "source": [
    "# collate_fn\n",
    " - Tie it in batches.\n",
    " - If the data set is variable length, it cannot be tied right away and causes an error, so you have to make a 'collate_fn' and hand it over.\n",
    "  - partial_collate_fn: when collating, with padding\n",
    "  \n",
    "  \n",
    "  - reference: https://hulk89.github.io/pytorch/2019/09/30/pytorch_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8b042-b1e0-474e-aa10-ee12568bc320",
   "metadata": {},
   "source": [
    "# collate_fn_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb6233db-6280-41c5-9345-3eaa1b17e6af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn_custom(data):\n",
    "    \n",
    "    input_sent = [sample[0] for sample in data]\n",
    "    labels = [sample[2] for sample in data]\n",
    "    \n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(input_sent, batch_first = True)\n",
    "    \n",
    "    return {'input': padded_inputs.contiguous(),\n",
    "            'label': torch.stack(labels).contiguous()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff02d2-d3c2-417c-9d08-e253ceed5a8a",
   "metadata": {},
   "source": [
    "# 0309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2a00e8ec-d795-4ba5-871b-a8d681bc4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_collate_fn import make_same_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2255ce8e-be7a-4337-88e8-4ae560ca68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_fn_custom(tokenizer, data):\n",
    "    \n",
    "    input_sent = [sample[0] for sample in data]\n",
    "    labels = [sample[2] for sample in data]\n",
    "\n",
    "    batch_inputs = tokenizer(input_sent, padding = True, return_tensors = \"pt\")\n",
    "    batch_labels = make_same_len(labels)\n",
    "#     batch_labels = torch.tensor(batch_labels)\n",
    "    \n",
    "    return batch_inputs, batch_labels\n",
    "#     return {'input': padded_inputs.contiguous(),\n",
    "#             'label': torch.stack(labels).contiguous()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d461a3b3-1dc0-4dd7-894c-a2cf31b96e0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-163-f11b6230085a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-163-f11b6230085a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    special_token =\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def make_batch(labels, max_len):\n",
    "    batch_labels = []\n",
    "    special_token = 0\n",
    "    \n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "55721e36-2af3-478f-8b6d-593a84655a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2932509f-7d91-43f4-ba54-5785f496c040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef ner_collate_fn(tokenizer, tag_converter):\\n    \\n'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "# partial (func, /, *args, **keywords)  positional argument: args, keyword argument: keywords\n",
    "\n",
    "# partial_collate_fn = partial(collate_fn_custom, tokenizer, tag_converter)\n",
    "# partial_collate_fn = collate_fn_custom\n",
    "\n",
    "partial_collate_fn = partial(ner_fn_custom, tokenizer)\n",
    "\n",
    "'''\n",
    "partial_collate_fn = partial(collate_fn_custom, tokenizer, tag_converter)\n",
    "'''\n",
    "# ner_collate_fn: padding & making batch?\n",
    "'''\n",
    "def ner_collate_fn(tokenizer, tag_converter):\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ed95e478-6ddd-4512-84f8-036ed62b35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = partial_collate_fn)\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = partial_collate_fn)\n",
    "\n",
    "dataloader_dev = DataLoader(\n",
    "    dataset_dev,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = ner_fn_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5e99fe90-6403-41fb-a7d8-539b2314f73f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ner_fn_custom() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-8e9b06a14b66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader_dev\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ner_fn_custom() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "for data in dataloader_dev:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a89c8-a08a-40da-9a23-3a814c65fa91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT_NER\n",
    " - Dropout: method to solve overfitting, one of the problems in deep learning learning. In summary, how to prevent some units of the hidden layer from being overfitted by inoperative<br>\n",
    "    ->the outputs are scaled by a factor of (1 / 1-p)  during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da42c48-8d2f-42cb-8d70-f9c32077fb1d",
   "metadata": {},
   "source": [
    "# *args, **kargs\n",
    " - *args(non-keworded arguments): Tuple type of argument without limitation.\n",
    " - **kargs(keworded arguments):  Dictionary type of argument without limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "dfdf5655-7206-475d-9dbd-3c0ac227023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_NER(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super().__init__()        \n",
    "        self.bert = bert\n",
    "        self.dorpout = nn.Dropout(p = 0.1)\n",
    "        self.linear = nn.Linear(768, 22)\n",
    "        self.softmax = nn.Softmax(dim = 2) # A dimension along which Softmax will be computed\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        emb = self.bert(**kwargs)\n",
    "        e = self.dropout(emb['last_hidden_state'])\n",
    "        w = self.linear(e)\n",
    "        \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fa1e6-1e9b-4afa-9df6-e4e3cd100529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6ee569ee-f610-4c04-be87-923a3b1f2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_id ={'[PAD]': 0, 'B-<휠': 1, 'B-OG': 2, 'I-조선': 3, 'I-PS': 4, 'B-LC': 5, 'B-1': 6, 'I-<휠': 7, 'I-LC': 8, 'B-PS': 9, 'I-TI': 10, \n",
    "            'B-목소': 11, 'O': 12, 'I-목소': 13, 'I-1': 14, 'I-': 15, 'B-조선': 16, 'I-OG': 17, 'B-': 18, 'I-DT': 19, 'B-TI': 20, 'B-DT': 21}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "2fe1b308-e874-4644-8498-53dc50e36057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "61f62ca1-ab23-4861-b3ab-7d36a59fca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_num = len(tag_to_id) # 22\n",
    "model = Bert_NER(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "1bdc5c0e-b048-44bd-ad93-408f2d940e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELoss = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = AdamW(model.parameters(), lr = 1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "1262c91b-d4c2-48d8-8eb7-b07d71349b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.cuda(6)\n",
    "# device = model.bert.device\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7832cd08-decd-42da-8dec-d97061fcdd07",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "for iteration, batch in enumerate(dataloader_train):\n",
    "    batch_inputs = {k: v for k, v in list (batch[0].items())}\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    print('='*33,'batch_inputs','='*33)\n",
    "    print(batch_inputs)\n",
    "    print('='*33,'labels','='*33)\n",
    "    print(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9502c-dae2-4389-8ff2-e44dcd280f23",
   "metadata": {},
   "source": [
    "# START!~\n",
    "\n",
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b464a17d-1538-4293-9abd-ac0cbdfdd738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each_len_list [27]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-315-d0b86dbd8294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#         loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-311-a13ffc10bb7e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_hidden_state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # 숫자 ->train_epoch\n",
    "    model.train()\n",
    "    \n",
    "    for iteration, batch in enumerate(dataloader_train):\n",
    "        batch_inputs = {k: v for k, v in list (batch[0].items())}\n",
    "        batch_labels = batch[1]\n",
    "\n",
    "        output = model(**batch_inputs)\n",
    "        print(output)\n",
    "        #         loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (iteration + 1) % 10 == 0:\n",
    "#             print(f'{iteration:3} - loss: {loss.item()}')\n",
    "\n",
    "#     # todo 매 에포크가 끝나면 dev 데이터로 성능 비교하기\n",
    "#     # Early Stopping 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "0c6ada26-f6ab-4b8a-ae8d-e2c1f11e75cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each_len_list [83]\n",
      "tensor([[[-0.2132,  0.0465, -0.0252,  ..., -0.5054, -0.3538,  0.1319],\n",
      "         [ 0.1933,  0.0557,  0.1594,  ...,  0.2523,  0.1222, -0.1717],\n",
      "         [ 0.1447,  0.3433,  0.3282,  ..., -0.2841, -0.0255,  0.1288],\n",
      "         ...,\n",
      "         [ 0.0983, -0.0163,  0.0341,  ..., -0.2113, -0.3903,  0.1640],\n",
      "         [ 0.3565,  0.0075,  0.0812,  ..., -0.1527, -0.2726,  0.0307],\n",
      "         [ 0.0881,  0.0353,  0.0372,  ...,  0.1172, -0.1950,  0.1636]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [14]\n",
      "tensor([[[-0.3674, -0.2836,  0.0923, -0.1114,  0.0924,  0.2022,  0.2518,\n",
      "          -0.1406, -0.3531,  0.0750,  0.3285, -0.0536,  0.3797, -0.0484,\n",
      "           0.5099, -0.1962, -0.2210, -0.2339, -0.1866,  0.1348, -0.2580,\n",
      "           0.3822],\n",
      "         [ 0.0033,  0.3397,  0.2714, -0.0169,  0.0635, -0.2374, -0.0345,\n",
      "          -0.3715, -0.0262,  0.1878,  0.3272,  0.3728,  0.3978,  0.1897,\n",
      "          -0.1752,  0.0014,  0.2955, -0.3374,  0.5311,  0.3953,  0.3317,\n",
      "           0.4161],\n",
      "         [ 0.0463,  0.3168,  0.3456,  0.1797, -0.2091,  0.2537, -0.2503,\n",
      "          -0.2943, -0.0671,  0.1963, -0.1201,  0.0272,  0.4432, -0.1836,\n",
      "          -0.1853,  0.0259, -0.1097,  0.3313,  0.4404,  0.1548,  0.0694,\n",
      "           0.1857],\n",
      "         [ 0.1513,  0.0180, -0.0619,  0.1030, -0.3971,  0.1700, -0.4395,\n",
      "          -0.0663,  0.0850,  0.4460, -0.0886, -0.2597,  0.0886, -0.0414,\n",
      "          -0.4514, -0.2088, -0.2877,  0.4193,  0.5797, -0.2362,  0.1209,\n",
      "           0.0498],\n",
      "         [ 0.2522, -0.1838,  0.5267,  0.5802,  0.3686, -0.1322, -0.1993,\n",
      "           0.0296,  0.4361,  0.4840, -0.1969,  0.2335,  0.1480,  0.2501,\n",
      "          -0.0304, -0.2152,  0.2555,  0.3988,  0.2531, -0.5577,  0.2842,\n",
      "          -0.0428],\n",
      "         [ 0.2264, -0.2216, -0.0121,  0.4853,  0.2864, -0.3109,  0.1529,\n",
      "          -0.0100,  0.0988,  0.6982, -0.3257, -0.2666,  0.1069,  0.2444,\n",
      "          -0.1492, -0.5505,  0.1240,  0.0587, -0.0495, -0.6025,  0.2813,\n",
      "           0.3005],\n",
      "         [ 0.3225, -0.0490, -0.1559,  0.0845,  0.1192,  0.0196,  0.4170,\n",
      "          -0.0274,  0.2306,  0.4994, -0.2493,  0.2124,  0.2211,  0.0359,\n",
      "          -0.3286,  0.0242, -0.3038,  0.2789, -0.0828, -0.1804,  0.3473,\n",
      "          -0.1324],\n",
      "         [-0.0864,  0.1555,  0.7729, -0.0839, -0.3163, -0.4382,  0.1780,\n",
      "          -0.0993,  0.1254,  0.1438,  0.4396, -0.0017,  0.0350,  0.5989,\n",
      "          -0.3577, -0.1264,  0.1369,  0.4919, -0.2085, -0.0619, -0.2172,\n",
      "          -0.0816],\n",
      "         [ 0.1769,  0.1438,  0.4835, -0.2167,  0.3969, -0.2873,  0.4755,\n",
      "          -0.3374, -0.1904,  0.1395,  0.2423,  0.1102,  0.0580,  0.5789,\n",
      "          -0.2391, -0.1735,  0.1149,  0.4103,  0.1968,  0.1415, -0.0394,\n",
      "           0.3648],\n",
      "         [ 0.2894, -0.1326,  0.0581, -0.2081,  0.2049, -0.0575, -0.1279,\n",
      "           0.0700, -0.1690,  0.6597, -0.1240,  0.1832,  0.1215, -0.0526,\n",
      "          -0.1231,  0.2284, -0.0265,  0.5283,  0.6032,  0.4144,  0.2879,\n",
      "           0.1106],\n",
      "         [ 0.1174, -0.1298,  0.3311, -0.2527,  0.4881,  0.0838,  0.4251,\n",
      "           0.1956, -0.1446,  0.3659, -0.0589,  0.3073, -0.0506,  0.1506,\n",
      "          -0.3738, -0.5438,  0.0224,  0.5272,  0.5954,  0.1187,  0.0341,\n",
      "           0.2272],\n",
      "         [ 0.1483,  0.0072,  0.4407, -0.1301,  0.3913,  0.0341,  0.3472,\n",
      "          -0.2006,  0.2192,  0.3787,  0.2083,  0.0859,  0.3074,  0.5836,\n",
      "          -0.6568, -0.5347,  0.1572,  0.2057,  0.2071,  0.0461, -0.0876,\n",
      "           0.3623],\n",
      "         [-0.0208,  0.2278,  0.2001, -0.0388,  0.1566,  0.4287, -0.2944,\n",
      "           0.1449,  0.1800,  0.3299,  0.1735,  0.0767,  0.2140,  0.3544,\n",
      "          -0.0732, -0.1173, -0.0319,  0.4025,  0.1906,  0.1990, -0.3042,\n",
      "           0.2121],\n",
      "         [-0.0091, -0.0355,  0.0858,  0.0790,  0.3167, -0.3511,  0.2457,\n",
      "          -0.1373,  0.2657,  0.5675, -0.0578,  0.0929, -0.1398,  0.1885,\n",
      "          -0.2507, -0.5870,  0.2456,  0.3896, -0.1066,  0.4605, -0.2187,\n",
      "           0.2364],\n",
      "         [ 0.0054,  0.0983, -0.0350, -0.0612,  0.0343,  0.3310,  0.0615,\n",
      "          -0.1505,  0.0377,  0.1746,  0.1677,  0.1161,  0.3209,  0.1153,\n",
      "          -0.4913, -0.2396, -0.0655,  0.1405, -0.0134,  0.1488, -0.2012,\n",
      "           0.0484],\n",
      "         [-0.0816,  0.1023,  0.2246, -0.0050,  0.0931, -0.0965,  0.1379,\n",
      "          -0.1721,  0.1454,  0.3172,  0.1287,  0.0570,  0.2540,  0.2664,\n",
      "          -0.4222, -0.3173,  0.1229,  0.2051,  0.1806,  0.2739, -0.0449,\n",
      "           0.2830]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [93]\n",
      "tensor([[[-0.0369, -0.0829,  0.3604,  ...,  0.0082,  0.5177, -0.0686],\n",
      "         [ 0.0439, -0.2865,  0.1406,  ...,  0.3290,  0.3456,  0.0805],\n",
      "         [-0.1546, -0.4132,  0.3477,  ...,  0.4847,  0.1604, -0.3442],\n",
      "         ...,\n",
      "         [ 0.5746,  0.1066,  0.2197,  ...,  0.3242,  0.1476, -0.1231],\n",
      "         [ 0.5288,  0.3134, -0.0440,  ...,  0.6948, -0.0165,  0.2325],\n",
      "         [ 0.4369,  0.4205,  0.1155,  ...,  0.5092,  0.1658, -0.0666]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [98]\n",
      "tensor([[[-0.2290, -0.0541,  0.0125,  ..., -0.0916,  0.4759, -0.1048],\n",
      "         [-0.1266, -0.0642, -0.0688,  ..., -0.3774,  0.4105,  0.1120],\n",
      "         [ 0.0150,  0.4257,  0.2101,  ..., -0.1779,  0.8023, -0.4246],\n",
      "         ...,\n",
      "         [ 0.1644, -0.0413, -0.0169,  ..., -0.4556,  0.6801, -0.2555],\n",
      "         [-0.0262,  0.0707, -0.3260,  ..., -0.1953,  0.2781, -0.0138],\n",
      "         [ 0.0883,  0.0903,  0.0786,  ..., -0.2750,  0.6563, -0.1817]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [15]\n",
      "tensor([[[-1.1205e-01, -7.6200e-02, -1.3603e-01, -8.8959e-02,  3.1675e-01,\n",
      "           3.6013e-01, -3.1149e-01,  1.1793e-01,  2.3856e-02, -8.9170e-02,\n",
      "           1.2648e-01, -5.9468e-03, -5.1441e-01,  1.3434e-01,  1.0895e-01,\n",
      "          -3.3353e-03,  2.7613e-01,  4.8309e-01,  2.0062e-02, -2.4269e-01,\n",
      "           1.1273e-01, -1.6997e-01],\n",
      "         [-3.9740e-01, -3.6022e-01,  3.2953e-01,  2.9078e-01,  2.4614e-01,\n",
      "          -1.9302e-01, -3.8063e-02, -1.7111e-01,  1.1750e-01,  5.8162e-01,\n",
      "          -3.9544e-02, -1.2280e-01, -2.1663e-01,  3.6815e-01,  3.0599e-01,\n",
      "          -6.2414e-02,  4.4341e-01,  3.1378e-01,  2.6835e-01,  3.5109e-01,\n",
      "           1.5096e-01, -2.3564e-01],\n",
      "         [-1.6718e-01, -4.7746e-01,  3.0615e-01,  5.4825e-02, -3.7354e-01,\n",
      "          -2.2652e-01,  1.3439e-01, -1.7995e-01, -3.7374e-01,  2.1821e-01,\n",
      "           1.2650e-02,  7.1607e-02, -2.4487e-01,  4.2528e-02,  4.9694e-01,\n",
      "          -5.3339e-02,  7.2440e-01,  4.0168e-01,  3.9079e-01,  1.7485e-01,\n",
      "          -1.0998e-01,  5.9294e-01],\n",
      "         [ 1.7431e-01, -4.4144e-01,  2.3557e-01,  3.7026e-01,  3.2319e-01,\n",
      "          -1.0646e-01, -3.2520e-01, -5.9493e-02, -1.6836e-01,  3.6048e-01,\n",
      "          -7.8288e-01, -2.5885e-01, -1.1550e-01, -1.8171e-01,  3.1886e-01,\n",
      "          -1.3541e-01,  6.1253e-01,  2.2970e-01,  1.0307e-01,  7.9503e-02,\n",
      "           1.5754e-01,  2.3910e-01],\n",
      "         [ 1.0569e-01, -6.1726e-01,  3.0834e-01, -5.9131e-02,  5.2201e-02,\n",
      "           4.0424e-01, -3.5604e-02, -5.9157e-02, -2.7230e-01,  6.0739e-01,\n",
      "          -5.7246e-01,  6.6255e-02, -3.5524e-01,  2.1821e-01, -1.7656e-02,\n",
      "          -5.2457e-02,  5.0850e-01,  4.2556e-01, -4.8056e-01,  2.7710e-01,\n",
      "          -4.5881e-01,  2.9500e-01],\n",
      "         [ 3.9719e-03, -4.7330e-01,  9.2226e-02, -1.0467e-01, -2.4398e-01,\n",
      "           2.0834e-01, -8.4197e-02, -3.0978e-01,  5.0587e-02,  4.9137e-01,\n",
      "          -2.5840e-01,  3.9452e-01,  1.8252e-01,  1.8668e-01,  2.0810e-01,\n",
      "           2.1506e-01,  8.9531e-01,  3.9796e-01, -7.1194e-01,  3.9958e-02,\n",
      "          -3.1421e-02,  3.1528e-01],\n",
      "         [-4.0656e-01, -2.9710e-01,  2.5678e-01, -2.4063e-01,  4.2310e-02,\n",
      "           1.3962e-02, -4.5820e-02, -3.0424e-01, -3.2921e-01,  5.9876e-01,\n",
      "          -4.3268e-01,  1.5983e-01, -1.5257e-01,  4.1876e-01,  8.4718e-02,\n",
      "          -2.5274e-01,  9.7938e-01,  2.5518e-01, -5.0624e-01,  1.3414e-01,\n",
      "           3.8698e-02,  1.0940e-01],\n",
      "         [-4.9585e-02, -3.7738e-01,  1.1185e-01, -1.0312e-01,  2.7604e-01,\n",
      "          -3.3212e-01, -1.5428e-01, -2.5649e-02, -4.3813e-01,  1.0296e-01,\n",
      "          -5.1624e-01,  1.6110e-01, -3.7802e-01,  1.2605e-01,  3.2636e-01,\n",
      "          -3.7774e-02,  7.2969e-01,  5.3863e-01, -2.9221e-01,  3.9634e-01,\n",
      "           2.3945e-01,  2.0729e-01],\n",
      "         [-2.7697e-02, -3.9629e-01, -2.0892e-01, -2.4687e-02,  7.0598e-02,\n",
      "          -1.1375e-01, -6.7490e-02, -3.9719e-02, -4.6890e-01,  5.9778e-01,\n",
      "          -4.6258e-01,  1.8209e-02,  9.5736e-02, -4.6065e-02,  4.9987e-02,\n",
      "          -5.3236e-01,  4.3096e-01,  4.6688e-01, -5.4689e-01, -4.5233e-02,\n",
      "           2.0007e-02, -9.4968e-02],\n",
      "         [-3.4431e-01, -8.0031e-01, -2.9173e-02,  3.7401e-01,  2.1019e-01,\n",
      "           1.2512e-01, -3.7488e-01,  2.0746e-01, -2.7509e-01,  5.4295e-01,\n",
      "          -1.8180e-02,  2.3398e-02,  1.9288e-02, -4.8595e-02,  5.9230e-01,\n",
      "          -4.8036e-03,  7.6423e-01,  4.5679e-01, -6.8888e-01,  3.4149e-01,\n",
      "           6.3846e-02,  1.9298e-01],\n",
      "         [-2.0243e-01, -9.5421e-01, -9.6753e-02,  3.9639e-01, -7.2627e-02,\n",
      "           1.5550e-01, -3.7437e-01,  1.7430e-01, -5.2390e-01,  3.9447e-01,\n",
      "           1.2154e-01,  4.3793e-01,  9.0979e-02,  1.6936e-01,  1.1692e+00,\n",
      "           1.9908e-01,  7.2735e-01,  4.8248e-02, -3.8938e-01,  7.5561e-01,\n",
      "           7.8564e-02,  5.0051e-01],\n",
      "         [-1.7593e-01, -1.5420e-01, -8.5570e-02,  1.0151e-01,  6.0030e-03,\n",
      "          -3.0647e-01, -1.1528e-01,  4.8468e-01,  1.1230e-01,  3.1809e-01,\n",
      "           1.1616e-01,  5.3062e-03,  4.1260e-01,  4.4310e-01,  5.6811e-01,\n",
      "          -3.0107e-01,  2.4657e-01,  1.8592e-01,  1.1800e-01,  1.1178e-01,\n",
      "           4.2793e-01,  2.1061e-02],\n",
      "         [-3.4052e-01, -7.6531e-01, -3.0211e-01,  2.3359e-01, -1.5023e-02,\n",
      "           1.8520e-01, -2.6077e-01,  2.9137e-01, -4.6885e-01,  4.5969e-01,\n",
      "          -5.9193e-02,  7.7807e-02, -3.5955e-03,  4.4908e-01, -1.9642e-02,\n",
      "           4.3815e-01,  6.2172e-01,  7.6739e-01, -2.8616e-01, -1.9026e-01,\n",
      "           5.3494e-02, -4.5422e-01],\n",
      "         [-3.7148e-01, -3.4245e-01, -3.3311e-01,  2.8218e-01,  2.8622e-01,\n",
      "           3.5554e-01, -5.2520e-02, -1.7648e-01,  1.1709e-02,  5.9507e-04,\n",
      "           1.1594e-01,  1.3805e-01,  1.4509e-01,  1.5196e-01,  4.1040e-01,\n",
      "           6.0994e-01,  3.8909e-01,  8.1903e-01, -1.1481e-01,  1.0300e-01,\n",
      "           9.7294e-02, -2.6541e-01],\n",
      "         [-3.8706e-01, -4.0177e-01, -2.0691e-01, -1.6427e-01,  5.3800e-01,\n",
      "          -8.9926e-03, -3.1464e-01,  1.1658e-01,  1.7164e-01,  6.6872e-01,\n",
      "          -3.5013e-01,  3.4469e-01,  2.0068e-01,  1.1367e-01, -4.0394e-02,\n",
      "          -1.7636e-03,  2.6980e-01,  5.0094e-01, -4.2055e-01, -3.3801e-01,\n",
      "          -5.0878e-02, -5.3704e-01],\n",
      "         [-1.7479e-01, -2.6945e-01, -2.6896e-01,  3.1524e-01,  3.1212e-01,\n",
      "           4.5852e-02, -2.1835e-02,  1.1273e-01,  7.2727e-03,  1.7509e-01,\n",
      "           1.0886e-01, -1.3464e-01,  3.1405e-02,  1.5456e-01, -1.3978e-01,\n",
      "           2.2456e-01,  6.2601e-01,  4.2577e-01, -2.3844e-01, -3.0113e-01,\n",
      "           9.6951e-02,  5.6943e-02],\n",
      "         [-3.4894e-01, -2.2200e-01, -3.2477e-01,  3.5924e-01, -1.4313e-02,\n",
      "          -3.2999e-02, -9.8845e-02,  2.1110e-01,  3.5905e-02,  1.3362e-01,\n",
      "          -2.7770e-01, -1.1754e-01,  6.6261e-02,  8.3600e-02,  7.8325e-02,\n",
      "          -7.3060e-02,  4.1367e-01,  5.4741e-01,  3.8463e-02, -2.5719e-01,\n",
      "           3.8488e-02, -1.4857e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [107]\n",
      "tensor([[[ 2.1790e-01,  2.2854e-01,  1.3420e-01,  ..., -1.5073e-01,\n",
      "          -6.9582e-02, -3.2097e-01],\n",
      "         [-3.9488e-01, -2.4891e-01, -5.3177e-01,  ...,  5.3169e-03,\n",
      "          -1.0929e-01, -3.2176e-01],\n",
      "         [-5.5849e-01, -3.4150e-01, -2.0531e-01,  ..., -1.6340e-01,\n",
      "          -1.5563e-01, -1.7924e-01],\n",
      "         ...,\n",
      "         [-1.8901e-01,  1.6080e-01, -7.4594e-02,  ..., -3.5955e-01,\n",
      "          -2.9734e-02,  1.4537e-01],\n",
      "         [-2.2884e-01, -2.9311e-02, -8.3994e-02,  ..., -1.0328e-01,\n",
      "          -1.5311e-01,  4.1300e-01],\n",
      "         [-1.1346e-01, -2.9118e-02,  2.5593e-02,  ...,  3.9171e-02,\n",
      "          -1.0339e-01,  1.2942e-04]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [44]\n",
      "tensor([[[-0.0013,  0.0794,  0.2086,  ..., -0.1691, -0.0514, -0.4065],\n",
      "         [-0.1999,  0.7456, -0.4965,  ..., -0.7961,  0.5791, -0.1715],\n",
      "         [ 0.0742,  0.2989, -0.2843,  ..., -0.5438,  0.3716, -0.1893],\n",
      "         ...,\n",
      "         [-0.2579,  0.4724,  0.0924,  ..., -0.2489,  0.0353, -0.0623],\n",
      "         [-0.0023,  0.5236, -0.0444,  ..., -0.0868,  0.2563,  0.0453],\n",
      "         [-0.0738,  0.2774, -0.0711,  ..., -0.1099,  0.1651,  0.1000]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [33]\n",
      "tensor([[[ 9.0454e-02,  3.2648e-01,  2.5753e-01, -3.2415e-01, -1.0038e-01,\n",
      "           3.5325e-01,  2.0341e-01,  2.8399e-01, -8.8279e-02, -5.1356e-01,\n",
      "           6.0683e-02,  6.8117e-02, -9.8611e-03,  2.8161e-02,  2.6649e-01,\n",
      "          -2.1166e-01,  4.5194e-01, -4.4624e-01, -2.2998e-01, -4.3147e-01,\n",
      "          -2.2213e-01, -2.8612e-01],\n",
      "         [ 3.8960e-01,  1.2487e-01, -5.4768e-02, -1.1826e-01, -3.1769e-01,\n",
      "          -2.7660e-01, -4.7920e-02, -3.5797e-02,  5.1404e-01,  1.2177e-01,\n",
      "          -8.6008e-02,  3.5394e-02, -3.5862e-02, -5.0717e-01,  1.3898e-01,\n",
      "           6.0858e-01, -4.5008e-01,  1.8026e-01,  2.2427e-01, -2.2880e-02,\n",
      "           2.4130e-03, -2.4772e-01],\n",
      "         [ 2.0672e-02,  2.3165e-01, -1.0826e-01, -3.1435e-03, -3.0233e-01,\n",
      "          -8.2624e-02, -4.3897e-02, -3.0604e-01,  5.2982e-01,  2.5141e-01,\n",
      "           8.7244e-02, -1.9913e-01,  4.0817e-01, -5.1074e-01, -7.0373e-02,\n",
      "           3.0198e-01, -2.3200e-01,  1.7625e-02,  1.0897e-01, -2.5798e-01,\n",
      "           7.8402e-02, -2.9239e-01],\n",
      "         [ 2.7603e-01,  1.5470e-01,  6.1136e-02, -2.6408e-01, -4.7881e-01,\n",
      "          -1.4822e-01, -1.4879e-01, -1.4643e-01,  5.4741e-01,  4.2210e-01,\n",
      "          -1.4887e-01, -5.1120e-01, -9.5594e-02, -2.1434e-01,  1.2238e-01,\n",
      "           2.8811e-01,  1.3409e-01,  4.1286e-01,  1.2659e-01, -4.8668e-01,\n",
      "           4.4680e-03, -5.3123e-01],\n",
      "         [ 4.5669e-01,  1.6708e-01,  2.2968e-01, -2.2051e-01, -3.8946e-01,\n",
      "          -7.9356e-02, -7.6359e-02, -2.7565e-01,  3.8185e-01,  3.7890e-01,\n",
      "           2.8756e-01, -3.7772e-01, -2.1154e-01, -1.0540e-01,  4.3594e-01,\n",
      "           3.6267e-01,  3.1663e-01,  9.9901e-02, -3.7501e-02, -3.1952e-01,\n",
      "           3.1138e-01, -7.9210e-02],\n",
      "         [ 3.0839e-01,  2.1737e-01,  6.8042e-03, -3.8625e-01, -4.0280e-01,\n",
      "          -1.7675e-01, -2.0380e-01,  4.4439e-02,  5.6242e-01,  5.1118e-01,\n",
      "          -6.5652e-02, -3.4823e-01,  3.7891e-01, -2.9999e-01,  6.5780e-01,\n",
      "           6.9989e-01,  4.2314e-02, -1.2453e-01,  1.2970e-02,  6.8167e-03,\n",
      "          -2.1467e-01,  2.8810e-01],\n",
      "         [ 3.3002e-01,  4.6164e-01, -2.3112e-01,  1.0825e-01, -4.3193e-01,\n",
      "          -1.4409e-02,  8.0518e-02,  1.8519e-01,  6.8358e-01,  1.4348e-01,\n",
      "           2.8003e-02,  2.3238e-01,  2.8314e-01, -2.6962e-03, -2.3070e-01,\n",
      "           1.7458e-01, -4.5582e-01,  9.0164e-02,  1.1188e-01, -2.6732e-01,\n",
      "           1.7143e-01, -3.8595e-01],\n",
      "         [ 1.1474e-01,  3.5897e-01, -9.5934e-02,  4.9271e-02, -7.8042e-01,\n",
      "          -8.8236e-02,  1.3832e-01,  1.8847e-02,  6.4367e-01,  3.3507e-01,\n",
      "           5.1617e-02,  1.2245e-01,  1.6182e-01, -3.7478e-01,  6.3711e-04,\n",
      "           4.5146e-01, -4.3426e-02,  1.2632e-01, -1.7247e-01, -2.8573e-02,\n",
      "           3.1158e-02, -1.0273e-01],\n",
      "         [ 6.7583e-02,  1.9719e-01, -4.4438e-01, -2.6202e-01, -1.5916e-01,\n",
      "          -1.0536e-01,  1.9449e-01, -5.3704e-02,  7.9947e-01, -9.3168e-02,\n",
      "          -4.5569e-02,  7.4842e-02,  6.3088e-01,  4.5263e-01,  4.2343e-01,\n",
      "           3.5710e-01, -4.1693e-01,  4.4595e-03,  1.1903e-02, -1.0277e-01,\n",
      "          -1.5028e-01,  1.6370e-01],\n",
      "         [ 1.5081e-01, -3.6091e-02, -3.7174e-01, -3.3232e-02, -1.7776e-01,\n",
      "          -2.9094e-01, -5.4404e-02,  1.9706e-03,  4.6746e-01,  1.3623e-01,\n",
      "          -3.2508e-01, -5.2900e-02,  9.3709e-01,  8.2473e-02,  9.6768e-02,\n",
      "           4.9564e-01, -1.2826e-01, -2.7553e-01,  2.0078e-01, -5.5658e-01,\n",
      "           7.3879e-02,  2.9324e-01],\n",
      "         [ 1.6967e-01, -3.6609e-01, -5.3342e-01, -1.7920e-01, -3.0451e-01,\n",
      "           1.1489e-01,  3.4431e-01,  8.5441e-02,  4.3630e-01,  6.9294e-02,\n",
      "           9.2384e-02, -8.7224e-03,  3.3064e-01, -2.2838e-02,  4.6525e-01,\n",
      "           3.5379e-01, -3.8272e-01, -3.1781e-01,  2.9092e-01, -5.3524e-01,\n",
      "          -7.6173e-02, -2.2405e-01],\n",
      "         [ 5.1959e-01, -7.4997e-02, -4.9859e-01,  2.4801e-01, -6.9532e-01,\n",
      "           1.6712e-01, -5.7087e-02, -6.2493e-01,  3.4845e-01, -6.1614e-02,\n",
      "          -1.8845e-01,  1.7689e-02,  5.4980e-02, -1.5047e-01,  2.5833e-01,\n",
      "           2.6624e-01, -2.2047e-01, -2.5227e-01, -3.2725e-02, -3.9915e-01,\n",
      "           8.6134e-02, -5.0990e-02],\n",
      "         [ 1.6197e-01, -2.4881e-01, -1.9861e-01, -2.4209e-01, -6.2627e-01,\n",
      "           2.9143e-01,  3.1760e-01, -3.2350e-01,  3.6964e-01, -6.9703e-02,\n",
      "           3.8666e-01,  1.0759e-01,  1.5622e-01,  1.0384e-02,  6.3021e-01,\n",
      "           1.5573e-01, -2.9521e-02,  4.1080e-01, -4.9547e-02, -2.8288e-01,\n",
      "          -4.3116e-01,  2.4651e-01],\n",
      "         [-5.0751e-02, -1.7561e-02,  9.1379e-02,  1.3510e-01,  1.0075e-01,\n",
      "          -1.5418e-01, -2.6564e-01,  1.2199e-01,  3.5505e-01,  1.1241e-01,\n",
      "           6.0474e-01, -2.1320e-01,  2.1215e-02,  7.7562e-01,  3.3794e-01,\n",
      "           9.5357e-02,  3.3597e-02, -1.2043e-01,  5.7932e-01, -5.4362e-01,\n",
      "          -2.2242e-01, -1.6167e-01],\n",
      "         [ 1.1067e-01,  5.6862e-02, -3.3522e-01, -7.2051e-01, -2.1987e-01,\n",
      "          -4.6440e-01, -3.0538e-01, -2.9638e-01,  2.7706e-01, -1.8900e-01,\n",
      "           4.3550e-01, -4.1629e-01,  6.6770e-01, -6.6982e-02,  1.3712e-01,\n",
      "           5.4159e-01, -2.0547e-01, -1.0957e-01,  6.6529e-01, -7.4724e-01,\n",
      "           1.5659e-01,  5.4778e-03],\n",
      "         [ 5.3950e-02, -8.2128e-01, -2.9303e-02, -4.0283e-01, -7.2729e-02,\n",
      "          -4.1733e-01, -6.8788e-01, -1.5074e-01,  5.8107e-01,  6.3737e-01,\n",
      "           4.3469e-01, -3.1938e-01,  3.2198e-01,  4.8202e-01, -6.3237e-01,\n",
      "           3.6615e-02,  1.5215e-01, -2.6829e-01,  2.2193e-01, -7.1995e-01,\n",
      "           2.1937e-01, -8.1169e-02],\n",
      "         [ 7.6579e-01, -2.2769e-01, -2.6847e-01,  9.1707e-03, -2.6563e-01,\n",
      "          -6.7736e-02, -2.9579e-01, -6.5362e-01, -3.2053e-01,  7.3106e-01,\n",
      "           3.8931e-02,  8.0323e-03,  1.2832e-02,  3.1926e-01, -4.1151e-01,\n",
      "           8.1946e-02, -4.7650e-02, -5.1866e-01,  3.9166e-01, -9.0501e-01,\n",
      "           1.7639e-01, -3.9603e-01],\n",
      "         [ 3.4190e-01, -6.5088e-01, -2.4307e-01,  2.1950e-01,  9.3290e-02,\n",
      "          -3.1105e-01, -5.7754e-01, -5.7153e-01,  2.5133e-01,  4.6526e-01,\n",
      "           3.5723e-01,  1.9152e-01,  2.3720e-01, -2.1138e-01, -2.0826e-01,\n",
      "          -1.7818e-01, -3.6838e-01,  9.8931e-02,  3.2568e-01, -6.9129e-01,\n",
      "          -6.4632e-02, -2.8941e-01],\n",
      "         [ 3.9035e-01, -1.9981e-02, -2.3803e-01,  2.8789e-01, -5.1112e-02,\n",
      "          -3.6365e-01,  7.0951e-02, -4.4257e-01,  6.1599e-01,  2.5486e-01,\n",
      "           2.3270e-01, -4.5303e-01, -8.1871e-02, -4.4738e-01, -5.7768e-01,\n",
      "           4.7506e-01, -1.8022e-01,  1.7921e-01,  5.2408e-01, -6.0314e-01,\n",
      "           2.3971e-01, -1.0554e-01],\n",
      "         [ 1.9844e-01,  8.9991e-02, -1.0787e-01,  2.3461e-01,  2.7331e-01,\n",
      "           1.8221e-01, -1.8147e-01,  3.7281e-02, -1.1472e-01,  4.9275e-01,\n",
      "           7.7167e-01,  7.8735e-02, -1.4494e-01, -5.5676e-01,  4.5650e-01,\n",
      "           3.3418e-01,  3.2286e-01, -2.6876e-01,  2.9001e-01, -5.8342e-02,\n",
      "          -2.1735e-01,  2.9320e-01],\n",
      "         [-3.3458e-02,  6.3833e-01, -8.1483e-02,  2.2137e-01, -4.5704e-01,\n",
      "          -3.4018e-01, -3.2477e-01,  2.7772e-01,  3.9103e-01,  1.8616e-01,\n",
      "           6.1584e-03,  1.9349e-01,  4.9081e-01, -2.5942e-01,  2.6300e-01,\n",
      "           2.6217e-01, -8.3017e-02, -4.0016e-01,  5.3964e-01,  5.0202e-02,\n",
      "          -2.3780e-01,  1.4009e-01],\n",
      "         [ 6.5628e-02,  2.0355e-02, -2.9826e-01, -2.7324e-01, -1.8136e-01,\n",
      "          -1.8799e-01,  1.8462e-01, -1.9161e-01,  4.2393e-01,  3.5247e-01,\n",
      "           3.3459e-01, -7.9341e-02,  4.8278e-01, -1.0209e-01,  3.0792e-01,\n",
      "           5.1639e-01, -3.7168e-01, -1.1070e-01,  2.0044e-01, -4.3638e-01,\n",
      "           3.5088e-01,  2.1118e-01],\n",
      "         [ 6.7239e-02,  2.8558e-01, -2.2508e-01, -1.6189e-02, -5.0924e-01,\n",
      "           1.1214e-02,  4.7603e-02, -4.4860e-03,  2.1761e-01,  4.0062e-01,\n",
      "          -2.4807e-01,  1.1476e-01,  5.8270e-01, -1.0519e-01,  3.6182e-01,\n",
      "           2.3648e-01, -2.3651e-01, -1.1949e-01,  3.1640e-02, -2.0464e-01,\n",
      "           4.1621e-02, -9.6220e-02],\n",
      "         [-9.9642e-02,  6.0486e-01, -9.1605e-02,  4.6338e-01, -7.0797e-01,\n",
      "           6.9695e-02, -3.2270e-01, -3.6926e-01,  6.1366e-01,  5.8549e-01,\n",
      "           3.3868e-01, -4.5151e-02, -1.1660e-01, -5.9774e-01, -8.8529e-02,\n",
      "           5.6294e-01,  9.0451e-02,  5.4770e-01, -2.0295e-02, -4.4118e-01,\n",
      "          -1.3943e-01, -1.8772e-01],\n",
      "         [ 1.7875e-01,  4.6279e-01,  3.9392e-02,  3.3481e-01, -1.2872e-01,\n",
      "          -3.5246e-01, -1.5092e-01,  3.8248e-01,  1.6044e-01,  6.7804e-01,\n",
      "           6.9089e-01,  3.6274e-01,  1.8496e-01, -3.7268e-01,  5.7982e-01,\n",
      "           1.1325e+00, -1.2693e-01,  8.0223e-02, -3.1849e-01, -6.2582e-01,\n",
      "          -4.4113e-02,  7.1746e-02],\n",
      "         [-6.7099e-01,  7.3809e-02,  4.7452e-01,  4.0523e-01, -1.2493e-02,\n",
      "           4.1906e-02, -1.1335e-01, -9.7146e-02,  3.2351e-02,  3.2844e-01,\n",
      "           3.9474e-01,  2.7703e-02, -4.0546e-01, -2.3857e-01,  6.6288e-02,\n",
      "           3.0629e-01,  2.7777e-01, -2.9715e-01,  1.4900e-01, -6.3495e-01,\n",
      "           1.6588e-01, -5.4254e-03],\n",
      "         [-3.8037e-02,  3.2497e-01,  1.0256e-01,  4.9454e-02, -1.5761e-01,\n",
      "          -9.1441e-02, -1.4461e-01, -6.4621e-01,  5.6638e-01,  5.1505e-02,\n",
      "           3.2504e-01, -9.5867e-02, -2.1085e-01,  1.0878e-01,  3.5138e-01,\n",
      "           8.1317e-01, -7.0327e-02,  2.9275e-02,  2.2740e-01,  7.7547e-02,\n",
      "          -9.3762e-02,  8.0041e-02],\n",
      "         [ 2.0788e-01,  2.5712e-01, -1.4073e-01, -1.0770e-01, -4.0488e-01,\n",
      "          -5.1507e-01,  3.0796e-01, -2.0018e-01,  2.0264e-01,  1.1992e-01,\n",
      "           3.2865e-01,  1.6744e-01,  5.0536e-01,  4.4920e-02, -1.2718e-01,\n",
      "           1.7101e-01,  7.8682e-02, -1.9005e-01,  1.0470e-01,  2.6361e-01,\n",
      "          -6.7002e-02, -2.7595e-01],\n",
      "         [ 5.6102e-02,  2.2741e-01,  4.6607e-01,  7.5644e-02, -1.9015e-01,\n",
      "          -1.6818e-01,  1.6268e-01, -4.5906e-01,  3.4637e-01,  3.1116e-01,\n",
      "           3.1426e-01, -3.7710e-01,  8.7372e-02, -3.2375e-01,  3.4895e-01,\n",
      "           2.1532e-01, -1.8207e-01, -2.3265e-03,  5.5509e-02,  1.3798e-01,\n",
      "          -4.3643e-01, -6.0440e-02],\n",
      "         [ 1.6913e-01,  2.1781e-01,  1.7514e-01,  3.4217e-02, -2.0590e-01,\n",
      "          -2.4670e-01, -4.4774e-01, -4.6514e-01,  4.2774e-01,  2.8254e-01,\n",
      "           5.9998e-01, -2.1155e-01,  3.5277e-03,  2.1130e-01,  2.8355e-01,\n",
      "           9.4368e-01,  1.1093e-01,  1.9138e-02,  3.1211e-01,  9.9909e-02,\n",
      "          -3.1655e-01,  1.9134e-01],\n",
      "         [ 1.9451e-02, -3.9545e-02,  1.5850e-01,  9.4567e-03, -2.8495e-01,\n",
      "          -1.7810e-01,  5.6938e-02, -5.3242e-01,  5.1570e-02,  9.3304e-02,\n",
      "           2.2285e-01, -1.4694e-01,  1.1795e-01, -3.4935e-02,  1.5681e-01,\n",
      "           3.0591e-01, -3.9669e-01, -3.4781e-01,  5.8128e-02, -1.3251e-01,\n",
      "          -1.8207e-01,  4.8451e-02],\n",
      "         [ 1.9663e-01,  1.0669e-02,  9.6905e-02,  1.0909e-01, -1.4892e-01,\n",
      "          -3.4293e-02, -2.8741e-01,  2.6004e-02,  1.2506e-01,  2.7601e-01,\n",
      "           5.3773e-01, -2.6764e-01, -1.4105e-01,  1.9656e-02,  2.0393e-01,\n",
      "           2.7323e-01,  3.2212e-01,  4.2048e-02,  2.8966e-01, -7.0880e-02,\n",
      "          -2.4810e-02,  3.8196e-02],\n",
      "         [ 3.0382e-01,  4.1732e-01,  5.0813e-01, -2.0932e-01,  4.0315e-02,\n",
      "          -1.1611e-01, -2.4410e-01, -2.7740e-01,  4.3308e-01,  1.4059e-01,\n",
      "           3.0447e-01, -4.3404e-02,  6.5822e-02,  3.2605e-01, -1.7966e-01,\n",
      "           3.6766e-01,  1.3063e-01, -3.2617e-01,  1.8851e-01, -1.6286e-01,\n",
      "          -1.7005e-01,  1.2726e-01],\n",
      "         [ 2.9444e-01,  1.1148e-01,  3.7121e-03,  2.6373e-01, -2.0499e-01,\n",
      "          -4.4549e-02, -2.6569e-01, -2.3911e-01,  8.3937e-01,  2.9450e-01,\n",
      "          -3.2356e-03, -8.0507e-02,  2.0339e-01, -4.8521e-02,  2.1015e-01,\n",
      "           2.7613e-01, -2.1216e-02,  1.6945e-01, -5.3348e-02,  8.1657e-02,\n",
      "           5.3317e-02,  1.4141e-01],\n",
      "         [ 1.2586e-01,  9.2616e-02,  7.7737e-02, -1.2633e-02,  3.5977e-03,\n",
      "           1.2188e-02,  7.4682e-02, -7.8169e-02,  5.0817e-01,  1.4973e-01,\n",
      "           2.3606e-01, -1.6691e-01, -1.0079e-01, -7.0058e-02,  2.5132e-01,\n",
      "           3.6904e-01, -1.2387e-01, -1.6978e-01, -8.4074e-02, -6.5201e-02,\n",
      "          -1.9585e-01, -2.2561e-02]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [25]\n",
      "tensor([[[-1.1684e-01,  8.6373e-02,  1.7866e-01,  3.5409e-02, -2.3157e-01,\n",
      "           8.3665e-02,  5.1556e-01, -2.7424e-01,  1.9815e-01,  2.6547e-01,\n",
      "          -2.5681e-01, -2.9515e-01, -8.8362e-02, -9.6488e-02,  2.8364e-01,\n",
      "          -1.5828e-01,  7.1712e-02, -3.9941e-01,  3.2430e-01,  2.1237e-01,\n",
      "          -6.8166e-02, -1.1421e-01],\n",
      "         [-3.0156e-01,  2.2397e-01, -7.4060e-02, -3.3449e-01,  3.9211e-01,\n",
      "           3.4504e-01, -3.1775e-01, -1.3836e-01,  1.7969e-01,  1.5701e-01,\n",
      "          -1.5209e-01,  1.2759e-01,  2.2348e-01,  3.9484e-01,  3.3978e-02,\n",
      "           3.7107e-01, -6.6134e-01,  1.8505e-01,  2.7870e-01, -1.5299e-01,\n",
      "          -1.1402e-01,  6.4487e-01],\n",
      "         [ 2.3272e-01,  1.6324e-01, -5.9556e-01, -4.0218e-01,  4.2300e-01,\n",
      "           1.5600e-01,  1.1340e-01, -1.4204e-01,  1.0001e-01,  9.3397e-02,\n",
      "           3.9778e-02,  4.0684e-02,  6.7233e-01,  6.5015e-02,  3.8290e-01,\n",
      "          -1.9522e-02, -6.7115e-01,  2.1989e-02,  5.6338e-01,  1.5731e-01,\n",
      "          -2.5994e-01,  4.2556e-01],\n",
      "         [ 9.4489e-02,  2.1057e-01, -6.6767e-01,  1.0387e-01,  1.7634e-01,\n",
      "           2.9954e-02,  2.4720e-01, -2.4941e-01, -2.4968e-01,  1.6597e-01,\n",
      "           3.0819e-01, -1.0625e-01,  3.9770e-01,  9.9320e-02,  4.8497e-01,\n",
      "          -2.2479e-01,  4.3651e-02,  2.9614e-01,  4.7195e-01,  1.4406e-01,\n",
      "          -4.2601e-01,  5.0986e-01],\n",
      "         [-2.7434e-01,  1.4369e-01, -3.1107e-02,  1.9370e-01,  6.2448e-02,\n",
      "          -4.4925e-01, -4.0743e-02, -3.6620e-01, -5.5129e-01,  5.7604e-01,\n",
      "          -2.0579e-01,  5.2272e-01,  8.1000e-02, -2.8542e-01,  5.2707e-02,\n",
      "          -3.3651e-01, -4.1783e-01,  3.8631e-01,  3.1774e-01,  5.8953e-02,\n",
      "          -4.4363e-01,  1.0368e+00],\n",
      "         [-6.5532e-02,  4.3669e-02, -5.1527e-01,  6.8188e-02, -6.3180e-02,\n",
      "           1.0082e-01,  1.0995e-01, -2.9737e-01, -1.5241e-01,  3.8642e-01,\n",
      "          -3.1907e-01,  5.2101e-02,  1.8862e-01, -3.5433e-01, -1.3391e-01,\n",
      "           3.0440e-02, -1.6518e-01,  5.3937e-01,  2.2681e-01,  6.5193e-02,\n",
      "          -4.9211e-02,  8.9306e-01],\n",
      "         [-4.7970e-01,  2.3216e-01, -5.1982e-01,  1.4192e-01,  3.0637e-01,\n",
      "          -6.6432e-02,  3.1963e-01, -4.2796e-01, -3.4430e-01,  2.5518e-01,\n",
      "           1.7150e-01,  2.5334e-01, -1.0900e-01,  8.4297e-02, -2.8350e-01,\n",
      "          -3.5049e-01, -2.7186e-01,  2.9726e-01,  6.9172e-02,  4.7620e-02,\n",
      "           1.7839e-01,  5.4051e-01],\n",
      "         [-3.1015e-01,  3.9821e-01, -1.4577e-02,  1.6548e-01,  5.2817e-01,\n",
      "           5.8016e-02, -1.7083e-01, -5.2810e-01,  4.7208e-02,  1.4574e-01,\n",
      "           2.5600e-01,  1.3415e-01, -2.9304e-01, -9.5144e-02, -6.5359e-01,\n",
      "           7.3714e-02, -5.0788e-01,  6.1491e-01,  5.7554e-01, -2.1836e-02,\n",
      "          -1.7787e-01,  6.8529e-01],\n",
      "         [-5.3825e-01,  2.6943e-02, -3.4406e-01,  4.0546e-02,  5.6361e-02,\n",
      "           3.2669e-01,  2.0726e-01,  1.5503e-01, -4.7282e-01,  6.7493e-02,\n",
      "          -1.3697e-01,  5.9250e-01,  1.5694e-01, -1.4940e-01, -4.5680e-01,\n",
      "           3.9299e-01, -4.8143e-01,  2.2856e-01,  4.0921e-01,  3.7875e-01,\n",
      "           2.2155e-01,  6.8861e-01],\n",
      "         [-3.7230e-01,  1.2316e-01, -6.4449e-01, -1.2177e-01,  1.6215e-01,\n",
      "           4.2749e-01,  2.5537e-02, -1.6892e-01, -3.8973e-01,  1.6124e-01,\n",
      "           5.6431e-02,  5.6170e-01,  2.1108e-01, -3.1871e-01, -5.1086e-01,\n",
      "          -2.8706e-01, -3.5319e-01,  1.5311e-01,  7.5110e-01,  6.0643e-01,\n",
      "           3.4674e-01,  2.8260e-01],\n",
      "         [-4.2154e-01, -1.4131e-01, -5.0508e-01,  5.4345e-01,  7.2942e-02,\n",
      "           3.6242e-02,  4.8308e-01,  1.0016e-01, -3.9291e-01, -1.4940e-01,\n",
      "           1.5662e-01, -2.6181e-01,  5.6392e-01, -1.2598e-01, -3.1671e-01,\n",
      "          -2.7153e-01, -4.1126e-02,  5.6227e-01,  1.9120e-01,  3.3260e-01,\n",
      "          -1.7444e-02,  4.5416e-01],\n",
      "         [-1.6628e-01, -6.0511e-02, -2.9779e-01,  2.5476e-01,  4.2133e-01,\n",
      "           2.3687e-01,  1.7286e-01,  4.9458e-02, -4.2818e-01,  4.5735e-01,\n",
      "           1.2380e-02,  7.8750e-01, -2.4528e-01,  1.5670e-01, -6.0600e-01,\n",
      "           9.2743e-04, -4.5551e-01,  1.9007e-01,  3.8555e-01,  4.2174e-01,\n",
      "          -1.8214e-01,  6.6441e-01],\n",
      "         [-1.6956e-01, -2.6403e-01, -4.4690e-01, -1.7486e-01,  2.2948e-01,\n",
      "           2.1464e-01,  2.6230e-01, -5.4209e-01,  7.9298e-03, -1.0595e-01,\n",
      "           3.2418e-01,  3.2640e-01,  1.2546e-01, -3.6889e-01, -2.4946e-01,\n",
      "          -6.3608e-02,  2.1996e-02,  2.0614e-01,  3.5665e-01, -3.0240e-02,\n",
      "          -1.1257e-01,  1.3617e-01],\n",
      "         [-5.7303e-01, -2.8608e-01,  9.4657e-02,  3.7523e-01,  5.7037e-01,\n",
      "          -2.8011e-01,  3.3795e-01, -2.4727e-02, -1.5783e-01,  6.9693e-01,\n",
      "          -1.8811e-01,  7.4450e-01,  2.1094e-01, -6.5708e-02,  2.7990e-01,\n",
      "          -2.5563e-01, -6.0184e-01,  5.5493e-01,  1.8245e-01,  6.0940e-01,\n",
      "          -1.4790e-01,  9.9145e-01],\n",
      "         [-6.9282e-02,  8.8757e-02, -2.3420e-01, -3.0995e-01,  2.0282e-01,\n",
      "          -2.4394e-01, -2.6712e-01, -8.0057e-02, -2.9499e-01,  5.2153e-01,\n",
      "          -3.3554e-01,  3.9134e-01,  6.0395e-02, -1.2157e-02, -6.3989e-01,\n",
      "          -2.7387e-01, -6.1901e-01,  5.5134e-01,  3.9454e-01,  6.1001e-01,\n",
      "          -2.4689e-01,  6.2669e-01],\n",
      "         [ 7.2259e-02,  1.9974e-01, -6.9697e-01,  1.3533e-01,  3.4426e-01,\n",
      "          -4.2728e-01,  1.1781e-03, -6.3312e-01, -7.7801e-02,  5.3034e-01,\n",
      "           1.3843e-03,  4.2517e-01,  6.6572e-01,  8.1836e-02, -3.3104e-01,\n",
      "          -3.8635e-01, -9.0532e-01,  5.0840e-01, -1.0025e-01,  9.6199e-02,\n",
      "           8.4902e-02,  2.6261e-01],\n",
      "         [-8.2383e-02, -5.0406e-01,  4.7639e-02,  2.6201e-01,  5.6296e-01,\n",
      "           2.5664e-01, -4.1834e-01, -3.7953e-01,  3.3862e-01,  2.7846e-01,\n",
      "          -4.6488e-02,  5.4833e-02,  4.6680e-01,  7.6023e-01, -8.4317e-02,\n",
      "           3.7072e-01, -3.9575e-01,  7.8222e-01,  1.2336e-01, -1.2053e-01,\n",
      "          -6.1844e-01,  2.7605e-01],\n",
      "         [-2.7655e-01,  4.6374e-02, -3.7226e-01,  3.7344e-02,  4.9283e-01,\n",
      "           7.0190e-03, -6.9562e-01, -3.6247e-01,  3.6326e-03,  3.0340e-01,\n",
      "          -1.1417e-01, -1.3158e-01,  2.3390e-01,  2.0570e-01,  2.1083e-01,\n",
      "          -2.2297e-01, -7.7216e-01,  3.2611e-01,  5.8018e-01,  4.4817e-01,\n",
      "          -4.9353e-01,  5.4775e-01],\n",
      "         [-9.0115e-02, -5.8133e-02, -3.6508e-01,  9.1521e-02,  1.7309e-01,\n",
      "          -3.8050e-01, -3.7409e-01, -2.9373e-01, -2.2272e-03, -2.4935e-02,\n",
      "           3.6354e-01, -4.8318e-01,  1.5413e-01, -9.8465e-02,  6.3170e-01,\n",
      "           1.7601e-02, -6.7883e-01,  1.9993e-01,  1.8423e-01, -1.5571e-01,\n",
      "           2.4580e-01,  3.2878e-01],\n",
      "         [ 1.7930e-01,  7.7715e-01,  5.0738e-02,  6.7578e-02,  5.9155e-01,\n",
      "          -1.6628e-01, -4.9519e-01, -2.5044e-01, -2.8277e-01, -2.7740e-01,\n",
      "          -9.3649e-02, -3.6536e-02,  2.1949e-01, -4.1989e-01,  9.2230e-02,\n",
      "          -2.3337e-01, -3.8994e-01, -3.0140e-02,  1.6669e-01, -2.2104e-01,\n",
      "          -8.0057e-02,  8.6701e-01],\n",
      "         [ 2.3080e-01,  9.8270e-01,  1.6976e-01, -3.3470e-01,  6.8793e-01,\n",
      "          -3.4852e-01, -1.1729e-02, -4.9535e-01, -2.9049e-01,  6.0577e-02,\n",
      "           1.2402e-01, -8.1273e-02,  3.8341e-01, -1.5450e-01,  2.4735e-01,\n",
      "          -3.1845e-02, -3.1573e-01,  2.6329e-01, -1.0904e-02, -5.1411e-01,\n",
      "          -1.0689e-01,  1.1328e-01],\n",
      "         [-1.7310e-01, -1.8784e-02,  1.8629e-01, -7.0542e-03,  4.4006e-01,\n",
      "          -3.9259e-01, -1.5010e-01, -1.7742e-01,  4.5002e-02, -1.2639e-01,\n",
      "           4.9553e-01, -9.2030e-02,  4.9133e-02, -1.1320e-01,  2.8973e-01,\n",
      "           1.1811e-01, -4.8758e-01,  7.4695e-01, -2.2554e-01, -1.4272e-01,\n",
      "          -7.9848e-02,  9.1166e-01],\n",
      "         [ 1.7197e-03,  2.8839e-01, -9.9633e-02, -1.2297e-01,  1.7587e-02,\n",
      "          -4.5827e-01, -5.8084e-01, -2.0165e-01,  1.5438e-02,  8.1408e-02,\n",
      "           9.5344e-02, -1.6293e-01,  1.6857e-03, -3.0901e-01, -7.7539e-02,\n",
      "          -8.8210e-02, -7.7602e-01,  3.8794e-01, -6.6174e-02, -2.8899e-01,\n",
      "          -2.8742e-01,  1.0502e+00],\n",
      "         [ 1.4948e-01, -6.9069e-02, -4.6181e-01, -2.3781e-01,  4.0091e-01,\n",
      "           1.0406e-02, -6.4836e-01, -3.3542e-01, -1.3082e-01,  7.0113e-02,\n",
      "           1.5400e-01, -5.2958e-02, -2.4733e-01,  2.0501e-01,  4.5588e-02,\n",
      "           6.0818e-02, -5.3603e-01,  2.3647e-01, -5.0430e-01, -5.9572e-02,\n",
      "          -2.2698e-01,  5.4930e-01],\n",
      "         [-2.9483e-01, -1.5249e-01, -6.8146e-01,  8.3915e-02,  6.7330e-01,\n",
      "          -2.0479e-01, -6.8809e-02, -5.1297e-01, -9.9279e-03,  2.4338e-01,\n",
      "          -3.4330e-02, -2.7446e-01,  9.2920e-02, -6.4995e-02,  1.1039e-01,\n",
      "           3.7435e-01, -4.6105e-01,  3.4911e-01,  3.0762e-01,  1.2172e-01,\n",
      "          -1.8807e-01,  3.4097e-01],\n",
      "         [-1.2886e-01,  7.7074e-02, -1.3159e-01,  7.2727e-02,  1.5149e-01,\n",
      "          -1.0182e-01, -1.3891e-01, -2.0356e-01, -7.6222e-02,  1.2961e-01,\n",
      "           1.3593e-01, -1.0343e-02,  1.5093e-01, -3.0069e-01,  2.2856e-01,\n",
      "           2.4884e-02, -2.8554e-01,  2.7498e-01,  2.4316e-01, -1.0127e-01,\n",
      "          -2.4899e-01,  5.0639e-01],\n",
      "         [-1.0136e-01, -1.9946e-02, -6.6134e-02,  1.5316e-01,  2.0208e-01,\n",
      "          -3.8504e-02, -8.6389e-02, -4.3901e-01, -1.3146e-02,  6.2846e-02,\n",
      "           1.2724e-01, -3.2322e-02,  8.6832e-02, -2.5789e-02,  1.6427e-01,\n",
      "           1.1803e-01, -1.9348e-01,  4.5167e-01,  1.6627e-02, -1.0746e-01,\n",
      "          -1.9022e-01,  3.7311e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [21]\n",
      "tensor([[[ 1.7761e-02, -2.3475e-01, -4.1477e-01, -7.2497e-02, -3.4928e-01,\n",
      "           1.1241e-01, -2.1456e-01, -3.6685e-01,  1.9185e-01,  3.8855e-01,\n",
      "          -4.3497e-02, -2.2915e-03, -2.7146e-01, -1.6754e-01,  4.3918e-03,\n",
      "          -2.9730e-01,  4.6797e-01, -2.7156e-01, -3.0207e-01,  2.3447e-01,\n",
      "           5.1941e-01, -3.0523e-01],\n",
      "         [ 5.5456e-02, -4.6800e-01,  5.7311e-01, -7.7940e-01,  5.6577e-02,\n",
      "          -2.5015e-01, -4.2425e-01, -7.8957e-03,  1.2816e-01,  2.2352e-01,\n",
      "           4.1924e-01, -1.0620e-01,  1.2132e-01,  2.1003e-01,  1.2608e-01,\n",
      "          -1.2173e-01,  5.6598e-01, -3.1664e-01, -1.3857e-01,  6.8057e-01,\n",
      "           4.7788e-01,  2.4944e-01],\n",
      "         [ 2.7452e-01, -7.3335e-01,  4.7954e-01, -7.1963e-01,  1.5148e-01,\n",
      "           3.3411e-02,  2.5631e-02,  1.5644e-01,  8.1557e-02,  1.0071e-01,\n",
      "           1.9251e-02,  5.5918e-02,  3.7267e-02,  1.9772e-01,  8.1545e-02,\n",
      "          -1.5764e-01,  5.6631e-01, -3.8763e-01, -1.5390e-01,  3.3552e-01,\n",
      "           6.9373e-01,  1.1373e-01],\n",
      "         [ 9.2665e-02, -8.3622e-02,  9.9064e-02, -3.1221e-01, -1.2627e-01,\n",
      "           7.5784e-02,  5.5583e-02,  2.3944e-01,  1.9796e-02,  6.6566e-01,\n",
      "           1.7849e-01,  3.3115e-01,  1.4016e-01,  5.0706e-01, -6.3974e-03,\n",
      "          -4.4482e-01,  3.7269e-01, -5.3736e-01,  2.7537e-01,  7.5713e-01,\n",
      "           6.3668e-01,  2.3973e-01],\n",
      "         [-6.5622e-02, -5.9087e-01,  1.9670e-02, -3.0286e-01,  4.9605e-01,\n",
      "           1.1723e-01,  3.7752e-02,  4.2180e-01, -1.5324e-02,  2.8017e-01,\n",
      "          -1.2073e-01, -2.2831e-01, -1.7676e-01, -1.7461e-01,  4.8848e-01,\n",
      "          -2.8237e-01, -7.1420e-02, -9.5148e-02,  2.0237e-02,  6.4021e-01,\n",
      "           4.0635e-01, -2.7729e-01],\n",
      "         [-5.0695e-02, -4.6326e-01, -1.9452e-01, -5.6281e-01,  3.2776e-01,\n",
      "           1.4255e-01,  9.3161e-02,  2.1573e-01,  4.9411e-02, -1.1384e-01,\n",
      "          -1.8490e-01, -2.3638e-01,  1.0354e-01,  1.7355e-01,  7.1593e-01,\n",
      "          -4.4415e-01,  7.8350e-02, -4.4452e-01,  1.2063e-01,  6.0149e-01,\n",
      "           2.4720e-01, -3.2492e-01],\n",
      "         [-3.7413e-01, -4.6869e-01, -9.5000e-03, -5.4195e-01,  5.3407e-01,\n",
      "           9.2528e-02,  3.3384e-01,  5.5913e-01, -2.1320e-01,  3.2546e-01,\n",
      "           2.4335e-01,  2.8726e-01, -2.4488e-01,  1.3639e-01,  3.6672e-01,\n",
      "          -3.9967e-01,  4.3334e-01,  8.7528e-02, -1.1805e-02,  3.3059e-01,\n",
      "           5.3402e-01, -1.1668e-01],\n",
      "         [ 4.5292e-02, -2.6157e-01, -1.8280e-01, -2.0498e-01,  3.2011e-01,\n",
      "           1.9343e-01,  2.1983e-01,  2.1757e-01, -3.0760e-01, -6.2403e-02,\n",
      "          -3.8859e-01, -1.7836e-01, -4.5166e-02,  5.0027e-01,  5.1640e-01,\n",
      "          -4.0760e-01,  1.1569e-01, -2.6481e-01,  2.9314e-02,  3.9074e-01,\n",
      "           7.3728e-01,  1.1013e-01],\n",
      "         [-2.3980e-01, -5.0595e-01,  2.5831e-01,  6.1417e-02,  3.3835e-01,\n",
      "           2.5317e-01,  9.7858e-02,  2.8864e-01, -2.1837e-01, -2.2170e-01,\n",
      "           3.4627e-01, -5.2174e-01, -1.1000e-01, -4.9946e-03,  3.0973e-01,\n",
      "          -3.1383e-01, -2.0907e-01, -6.3811e-02,  2.3859e-01,  2.6409e-01,\n",
      "           4.8829e-01, -5.4844e-01],\n",
      "         [-1.2583e-01,  7.4371e-02,  8.8227e-02, -4.1296e-01,  2.7802e-01,\n",
      "          -4.3085e-01,  2.3673e-01,  2.3662e-01, -1.0644e-01, -2.3836e-01,\n",
      "           2.5171e-01, -1.4100e-01,  5.1706e-02,  8.9901e-02,  4.3127e-01,\n",
      "          -2.0342e-03,  3.4566e-01, -2.5958e-02,  3.9115e-01,  4.3657e-01,\n",
      "           1.5887e-01, -5.0834e-01],\n",
      "         [-3.8183e-02, -4.5407e-01,  3.9473e-02, -5.5338e-01,  2.1622e-01,\n",
      "           1.2875e-02,  1.4571e-01,  2.4819e-01, -2.0731e-02, -4.1946e-01,\n",
      "          -2.4575e-01,  6.5436e-03, -2.4862e-01,  4.9024e-01,  3.8140e-01,\n",
      "          -5.8272e-01,  1.1148e-01, -9.6768e-02,  1.7170e-01,  6.0968e-01,\n",
      "           5.8010e-01, -1.9070e-01],\n",
      "         [-1.6487e-01, -7.0046e-01,  1.6738e-01, -1.5679e-01,  3.7742e-01,\n",
      "           5.0555e-01,  7.3208e-02,  9.9243e-02,  1.9646e-01,  9.1154e-02,\n",
      "          -2.4866e-01, -2.8861e-01, -1.0944e-01, -3.6701e-02,  4.6464e-01,\n",
      "          -4.8847e-01, -1.0244e-01,  7.7559e-02,  2.2923e-01,  2.0038e-01,\n",
      "           4.7377e-01, -3.7437e-01],\n",
      "         [-4.4743e-01, -4.7946e-01, -1.6114e-01, -2.1923e-01,  2.5798e-01,\n",
      "           2.9246e-01,  2.8609e-01,  6.2462e-02, -1.6887e-01,  1.6407e-02,\n",
      "          -1.6984e-01, -5.4962e-04, -1.8582e-01,  2.0385e-01,  6.7807e-01,\n",
      "          -5.8970e-01, -4.8190e-02,  1.9297e-01,  3.8250e-02,  1.0065e-01,\n",
      "           2.0216e-01, -3.2277e-01],\n",
      "         [-2.2264e-01, -8.5369e-01,  1.5343e-01, -4.0614e-01,  1.9228e-01,\n",
      "           2.9101e-01,  3.0186e-01,  2.5096e-01, -6.6644e-02,  2.9160e-01,\n",
      "           5.6556e-02, -2.6914e-02, -1.4057e-01,  6.4296e-02,  8.1479e-01,\n",
      "          -6.7708e-01,  3.7573e-02,  8.7081e-02, -1.6876e-01,  4.4757e-01,\n",
      "           4.9892e-01, -6.3191e-01],\n",
      "         [-3.9845e-01, -8.7643e-01, -2.0232e-02, -1.9057e-01,  2.5067e-02,\n",
      "           5.7988e-01,  3.5181e-01,  5.3902e-01,  3.1037e-01,  5.9997e-01,\n",
      "          -4.2050e-01,  1.8907e-02, -9.8640e-02,  3.0495e-01,  4.1162e-01,\n",
      "          -2.0818e-01,  4.6231e-01,  2.6419e-01, -1.3859e-01,  4.5841e-01,\n",
      "           2.5493e-01, -6.6430e-01],\n",
      "         [-1.3082e-01, -8.6549e-01, -8.1498e-02, -1.6704e-01, -3.6126e-03,\n",
      "           5.0795e-01,  7.4044e-02,  3.6922e-01,  3.7651e-01,  5.8109e-01,\n",
      "          -4.7873e-01, -2.3743e-01, -2.2476e-01,  1.7943e-01,  5.4856e-01,\n",
      "          -1.0184e-01,  1.1873e-01, -2.2937e-01, -1.1342e-01,  2.6592e-01,\n",
      "           3.2089e-01, -3.2591e-01],\n",
      "         [-1.4254e-01, -6.3289e-01,  3.0016e-01, -3.1895e-01,  3.2869e-01,\n",
      "           4.1523e-01,  3.0608e-01,  3.7958e-01,  1.4707e-01,  5.4566e-01,\n",
      "          -2.3163e-01, -1.0589e-01, -2.3506e-01, -2.2747e-01,  7.5738e-01,\n",
      "          -2.8158e-01,  2.6503e-01, -8.2824e-02, -2.3228e-01,  5.1643e-01,\n",
      "           5.3177e-01, -4.4591e-01],\n",
      "         [-1.6579e-01, -3.3769e-01,  3.3438e-01, -6.4802e-01, -1.8894e-01,\n",
      "           3.4529e-01, -5.0066e-01,  4.6759e-01,  1.4149e-01,  7.8558e-01,\n",
      "          -1.3115e-01, -1.5463e-01,  2.8245e-01, -2.0706e-01,  3.7215e-01,\n",
      "           1.0679e-01,  3.6590e-01, -4.8935e-02, -2.3099e-01,  3.1309e-01,\n",
      "           1.9062e-01, -7.0640e-02],\n",
      "         [ 4.1250e-02, -7.4073e-01,  7.1597e-02, -4.1747e-01, -2.4470e-02,\n",
      "           4.4873e-01, -3.4261e-01,  3.6032e-01,  4.7454e-01,  5.9597e-01,\n",
      "          -3.9397e-02, -4.0870e-01,  7.6854e-02, -2.5989e-01,  3.1078e-01,\n",
      "          -2.0608e-01,  2.5483e-01,  3.2027e-02, -7.0166e-02,  3.6373e-01,\n",
      "           6.0259e-01,  5.0085e-02],\n",
      "         [ 4.0779e-01, -6.5153e-01,  3.5283e-02, -1.8759e-01, -1.8758e-01,\n",
      "           4.3172e-01, -4.1691e-01,  5.6968e-02,  1.9560e-01,  5.8218e-01,\n",
      "          -2.0123e-01, -4.2751e-01,  9.1636e-02, -4.8726e-01,  3.9040e-01,\n",
      "           1.8352e-01,  1.0318e-01, -1.8114e-01, -1.4320e-01,  6.5244e-01,\n",
      "           5.0811e-01, -2.5384e-01],\n",
      "         [-1.0155e-01, -6.5650e-01,  2.5893e-02, -1.1078e-02, -8.1589e-02,\n",
      "           6.5203e-01, -6.6707e-01,  2.0948e-01,  3.0741e-01,  4.8427e-01,\n",
      "          -9.5975e-02, -4.8553e-01,  1.1041e-02, -3.4898e-01,  1.3882e-01,\n",
      "           6.8416e-01,  3.9663e-01, -2.3652e-01, -3.5041e-01,  6.1304e-01,\n",
      "           6.1778e-01,  4.4148e-02],\n",
      "         [ 2.8348e-01, -6.8191e-01,  5.6177e-02, -4.2004e-01,  3.4633e-01,\n",
      "           3.3709e-01, -3.3723e-01,  3.8259e-01,  2.0082e-01,  6.2633e-01,\n",
      "          -2.2161e-01, -5.7423e-01, -8.9554e-02, -2.9124e-01,  6.0861e-01,\n",
      "           6.6238e-01,  1.3462e-01, -9.8902e-02, -2.6626e-01,  9.5159e-01,\n",
      "           7.6986e-01, -2.8688e-01],\n",
      "         [ 9.3594e-02, -4.1717e-01,  7.2698e-02, -2.7870e-01, -2.3869e-01,\n",
      "           4.3317e-01, -3.8524e-01,  2.6338e-01,  2.3021e-01,  5.5350e-01,\n",
      "          -1.3401e-01, -2.7627e-01,  7.5706e-02, -8.4712e-02,  2.9181e-01,\n",
      "           3.2000e-01,  1.5839e-01, -1.7673e-01, -1.9638e-02,  5.8784e-01,\n",
      "           7.8606e-01, -9.1648e-02]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [21]\n",
      "tensor([[[ 1.1720e-01, -1.3700e-01, -3.3969e-01,  1.9074e-01, -4.6266e-01,\n",
      "           4.0922e-01,  1.0280e-02, -8.4236e-02,  6.7645e-01, -3.9967e-01,\n",
      "           3.1284e-01,  9.2138e-02,  2.6016e-01,  3.4509e-02,  1.1783e-01,\n",
      "          -4.1218e-01,  2.7044e-01, -3.1385e-01,  1.6106e-01, -2.2764e-01,\n",
      "           2.4775e-01, -3.4660e-01],\n",
      "         [ 1.3817e-01, -3.5277e-01,  3.2543e-02, -2.9863e-01, -4.8400e-01,\n",
      "           1.5174e-01,  3.1298e-01, -2.7854e-03, -1.3427e-01,  3.0632e-01,\n",
      "          -5.3013e-01,  4.4216e-01,  9.6842e-01, -3.3503e-01, -1.8244e-01,\n",
      "           1.1137e-01,  3.3523e-01,  2.3486e-02, -7.7925e-02, -3.3182e-01,\n",
      "          -7.8121e-02, -6.2573e-01],\n",
      "         [-1.8532e-01, -8.4792e-02,  5.2334e-01, -7.4682e-02,  3.1243e-02,\n",
      "           2.6125e-01, -1.0208e-01, -5.1898e-02, -1.1336e-01, -2.6489e-01,\n",
      "          -1.8529e-01, -2.0625e-02,  8.3299e-01,  4.4833e-03, -1.5273e-01,\n",
      "          -4.7143e-01,  1.9499e-01,  1.2723e-01, -3.1052e-01, -3.9327e-01,\n",
      "          -1.7540e-01, -1.4181e-01],\n",
      "         [-4.7370e-01, -3.7330e-01, -6.0747e-03, -3.2431e-01, -6.0186e-02,\n",
      "          -2.9668e-01,  6.7225e-02, -6.9374e-02,  1.3182e-01,  3.9574e-01,\n",
      "          -2.4118e-01,  4.8267e-01,  4.5518e-01, -3.3431e-01,  2.4981e-01,\n",
      "          -5.9953e-02,  2.1859e-01, -2.5765e-01,  5.5914e-02, -1.6644e-01,\n",
      "          -2.5605e-01, -5.9851e-01],\n",
      "         [ 2.0420e-01, -3.0965e-01, -7.7752e-02,  7.6102e-03,  7.7843e-02,\n",
      "           3.9920e-01, -4.4613e-02,  7.2612e-02,  2.5861e-01, -1.1062e-01,\n",
      "           2.3108e-01,  4.4663e-01,  6.7372e-01, -2.6705e-01, -3.8692e-01,\n",
      "          -2.9719e-02,  2.6932e-01, -6.7070e-01, -1.0983e-01, -3.6764e-01,\n",
      "           6.0320e-03, -1.8749e-01],\n",
      "         [ 2.2121e-02, -3.6791e-01, -4.1908e-02,  1.1181e-01, -1.3445e-02,\n",
      "           3.2927e-01,  3.1710e-01,  8.7394e-02,  1.4065e-01, -2.0097e-01,\n",
      "          -1.6566e-01,  6.6210e-01,  2.7493e-01, -1.4890e-01, -7.8607e-02,\n",
      "           1.1821e-02,  3.9970e-01, -4.1152e-01,  7.4630e-02, -2.1115e-01,\n",
      "           7.0980e-02, -3.6731e-01],\n",
      "         [-3.8313e-01, -4.5238e-01,  3.5251e-04,  4.5025e-01, -9.3332e-03,\n",
      "           2.6056e-01,  2.1885e-01,  7.2728e-02, -1.3502e-01,  1.5484e-01,\n",
      "          -4.7950e-03,  3.2926e-01,  2.7459e-01, -3.1618e-01, -8.3904e-02,\n",
      "          -7.5142e-02,  5.5036e-01, -1.9856e-01,  1.3613e-01, -2.2996e-01,\n",
      "           5.1004e-02, -3.8728e-01],\n",
      "         [-3.7529e-01, -5.1660e-01, -2.6069e-01,  3.6866e-02,  1.9989e-01,\n",
      "          -1.5037e-01, -1.5000e-01,  3.2160e-01, -3.6220e-02,  3.1831e-01,\n",
      "           6.3921e-02,  6.3763e-01,  1.4830e-01, -3.3915e-01, -4.3601e-01,\n",
      "           2.0996e-01,  4.0812e-01, -5.4203e-01, -1.5028e-01, -2.4651e-01,\n",
      "           1.7311e-01, -5.1968e-01],\n",
      "         [ 2.4183e-01, -2.8480e-01,  2.8166e-02,  8.4838e-02,  1.0459e-01,\n",
      "           3.4552e-01, -3.5470e-01,  8.7427e-01,  1.8324e-01,  1.8487e-01,\n",
      "          -1.2889e-01,  4.8512e-01,  5.0294e-01, -8.5007e-02, -6.6260e-03,\n",
      "          -1.6805e-01,  3.8718e-02, -4.2702e-01, -1.7481e-01, -2.4620e-01,\n",
      "           8.1078e-02, -2.8315e-01],\n",
      "         [-8.9583e-02, -3.2905e-01,  2.1900e-02,  7.2854e-01, -1.5533e-01,\n",
      "           7.0376e-01, -5.2286e-02,  2.6792e-02,  6.1163e-01,  1.9420e-01,\n",
      "           3.2143e-02,  1.8061e-01, -4.2855e-02, -5.4658e-01, -1.7299e-01,\n",
      "          -2.0290e-01,  4.0825e-01, -5.1344e-01,  8.2608e-02, -1.9046e-01,\n",
      "           3.7889e-01, -4.7902e-01],\n",
      "         [ 7.5016e-02, -6.7574e-01,  1.6934e-01,  1.7288e-01, -4.6643e-01,\n",
      "           5.8809e-01, -2.8063e-01,  2.0760e-01,  4.1599e-01,  2.6910e-01,\n",
      "           3.3152e-01,  2.7378e-01,  2.1639e-04, -3.6708e-01, -2.6879e-01,\n",
      "          -4.0533e-02, -4.1892e-02, -4.1678e-01, -1.1395e-01, -2.1582e-01,\n",
      "           3.3235e-01, -3.2373e-01],\n",
      "         [ 7.4793e-03, -3.9775e-01,  4.6006e-01, -7.1158e-02, -1.7716e-01,\n",
      "           6.2049e-01, -1.7583e-01,  2.8716e-02,  2.6080e-01, -2.5766e-02,\n",
      "           4.4235e-01,  1.0423e-01,  3.1100e-01, -1.0628e-01, -1.4674e-01,\n",
      "           3.3492e-02,  2.6500e-01, -4.0394e-01, -1.1821e-01,  1.5955e-02,\n",
      "           2.2864e-01, -2.1803e-01],\n",
      "         [-2.4122e-01, -2.4946e-01, -4.7863e-02,  5.6786e-02, -1.9928e-01,\n",
      "           6.2774e-01,  3.6219e-01,  9.1060e-02,  3.0724e-01,  3.2382e-01,\n",
      "          -5.0892e-02,  2.5130e-01,  3.6110e-01, -2.0061e-01, -4.6372e-01,\n",
      "          -1.0439e-01,  2.8491e-01, -4.4176e-01,  3.2675e-01, -2.4101e-01,\n",
      "           8.2657e-02, -4.5100e-01],\n",
      "         [ 1.3473e-01, -2.4924e-01,  9.3044e-02,  1.0418e-01,  1.1281e-01,\n",
      "          -1.1410e-01,  3.1692e-01,  9.5931e-02, -9.1936e-02,  2.6881e-01,\n",
      "           4.2571e-02,  2.6505e-01,  3.9279e-01, -4.0378e-01, -1.5498e-01,\n",
      "           3.6636e-02,  2.0583e-01, -1.0760e-01, -1.3972e-01,  1.3469e-01,\n",
      "           4.0475e-02, -3.9987e-01],\n",
      "         [ 7.3356e-02, -4.1440e-01,  5.2390e-01,  2.2682e-01,  2.8457e-02,\n",
      "          -2.4753e-01, -2.7384e-01,  2.7947e-01, -2.3641e-01, -1.3220e-02,\n",
      "           1.7572e-01,  8.5127e-01,  4.5183e-01, -2.4441e-01, -1.1040e-01,\n",
      "           3.1556e-01,  3.1476e-01,  2.3898e-01, -1.1136e-01,  6.9413e-02,\n",
      "           2.9478e-01, -2.8211e-01],\n",
      "         [ 1.8292e-01, -3.7175e-01,  2.6900e-02, -2.2361e-01,  3.2035e-01,\n",
      "           2.3494e-02, -1.5664e-01, -2.5082e-01,  1.4150e-01,  7.8410e-02,\n",
      "           3.9197e-01,  4.0702e-01,  5.0434e-01, -6.1673e-01, -4.8413e-01,\n",
      "           3.3777e-02,  1.9282e-01, -6.0559e-01, -3.9417e-02, -6.0280e-02,\n",
      "           2.6223e-01, -6.3110e-01],\n",
      "         [ 3.5004e-01, -1.0244e-01, -4.7449e-02,  2.4278e-01, -9.7017e-02,\n",
      "           1.1040e-01, -1.5541e-01,  6.8590e-02,  6.3852e-02,  2.7084e-01,\n",
      "           4.8300e-01,  5.3641e-01,  5.3954e-01,  3.1434e-01, -4.0992e-01,\n",
      "          -5.2041e-02,  2.5443e-01, -8.6519e-01,  3.5885e-01, -6.6433e-01,\n",
      "          -1.7350e-01, -7.4587e-01],\n",
      "         [ 3.8204e-01, -2.1399e-01,  6.2964e-01, -7.9134e-02, -6.3920e-02,\n",
      "           1.6447e-01, -7.0883e-02,  2.6736e-01, -2.6708e-01,  3.3683e-01,\n",
      "           1.2044e-03,  5.2557e-01,  2.6914e-02, -1.3747e-01, -4.0172e-01,\n",
      "          -5.6357e-01, -2.5907e-01, -1.7177e-01,  3.3562e-01, -4.0224e-01,\n",
      "          -1.0700e-01, -7.9214e-01],\n",
      "         [ 3.4156e-01, -2.4526e-01,  3.2000e-01, -9.6717e-02, -1.7188e-01,\n",
      "           1.8345e-01, -2.2784e-01, -2.1634e-02, -3.1171e-01,  2.9214e-01,\n",
      "          -1.9150e-01,  4.7245e-01,  2.3612e-01, -3.0001e-01, -6.7757e-01,\n",
      "          -5.1722e-01,  2.4220e-01, -1.5317e-01,  2.0595e-01, -5.2354e-01,\n",
      "          -2.0871e-01, -4.5663e-01],\n",
      "         [ 6.8277e-02, -4.5885e-01,  5.6708e-01,  4.6384e-01,  3.3278e-03,\n",
      "           3.3285e-02, -1.1321e-02, -8.0288e-02, -4.5884e-01,  9.5470e-03,\n",
      "           3.7769e-02,  5.8075e-01,  4.8273e-01, -3.4308e-01, -6.9074e-02,\n",
      "          -5.2981e-01, -7.5179e-02, -4.5128e-01,  2.7148e-01, -4.8005e-01,\n",
      "          -9.7801e-02, -5.2096e-01],\n",
      "         [ 2.3762e-01, -1.1266e-01,  1.5893e-01,  2.6036e-01, -1.8956e-01,\n",
      "          -1.7076e-02, -1.8804e-01, -1.7568e-01, -1.3771e-01,  2.0580e-01,\n",
      "           2.5435e-01,  3.7546e-01,  4.9460e-01, -2.2267e-01, -3.4647e-01,\n",
      "          -4.8241e-02,  2.2488e-01, -3.6488e-01,  6.4716e-01, -5.8539e-01,\n",
      "          -2.9667e-01, -2.7536e-01],\n",
      "         [ 2.3244e-02, -4.4117e-01, -1.1169e-01, -2.1233e-02, -8.6438e-02,\n",
      "           1.8937e-01,  1.3581e-02,  1.8845e-01,  3.3385e-01, -7.7367e-02,\n",
      "           1.3421e-01,  3.4331e-01, -6.0593e-02,  1.2121e-01, -2.6037e-01,\n",
      "          -2.7666e-01,  1.4046e-01, -3.5374e-01,  3.6527e-01, -2.7571e-01,\n",
      "           2.6415e-01, -5.4025e-01],\n",
      "         [ 2.9341e-01, -3.4658e-01, -1.7789e-01,  3.6443e-01,  7.1633e-02,\n",
      "           3.2484e-01,  2.6883e-01,  1.5154e-01,  1.0634e-02, -1.6984e-01,\n",
      "           1.0080e-01,  5.7725e-01,  5.1762e-02,  1.3896e-02, -1.4720e-01,\n",
      "          -3.3839e-02,  2.3366e-01, -1.9560e-01,  2.1681e-01, -3.7290e-01,\n",
      "           1.2458e-01, -3.9899e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [22]\n",
      "tensor([[[ 2.6613e-02, -4.0237e-01,  8.1656e-02, -3.6267e-01, -2.0889e-01,\n",
      "          -1.8310e-01, -2.7872e-01,  3.5355e-01, -4.3364e-01,  1.8154e-01,\n",
      "          -8.5437e-02, -1.0759e-01, -1.1618e-01, -1.8204e-01,  2.2096e-01,\n",
      "           1.1422e-01,  3.2264e-02, -7.9041e-02, -1.7017e-01,  5.4948e-01,\n",
      "           9.9054e-02,  3.9737e-01],\n",
      "         [ 2.9564e-01, -1.2220e-02, -1.4341e-01, -2.2461e-01,  3.4335e-01,\n",
      "          -1.9298e-01, -4.7960e-01,  1.5529e-01,  2.1752e-01,  5.0575e-01,\n",
      "          -8.4122e-01,  1.3749e-01, -2.4127e-02,  2.5015e-01, -1.2666e-01,\n",
      "           3.9368e-01,  1.6238e-01, -6.9044e-02, -2.8943e-02, -1.5470e-01,\n",
      "          -1.3681e-01,  2.6880e-01],\n",
      "         [ 1.9133e-01,  9.2750e-02, -1.5030e-01, -2.3742e-01, -1.5573e-01,\n",
      "          -6.4388e-01, -1.8126e-01,  6.4787e-02, -2.5750e-02,  3.7543e-01,\n",
      "          -2.0490e-01,  2.6251e-02,  2.9797e-01,  3.4984e-01, -1.5264e-01,\n",
      "           3.0153e-02,  6.7225e-01, -2.3709e-01, -6.0189e-02, -9.8101e-03,\n",
      "          -7.7294e-03,  3.4051e-01],\n",
      "         [ 8.9112e-02, -4.3671e-01, -1.3091e-01, -3.8215e-01,  1.6394e-01,\n",
      "          -4.3621e-01, -2.3260e-01,  9.2101e-02,  1.7033e-01,  2.8233e-01,\n",
      "          -6.7877e-01,  1.5000e-01,  4.0030e-01,  2.2451e-01,  5.2003e-01,\n",
      "           7.3348e-03,  4.0495e-01, -1.6841e-01, -3.0880e-01,  8.5817e-02,\n",
      "          -3.8666e-01,  1.1403e-01],\n",
      "         [-2.5999e-01,  6.0111e-01,  6.8843e-02, -3.6824e-01, -3.0614e-01,\n",
      "          -5.0185e-01, -6.1471e-01, -1.0899e-01,  2.3631e-01,  4.2238e-01,\n",
      "          -6.6799e-01, -4.7095e-02,  2.6709e-01,  1.3992e-01, -7.4419e-02,\n",
      "           1.1963e-01, -2.6961e-01, -6.5026e-02,  4.6688e-02,  1.0192e-01,\n",
      "          -1.4224e-01,  1.2401e-01],\n",
      "         [ 2.5680e-01,  2.5861e-01,  2.8426e-02, -7.1412e-01,  1.5641e-01,\n",
      "          -6.0644e-01, -1.6293e-01,  2.6218e-01,  1.0212e-01,  4.2228e-02,\n",
      "          -1.0518e+00,  8.4049e-02,  4.0754e-01,  1.6774e-01,  1.5959e-02,\n",
      "           8.5998e-03,  3.2691e-01, -3.9567e-01, -3.7826e-02, -9.7152e-03,\n",
      "          -4.7442e-02,  4.3845e-01],\n",
      "         [-1.5400e-01, -1.4782e-01,  1.6160e-01, -2.0729e-01,  3.8760e-02,\n",
      "          -3.9602e-01, -2.0359e-01,  2.0239e-01,  3.9574e-01,  3.7747e-01,\n",
      "          -4.4192e-01,  2.2998e-02,  3.4755e-01,  8.9377e-02,  1.8360e-01,\n",
      "           3.8841e-02,  3.3074e-02, -2.6402e-02, -3.4534e-01, -9.7973e-02,\n",
      "          -1.7233e-01, -2.2328e-01],\n",
      "         [ 1.1442e-01,  5.9408e-01,  1.3569e-01, -4.5085e-01, -8.5905e-02,\n",
      "          -3.2736e-01,  9.2369e-02,  1.6087e-01,  4.6927e-01,  5.6675e-01,\n",
      "          -6.4950e-01,  1.4683e-01,  1.0678e-01,  5.1020e-02,  3.9764e-01,\n",
      "           4.5548e-01,  1.3642e-01, -5.6306e-01,  1.4589e-01, -2.0586e-01,\n",
      "          -5.6182e-01,  2.9488e-01],\n",
      "         [ 3.9437e-01,  7.6751e-02, -1.1218e-01, -4.1750e-01,  1.4280e-01,\n",
      "          -3.1153e-01,  1.2925e-01,  1.5484e-01,  3.5387e-02,  8.9344e-01,\n",
      "          -5.0708e-01,  3.1031e-02,  1.5964e-01,  7.5495e-02,  2.1040e-02,\n",
      "           2.6259e-01,  4.5158e-01, -2.2742e-01,  5.8009e-02, -5.3174e-01,\n",
      "          -2.2473e-01,  2.0613e-01],\n",
      "         [ 3.8351e-01, -1.7920e-01, -3.4276e-01, -5.4087e-01,  6.5258e-02,\n",
      "          -3.9534e-01, -3.6116e-01, -5.9081e-01,  1.7797e-01,  4.0885e-01,\n",
      "          -6.8379e-01,  2.4580e-01, -1.4321e-01,  2.3526e-01,  2.4443e-02,\n",
      "           4.8868e-01,  1.1216e+00, -3.2078e-01, -2.9562e-01, -3.1217e-01,\n",
      "          -6.7357e-02,  3.5521e-01],\n",
      "         [ 5.8582e-01, -3.9260e-01, -3.1594e-01, -3.3130e-01,  3.8604e-01,\n",
      "          -3.7127e-02, -4.7264e-01,  4.9967e-01,  1.0824e-01,  4.8889e-01,\n",
      "          -7.2270e-01, -8.8749e-02, -4.9683e-02,  3.0909e-01, -4.1950e-01,\n",
      "           2.4770e-01,  1.8948e-01, -3.7446e-01,  1.8077e-02,  2.0608e-01,\n",
      "          -1.5053e-01,  1.2996e-01],\n",
      "         [ 3.2316e-01, -4.0064e-01, -3.4704e-01, -5.1326e-01, -6.5316e-02,\n",
      "           8.2202e-02,  1.8162e-01,  9.4066e-02, -1.0448e-01, -7.3077e-02,\n",
      "          -5.4042e-01,  2.8906e-01,  3.2748e-01, -9.5806e-02, -2.5656e-01,\n",
      "           7.0733e-01,  2.3317e-01, -3.7127e-01,  1.8978e-01, -6.3207e-02,\n",
      "           8.1870e-02,  3.4223e-01],\n",
      "         [ 7.5431e-01, -5.9907e-01,  1.4532e-02,  2.4488e-01, -3.5044e-01,\n",
      "          -3.0900e-01, -2.9884e-01,  4.3797e-02,  4.1982e-01,  4.0201e-01,\n",
      "          -7.5467e-01,  1.9228e-01,  6.0840e-01, -2.5539e-01, -2.2910e-01,\n",
      "           3.1690e-01,  2.7498e-01, -4.3659e-02, -8.9754e-02,  4.5674e-01,\n",
      "           5.1157e-02,  5.0069e-01],\n",
      "         [ 2.7002e-01, -3.7231e-01,  1.5918e-01,  1.7216e-02, -1.1898e-01,\n",
      "          -9.0291e-01, -4.1510e-02,  5.3442e-02, -4.9943e-02,  2.6838e-01,\n",
      "          -6.0794e-01,  1.6374e-02,  9.9756e-02, -5.8377e-01,  5.2701e-01,\n",
      "           2.5016e-01,  2.2965e-01, -6.5200e-01,  2.1561e-01, -1.8830e-01,\n",
      "          -6.9637e-01,  5.7964e-01],\n",
      "         [ 7.2105e-02, -4.0188e-01, -2.0724e-01, -6.0771e-01,  9.6640e-02,\n",
      "          -2.3097e-01, -1.3326e-01, -8.8358e-02,  9.1681e-02,  3.3114e-01,\n",
      "          -3.5990e-01, -1.7995e-01, -2.6195e-01, -3.2766e-02,  1.8975e-01,\n",
      "           3.0692e-01,  1.3170e-01, -2.1120e-01, -6.4420e-02, -4.3010e-01,\n",
      "          -4.5828e-01,  4.6904e-01],\n",
      "         [ 1.2332e-01, -2.2542e-01, -2.5777e-01, -5.9590e-01,  3.9176e-01,\n",
      "          -1.6705e-01,  2.1821e-02,  2.7696e-02, -1.9119e-01,  3.9104e-01,\n",
      "          -2.8347e-01,  2.7435e-02,  2.2376e-02,  3.0474e-01,  3.4093e-02,\n",
      "           3.5784e-01,  1.4586e-01, -3.0910e-01, -2.4125e-01,  8.3152e-02,\n",
      "          -3.0276e-01,  3.2730e-01],\n",
      "         [ 6.2244e-01,  1.9058e-01, -3.0242e-01, -3.8656e-01,  1.5461e-01,\n",
      "          -4.7168e-01,  4.7596e-01,  1.5413e-01,  1.2416e-01, -1.5801e-02,\n",
      "          -6.0029e-01,  1.4758e-01,  3.4485e-01,  4.2893e-02,  5.2520e-02,\n",
      "           3.9390e-01,  6.6491e-01, -2.3960e-01, -8.8794e-02, -1.3726e-01,\n",
      "          -1.4117e-01,  5.4376e-02],\n",
      "         [ 3.0847e-01, -2.5748e-01, -4.3689e-01, -2.7584e-01,  3.3577e-02,\n",
      "          -6.4728e-01, -3.5094e-01,  2.2045e-01, -1.3481e-01,  1.6486e-01,\n",
      "          -3.3598e-01, -8.4296e-02,  2.2055e-01, -3.2677e-01, -1.8459e-02,\n",
      "           4.6416e-01,  5.0236e-01, -7.1690e-02,  1.6336e-01, -4.5085e-01,\n",
      "          -2.4090e-02,  7.5985e-02],\n",
      "         [ 6.0480e-01, -8.6407e-02, -2.1237e-02, -5.2868e-01,  5.9069e-02,\n",
      "          -4.2948e-01, -2.9875e-01, -1.9474e-01,  1.7513e-01,  3.7037e-01,\n",
      "          -4.1889e-01,  3.8434e-01,  5.5456e-03,  1.6144e-01,  3.1219e-01,\n",
      "           4.7635e-01,  6.7495e-01, -3.7944e-01, -5.1344e-01, -1.5969e-01,\n",
      "           2.1842e-01,  5.8314e-01],\n",
      "         [ 3.2968e-01, -1.8556e-01, -2.9895e-01, -7.9787e-01,  7.0645e-01,\n",
      "          -2.9300e-01, -5.2893e-02,  2.3063e-02,  1.7542e-01,  5.0968e-01,\n",
      "          -5.0952e-01, -3.0048e-02,  1.1586e-01,  5.7688e-02,  1.9773e-01,\n",
      "           4.2872e-01,  2.4404e-01, -2.9211e-01, -1.5537e-01,  3.1259e-01,\n",
      "          -1.1167e-01,  2.2251e-01],\n",
      "         [ 2.1010e-01,  3.4708e-02,  4.2432e-01, -4.8538e-01,  3.8879e-02,\n",
      "          -2.6457e-01, -7.2114e-02,  4.8062e-01, -4.2571e-01,  5.5351e-01,\n",
      "          -6.1419e-01,  8.8114e-02, -1.5637e-01, -1.9236e-01,  1.1128e-02,\n",
      "           2.7871e-01,  2.4643e-01, -5.8692e-01, -9.5373e-02,  5.3746e-02,\n",
      "          -7.2952e-02,  5.2342e-01],\n",
      "         [ 1.2503e-01, -8.2362e-01,  3.3156e-03, -9.8132e-02, -4.9922e-01,\n",
      "          -1.3408e-01, -4.3095e-01, -3.3761e-03, -4.8205e-01,  4.7602e-01,\n",
      "          -3.3403e-01,  1.2016e-01, -2.9668e-01, -3.4098e-01,  1.8007e-01,\n",
      "           4.4094e-01,  1.6113e-01, -1.6012e-01, -1.2642e-01, -4.3638e-02,\n",
      "          -6.9432e-04,  1.4775e-01],\n",
      "         [ 5.6422e-01, -4.3531e-01, -8.7948e-02, -2.5753e-01, -1.1293e-03,\n",
      "          -4.3475e-01,  1.7210e-02,  7.9605e-02, -1.7320e-02,  1.6697e-01,\n",
      "          -2.2448e-01,  2.2618e-01, -4.8897e-02, -3.8424e-01,  2.7201e-01,\n",
      "           3.9954e-01,  2.2056e-01, -3.6835e-01, -1.7322e-01, -1.5753e-01,\n",
      "           9.1851e-02,  3.7018e-01],\n",
      "         [ 4.6441e-01, -3.7764e-01, -1.8698e-01, -8.3726e-02,  3.4887e-02,\n",
      "          -2.1244e-01, -4.9385e-02, -1.0329e-01, -1.0928e-01,  7.9886e-02,\n",
      "          -2.0215e-01,  1.2592e-01,  2.9622e-02, -2.0297e-01,  1.7437e-01,\n",
      "           1.7994e-01,  1.8376e-01, -1.2386e-01, -1.3482e-01,  1.6535e-01,\n",
      "           7.0155e-02,  3.3602e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [75]\n",
      "tensor([[[ 0.0751, -0.1490,  0.4410,  ...,  0.1529,  0.1888,  0.1664],\n",
      "         [ 0.4171, -1.2644,  0.5353,  ..., -0.3702,  0.4864, -0.4476],\n",
      "         [ 0.5036, -0.6947,  0.3184,  ..., -0.3086,  0.1585, -0.5058],\n",
      "         ...,\n",
      "         [ 0.0663, -0.2398,  0.6380,  ...,  0.3637, -0.0638,  0.0977],\n",
      "         [ 0.3648, -0.2192,  0.1591,  ...,  0.1564, -0.1297, -0.0445],\n",
      "         [ 0.0420, -0.1688,  0.2899,  ...,  0.0196, -0.0991,  0.0095]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [26]\n",
      "tensor([[[-0.0509,  0.4514,  0.0821,  0.0794,  0.1509,  0.0733, -0.0036,\n",
      "          -0.8055, -0.3520, -0.1695,  0.1112, -0.2502,  0.1169, -0.1043,\n",
      "          -0.1590,  0.2027, -0.1047, -0.1358,  0.6409, -0.1856,  0.1371,\n",
      "          -0.1012],\n",
      "         [ 0.2402,  0.2931,  0.3870,  0.1036, -0.0899, -0.1170,  0.5040,\n",
      "          -0.2338,  0.3525,  0.0203, -0.0639,  0.5258, -0.0270,  0.2591,\n",
      "           0.1973,  0.0013, -0.2678, -0.6909, -0.3507, -0.3877,  0.0172,\n",
      "          -0.2682],\n",
      "         [-0.0128,  0.2035,  0.6705,  0.0963,  0.2827,  0.2395,  0.8230,\n",
      "          -0.1812, -0.1816,  0.0366, -0.0726,  0.2640,  0.3116,  0.7077,\n",
      "           0.3031,  0.0078, -0.3437, -0.5028, -0.0481, -0.2857, -0.4667,\n",
      "          -0.0452],\n",
      "         [-0.0679,  0.2813,  0.6462, -0.0934,  0.0505, -0.1169,  0.5216,\n",
      "          -0.2306, -0.0231, -0.0925,  0.0744,  0.1398, -0.0265,  0.1800,\n",
      "           0.3388, -0.5516, -0.1484, -0.4314,  0.1053, -0.2232, -0.3374,\n",
      "           0.1126],\n",
      "         [-0.0405,  0.2021,  0.2538, -0.1408,  0.3316,  0.0987,  0.7376,\n",
      "           0.0951,  0.1915,  0.5848, -0.1643, -0.0576,  0.0255,  0.6349,\n",
      "           0.5701, -0.1044,  0.0870, -0.7601, -0.1107, -0.2487, -0.0918,\n",
      "          -0.4155],\n",
      "         [ 0.1071, -0.0754,  0.4945, -0.0544,  0.1150, -0.1134,  0.3082,\n",
      "          -0.3161,  0.3269,  0.7280,  0.0044,  0.2198,  0.0769,  0.2275,\n",
      "           0.5029,  0.0960,  0.1211, -0.5223, -0.0156, -0.6832, -0.2444,\n",
      "           0.0579],\n",
      "         [ 0.1283, -0.4043,  0.4684, -0.5781, -0.2048,  0.4143,  0.5288,\n",
      "          -0.4208,  0.0728,  0.0866,  0.1313,  0.5456, -0.1149, -0.5139,\n",
      "           0.1890,  0.0234, -0.0326, -0.8969, -0.3118, -0.0813,  0.3707,\n",
      "          -0.3119],\n",
      "         [-0.2945,  0.1033,  0.7241,  0.0692,  0.2517,  0.0852,  0.9620,\n",
      "          -0.7435,  0.0576, -0.0959, -0.8071,  0.6250, -0.4072, -0.5676,\n",
      "           0.6664, -0.5535,  0.0979, -0.0344,  0.1102, -0.3661, -0.3128,\n",
      "          -0.3000],\n",
      "         [-0.4178,  0.4315,  0.6072, -0.4084,  0.3706,  0.5025,  0.5490,\n",
      "          -0.2274,  0.3663,  0.1787, -0.0807,  0.4325,  0.2858,  0.1975,\n",
      "           0.6916,  0.0820, -0.0790, -0.5080,  0.4829, -0.4798, -0.4456,\n",
      "           0.0116],\n",
      "         [-0.5929, -0.1686,  0.6673, -0.2523,  0.0887,  0.2962,  0.6762,\n",
      "          -0.0836, -0.1088,  0.2527, -0.0514,  0.4418, -0.1517,  0.4072,\n",
      "           0.7729,  0.0720,  0.2874, -0.8035,  0.0277, -0.3185, -0.3486,\n",
      "          -0.0150],\n",
      "         [-0.0703,  0.0952,  0.6066, -0.0303, -0.1838,  0.5617,  0.3596,\n",
      "          -0.2863, -0.1928, -0.2317, -0.1417,  0.3833,  0.1881,  0.1162,\n",
      "           0.3112, -0.1567,  0.1325, -0.4448,  0.0490, -0.1438,  0.0507,\n",
      "          -0.0140],\n",
      "         [ 0.0461, -0.0154,  0.2405, -0.0929, -0.0547,  0.0672,  0.1000,\n",
      "          -0.2169,  0.5029,  0.9654,  0.3723, -0.0995,  0.1977,  0.4634,\n",
      "           0.0183,  0.2890,  0.1637, -0.1384,  0.1323, -0.1631, -0.3826,\n",
      "          -0.7183],\n",
      "         [-0.0186, -0.4215, -0.0841, -0.1160, -0.2226,  0.0309,  0.2555,\n",
      "          -0.1971,  0.9363,  0.3282, -0.2674,  0.1330,  0.4151, -0.0226,\n",
      "           0.2611,  0.2477,  0.1331, -0.5039, -0.0469, -0.2311,  0.0015,\n",
      "          -0.1425],\n",
      "         [-0.2670,  0.2809,  0.0339,  0.0127, -0.2764,  0.4304,  0.2911,\n",
      "          -0.4276,  0.3073,  0.2809, -0.1739,  0.2394, -0.2750,  0.0862,\n",
      "           0.6788, -0.3980,  0.0899, -0.3109,  0.1360, -0.1361, -0.0981,\n",
      "          -0.4937],\n",
      "         [-0.3269, -0.1816,  0.0979, -0.1545, -0.1614,  0.7465,  0.4359,\n",
      "          -0.2097, -0.0281,  0.8939, -0.4448,  0.6331, -0.5640,  0.4176,\n",
      "          -0.0694, -0.9250,  0.2843, -0.1708, -0.2890, -0.1823,  0.0718,\n",
      "          -0.4611],\n",
      "         [-0.0552,  0.3879,  0.2561,  0.2166,  0.1413,  1.1973,  0.8047,\n",
      "          -0.8614,  0.1381,  0.3461,  0.0817,  0.2356, -0.3579,  0.1490,\n",
      "           0.4560, -0.2316,  0.1044, -0.5907, -0.0499,  0.0529, -0.2737,\n",
      "          -0.3948],\n",
      "         [ 0.2351,  0.4894,  0.1053, -0.4070, -0.1974,  0.2029,  0.2791,\n",
      "          -0.5206, -0.4033, -0.0927,  0.0020,  0.5080, -0.1731,  0.7835,\n",
      "          -0.0069, -0.2612,  0.0092, -0.1708,  0.0539, -0.2608, -0.0246,\n",
      "          -0.0703],\n",
      "         [-0.1269,  0.5629,  0.5933, -0.0406, -0.4734,  0.3802,  0.6669,\n",
      "          -0.6825, -0.1270,  0.0014,  0.1981,  0.6532, -0.1566, -0.0313,\n",
      "           0.2176,  0.6655,  0.3357, -0.1504,  0.2180, -0.2417, -1.1188,\n",
      "          -0.3186],\n",
      "         [ 0.5573,  0.1053,  0.7483, -0.4760, -0.6637, -0.0111,  0.4384,\n",
      "          -0.0137,  0.2534,  0.1441,  0.0667,  0.3609, -0.3603,  0.4965,\n",
      "           0.7539, -0.1628,  0.5050,  0.0526,  0.1802,  0.5481, -0.2025,\n",
      "          -0.2329],\n",
      "         [ 0.4795,  0.4257,  0.4206,  0.2990,  0.0983, -0.2282,  0.1171,\n",
      "          -0.0581, -0.0117,  0.2195,  0.4348,  0.2208, -0.4771,  0.5048,\n",
      "           0.3566,  0.6914,  0.6223, -0.2623, -0.7610, -0.1144, -0.1977,\n",
      "          -0.1057],\n",
      "         [-0.0274, -0.1343,  0.4640, -0.0776,  0.0455,  0.3804,  0.1718,\n",
      "          -0.3748,  0.5512, -0.0588,  0.2709,  0.6065, -0.5459,  0.3973,\n",
      "           0.0401,  0.4620,  0.4897, -0.2594, -0.6153, -0.2429, -0.2140,\n",
      "          -0.2621],\n",
      "         [ 0.4410,  0.1240,  0.0199,  0.5022,  0.3701,  0.6103,  0.3948,\n",
      "          -0.3935,  0.2201,  0.1374,  0.0680,  0.4485, -0.6387,  0.3671,\n",
      "           0.3693,  0.4371,  0.7925, -0.1893, -0.8970,  0.2052, -0.0447,\n",
      "          -0.0639],\n",
      "         [ 0.7185,  0.2156,  0.5774, -0.0839, -0.2929,  0.0016,  0.0060,\n",
      "          -0.5091,  0.5449, -0.0471,  0.2235,  0.3971, -0.4734,  0.2308,\n",
      "          -0.0538,  0.3782,  0.4593, -0.1191, -0.3262,  0.6559,  0.0400,\n",
      "          -0.2888],\n",
      "         [ 0.8277,  0.5471,  0.5321,  0.1169,  0.0267, -0.1226,  0.3856,\n",
      "          -0.6795, -0.3071,  0.3726,  0.0257,  0.6368, -0.5272,  0.3312,\n",
      "          -0.0910,  0.9475,  0.5259, -0.3044, -0.6618,  0.6603, -0.4849,\n",
      "          -0.1992],\n",
      "         [ 0.5693,  0.2157,  0.5920, -0.5105, -0.0097, -0.3238,  0.2254,\n",
      "          -0.5521, -0.2084,  0.1384, -0.0052,  0.5795, -0.5578,  0.4185,\n",
      "           0.0374, -0.6230,  0.2242, -0.0515,  0.2186,  0.1207,  0.3486,\n",
      "          -0.1938],\n",
      "         [ 0.5199,  0.6984,  0.0061, -0.0836,  0.2273,  0.1100,  0.2208,\n",
      "          -0.6056, -0.4814, -0.4510,  0.1795,  0.3929, -0.8939,  0.0811,\n",
      "           0.3971,  0.5541,  0.4193, -0.2904, -0.4064, -0.2387,  0.1913,\n",
      "          -0.6629],\n",
      "         [ 0.1422,  0.2919,  0.1399, -0.2640, -0.1397,  0.6625,  0.2864,\n",
      "          -0.5568, -0.2788,  0.0488, -0.4023,  0.5903, -0.0498,  0.4533,\n",
      "           0.4568, -0.3638, -0.0120,  0.0675, -0.0321, -0.0519,  0.2427,\n",
      "          -0.6286],\n",
      "         [-0.1893,  0.0244,  0.0523,  0.0364, -0.3001,  0.3063,  0.4202,\n",
      "          -0.3490, -0.2038,  0.2196, -0.3702,  0.3089,  0.0785,  0.1752,\n",
      "           0.2260, -0.1939, -0.0671, -0.4243,  0.0677, -0.3282, -0.3551,\n",
      "          -0.2760]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [43]\n",
      "tensor([[[ 9.6527e-02, -2.0975e-01, -1.4166e-01,  2.2211e-01,  1.9488e-01,\n",
      "          -6.3029e-01,  1.1049e-02, -5.0227e-01, -9.6064e-02,  4.0072e-01,\n",
      "           2.8308e-01,  3.7016e-01, -2.8184e-01,  4.1866e-01, -3.7708e-01,\n",
      "           1.4551e-01,  2.9253e-01,  6.1388e-04, -2.4431e-01,  3.6018e-03,\n",
      "           2.4336e-01,  1.5678e-01],\n",
      "         [-1.9885e-01, -9.8660e-02, -3.9697e-01,  5.2818e-01,  1.6312e-01,\n",
      "           1.9411e-01, -3.7638e-01, -2.5271e-02, -1.4589e-01, -3.5753e-01,\n",
      "          -1.2797e-02,  3.8672e-01,  2.2538e-01, -4.3031e-02, -3.9218e-01,\n",
      "           5.7995e-01, -8.6440e-03, -4.2272e-01, -3.3874e-01,  3.3259e-01,\n",
      "           2.6887e-01,  3.1930e-01],\n",
      "         [-8.0574e-02, -1.7980e-01, -1.3186e-01,  4.7786e-01, -4.7890e-01,\n",
      "           3.6145e-02, -5.9678e-01,  2.0431e-01, -2.8686e-01, -3.2385e-01,\n",
      "           1.4251e-01,  7.6591e-01,  6.9005e-02, -1.3198e-02, -7.4023e-01,\n",
      "           9.6460e-02,  1.5351e-01, -4.2267e-01, -7.3669e-01,  1.5500e-01,\n",
      "           3.0414e-01,  2.8638e-01],\n",
      "         [ 1.3110e-02, -2.2392e-01, -2.5527e-01,  4.1908e-01,  2.0028e-01,\n",
      "          -1.1933e-01, -6.2713e-01,  5.1057e-02, -4.2564e-01, -1.5866e-02,\n",
      "           1.7035e-01,  8.5092e-01,  1.4754e-01,  5.9461e-01, -1.0653e+00,\n",
      "           3.2336e-01,  6.2109e-01, -6.0199e-02, -4.5597e-01,  2.4875e-01,\n",
      "          -5.7460e-02,  1.8438e-02],\n",
      "         [ 1.1307e-01,  6.4737e-04, -4.3263e-01,  6.2406e-01,  3.2521e-02,\n",
      "           2.6419e-01, -2.6498e-01,  2.8930e-02, -6.3118e-02,  6.2460e-02,\n",
      "           3.3126e-01,  3.1746e-01,  1.5436e-01, -3.6721e-02, -8.0075e-01,\n",
      "           2.7609e-01,  4.5321e-01, -2.5523e-01, -3.7759e-01,  2.1134e-01,\n",
      "           1.6662e-01,  2.1787e-01],\n",
      "         [-8.3527e-02, -1.0038e-01, -3.7080e-01,  2.2877e-01,  1.3267e-01,\n",
      "           5.4124e-01, -2.4805e-01, -3.0187e-02, -2.3336e-01,  9.0096e-02,\n",
      "           3.3673e-01,  7.3715e-01,  1.8234e-02, -4.1186e-01, -7.5456e-01,\n",
      "           3.4797e-01,  4.8337e-01, -2.1746e-01, -2.0366e-01, -8.8733e-02,\n",
      "           5.0445e-02, -1.2938e-01],\n",
      "         [ 2.0581e-01, -1.4460e-01, -2.5481e-01,  1.4457e-01,  1.9211e-01,\n",
      "           2.2613e-01, -1.3612e-01, -5.2556e-02, -1.8492e-01,  7.6964e-02,\n",
      "           1.7480e-01,  7.4971e-01, -1.0881e-01, -1.3070e-01, -2.4696e-01,\n",
      "           3.2820e-01,  3.5440e-01,  3.0856e-02, -1.7278e-01, -4.1458e-01,\n",
      "          -2.1663e-01,  1.3392e-01],\n",
      "         [-1.3313e-01, -8.2549e-02, -5.0387e-01,  4.3710e-01,  5.3078e-02,\n",
      "          -3.6074e-01, -5.5739e-01,  5.3968e-02, -1.9629e-02,  2.1469e-01,\n",
      "           4.1745e-02,  1.6616e-01, -2.4065e-01,  4.2814e-02, -4.6665e-01,\n",
      "          -2.0179e-01,  4.9232e-01,  7.2994e-03, -4.5377e-01,  1.6920e-01,\n",
      "          -3.8246e-01, -7.3297e-02],\n",
      "         [ 2.2081e-01, -3.6862e-01, -4.7073e-01,  1.1020e-01, -8.2959e-02,\n",
      "           1.0741e-01, -6.6373e-01,  5.1443e-01, -1.6130e-01, -2.5607e-01,\n",
      "           1.0791e-01,  4.5120e-01,  3.3826e-01,  7.5233e-02, -5.4647e-01,\n",
      "           5.2637e-01,  2.6252e-01, -3.5636e-01, -4.2894e-01,  6.5418e-02,\n",
      "           2.0353e-01,  6.4395e-01],\n",
      "         [ 6.1922e-01, -4.5326e-01, -3.5134e-01, -1.8609e-01, -9.9370e-02,\n",
      "           1.8444e-01, -7.9985e-01, -2.8735e-01, -4.1812e-01, -5.2114e-01,\n",
      "           3.3477e-02,  9.2780e-01,  6.2446e-02, -2.6643e-01, -1.0359e+00,\n",
      "           6.6689e-01,  3.3626e-01, -5.7674e-01, -1.7789e-01, -1.0951e-02,\n",
      "           5.0422e-02,  6.1015e-01],\n",
      "         [ 3.8467e-01, -6.3306e-01, -6.1322e-02,  5.0028e-01,  2.2382e-02,\n",
      "           2.6266e-01, -1.1200e+00,  2.3270e-01, -5.4790e-02, -9.6126e-01,\n",
      "           8.1517e-02,  1.2476e+00, -7.6849e-02, -4.7943e-02, -1.2931e+00,\n",
      "           7.9392e-01,  5.0611e-01, -5.1203e-01, -2.8303e-01,  3.0974e-01,\n",
      "           2.0643e-01,  4.9714e-01],\n",
      "         [-5.9524e-02, -4.1639e-01,  3.1196e-01,  2.7215e-01, -2.3827e-01,\n",
      "           1.4194e-01, -3.4126e-01,  2.9950e-01, -2.2705e-01, -6.0480e-01,\n",
      "           1.3188e-01,  7.4660e-01,  8.0971e-03,  1.7104e-01, -7.5355e-01,\n",
      "           3.6629e-01,  4.1755e-01, -5.4765e-01, -7.1386e-01,  2.4733e-01,\n",
      "           3.6858e-01,  3.3274e-02],\n",
      "         [ 7.1412e-02,  1.0438e-01, -3.8091e-01,  4.7248e-01, -1.0635e-01,\n",
      "          -2.6822e-01, -6.5959e-01, -8.0436e-02, -4.3310e-01, -3.4065e-01,\n",
      "          -1.5473e-01,  4.1752e-01, -2.5617e-01, -1.1093e-02, -3.8788e-01,\n",
      "           1.6204e-01,  6.4688e-01, -3.5216e-01, -3.4567e-01, -2.8404e-01,\n",
      "          -1.5867e-01, -5.7070e-02],\n",
      "         [ 1.0355e-01,  3.2703e-02, -5.9521e-01,  4.4168e-01,  2.3574e-02,\n",
      "           5.3762e-01, -6.9279e-01,  4.7897e-01, -3.4603e-01, -2.0058e-01,\n",
      "           1.4759e-01,  4.2736e-01,  4.7924e-01, -1.7290e-01, -4.9800e-01,\n",
      "           1.8726e-01,  9.7464e-02, -9.1411e-02, -4.6803e-01, -6.6705e-02,\n",
      "           1.6941e-01,  3.6645e-01],\n",
      "         [-1.8154e-01,  2.6577e-02, -3.2997e-01,  4.8053e-01, -3.2345e-01,\n",
      "           2.8031e-01, -2.7472e-01,  2.6126e-01, -2.2559e-01, -3.4805e-01,\n",
      "           2.3667e-01,  4.2125e-01, -6.1261e-02, -2.4079e-01, -6.3731e-01,\n",
      "           2.3239e-01,  1.0380e-01, -1.7501e-01, -7.4917e-01, -1.2448e-01,\n",
      "          -1.6624e-01, -6.1495e-02],\n",
      "         [-2.5599e-01,  9.9398e-03, -1.8216e-01,  2.6071e-01,  1.9676e-01,\n",
      "           1.8797e-01, -1.6819e-01,  1.7947e-05, -1.6396e-01,  9.8650e-02,\n",
      "           3.4377e-01,  1.6939e-01, -4.7340e-02, -2.7775e-01, -2.8430e-01,\n",
      "           6.6566e-01,  1.7973e-01,  1.0261e-01, -3.8139e-01, -6.7570e-01,\n",
      "          -8.6755e-02, -5.5216e-03],\n",
      "         [-2.9965e-01,  1.7697e-02, -5.2496e-01,  4.7476e-01,  5.9907e-02,\n",
      "          -6.6474e-02, -4.9984e-01,  9.6985e-03, -2.0027e-01, -9.0634e-02,\n",
      "          -1.2206e-01,  5.0732e-02, -2.4122e-01,  1.8101e-03, -2.9489e-01,\n",
      "          -4.5899e-01,  4.6939e-01, -5.0277e-01, -2.7072e-01,  1.8261e-01,\n",
      "          -4.4263e-01,  6.5196e-02],\n",
      "         [ 2.3637e-01, -1.1948e-01, -2.2631e-01,  4.1979e-01, -3.1714e-01,\n",
      "          -7.7282e-02, -5.9634e-01,  5.3275e-02, -2.3393e-01, -2.7167e-01,\n",
      "          -8.8319e-02, -8.2822e-02, -1.5415e-01,  1.1781e-01, -5.7970e-01,\n",
      "          -1.5500e-01,  6.1890e-02, -2.8056e-01, -4.5919e-01, -4.0447e-02,\n",
      "          -1.7046e-01, -4.3723e-02],\n",
      "         [ 1.5928e-01, -1.5899e-01, -8.7433e-02, -1.1570e-01,  2.0514e-01,\n",
      "           1.1310e-02, -4.6984e-01, -3.6398e-01,  1.3303e-01,  8.9043e-02,\n",
      "           1.6863e-01,  2.3210e-01, -2.1650e-01,  8.4513e-02, -7.5677e-01,\n",
      "           3.0948e-01,  1.6266e-01, -1.1995e-01, -2.6959e-01,  4.7777e-02,\n",
      "          -3.3198e-01,  1.5272e-01],\n",
      "         [-1.9711e-01, -6.0780e-02, -4.1534e-02,  6.3452e-01,  1.7378e-01,\n",
      "          -1.7325e-01, -1.3364e+00,  9.8090e-02, -2.4830e-01, -1.8568e-01,\n",
      "           1.9450e-01,  3.9075e-01, -2.4353e-01, -5.4557e-02, -5.1198e-01,\n",
      "           2.8487e-03,  2.4100e-02, -4.4747e-01, -4.6785e-02, -4.0971e-02,\n",
      "          -2.2988e-01,  5.8277e-01],\n",
      "         [-7.8339e-02, -3.4686e-01, -2.4014e-01,  4.4935e-01,  2.8635e-01,\n",
      "           1.3419e-01, -6.0125e-01,  2.2608e-02, -7.5949e-02, -2.8996e-01,\n",
      "           2.3869e-01,  2.5855e-01, -5.0300e-03,  1.2310e-01, -7.6511e-01,\n",
      "           2.5994e-01,  2.0309e-01, -3.0587e-01,  2.0461e-01,  4.0383e-02,\n",
      "          -3.8560e-01,  3.6122e-01],\n",
      "         [ 3.3356e-01, -3.5469e-01, -3.2437e-01,  1.3793e-02, -2.1939e-01,\n",
      "          -4.3974e-01, -9.8817e-01,  2.0729e-01, -2.1118e-01,  1.0932e-01,\n",
      "           7.0076e-02,  5.2969e-01, -4.0541e-02,  1.6532e-01, -5.1705e-01,\n",
      "           1.8627e-02, -5.3565e-02, -1.3223e-01, -4.8719e-01,  1.5971e-01,\n",
      "           5.0380e-02,  1.6065e-01],\n",
      "         [-3.7441e-02, -6.2520e-01, -6.3558e-01,  5.4810e-01,  2.6020e-01,\n",
      "          -5.1040e-01, -6.7649e-01,  1.8733e-01, -3.8194e-02, -1.4753e-01,\n",
      "          -1.1978e-01,  7.9847e-01, -2.7721e-01, -9.1077e-02, -5.6253e-01,\n",
      "          -1.3775e-01,  5.2925e-01, -3.1106e-01, -3.0330e-01, -3.5573e-02,\n",
      "          -1.2906e-01, -2.6646e-01],\n",
      "         [ 2.0974e-01, -5.5977e-01, -3.6835e-01,  2.1119e-01, -6.2107e-02,\n",
      "          -2.1551e-01, -9.9719e-01,  6.2706e-03, -3.2342e-02,  3.0429e-03,\n",
      "           1.4031e-01,  3.6666e-01, -2.9020e-02,  1.3683e-01, -2.3401e-01,\n",
      "           2.0547e-01,  5.0402e-01, -1.7923e-01, -7.7802e-01,  6.0869e-02,\n",
      "           5.4219e-02, -1.6151e-01],\n",
      "         [-4.5415e-01, -2.6581e-01, -1.3077e-02,  2.0680e-01, -2.2464e-01,\n",
      "           2.2326e-01, -5.3324e-01, -2.1023e-01, -5.0344e-01,  2.4993e-01,\n",
      "           2.7655e-01,  2.9591e-01,  1.6482e-01, -1.4901e-01, -2.4395e-01,\n",
      "           3.9087e-01,  4.0356e-01, -3.1977e-01, -6.7644e-01,  6.5578e-01,\n",
      "           4.3992e-01,  2.5378e-01],\n",
      "         [-3.3908e-02, -4.9601e-01,  1.5787e-01,  1.7569e-01,  1.0361e-01,\n",
      "          -3.0713e-01, -1.1931e+00,  3.5170e-01, -5.5948e-01, -1.2290e-01,\n",
      "           1.1136e-01,  2.0686e-01, -1.1984e-01,  3.4017e-02, -1.2014e+00,\n",
      "          -2.6259e-01,  2.8705e-01, -3.8184e-01, -4.0906e-01,  5.5876e-01,\n",
      "          -3.3560e-01, -2.8387e-02],\n",
      "         [ 2.3569e-01, -1.6077e-01, -2.1673e-01,  6.6175e-01,  1.4254e-01,\n",
      "           4.0848e-02, -6.8481e-01, -1.0133e-01, -3.1037e-01,  6.7258e-02,\n",
      "           2.8230e-01,  2.2900e-01,  1.2936e-01,  1.8870e-01, -5.1124e-01,\n",
      "           1.7806e-01,  1.9907e-01, -1.6832e-01, -1.5116e-01,  2.0735e-01,\n",
      "          -1.7095e-01,  1.6896e-01],\n",
      "         [ 2.0352e-01, -4.3085e-01,  4.6706e-02,  8.0191e-01, -2.7248e-01,\n",
      "          -7.6547e-02, -7.4274e-01,  3.1049e-01, -7.3305e-02,  2.7921e-01,\n",
      "           1.1928e-01,  7.8050e-01,  1.9943e-01,  8.2623e-02, -7.1385e-01,\n",
      "          -5.7575e-02,  3.2588e-01, -1.3291e-01, -5.4980e-01,  5.6374e-01,\n",
      "          -7.5818e-02, -1.0427e-01],\n",
      "         [ 4.5890e-02, -3.3834e-01,  3.9474e-01,  4.5368e-01, -1.5047e-01,\n",
      "          -1.2127e-01, -6.3470e-01, -1.1828e-01, -1.9879e-01, -1.0564e-01,\n",
      "          -1.0654e-01,  8.7180e-01,  3.1991e-01,  5.2736e-02, -9.2621e-01,\n",
      "           3.0956e-02,  2.5257e-01, -4.5813e-01, -3.3689e-01,  2.5197e-01,\n",
      "          -8.8010e-02, -1.3552e-01],\n",
      "         [ 1.9002e-02,  2.2117e-03, -3.7806e-01,  2.0294e-01,  2.6676e-02,\n",
      "          -2.5433e-01, -8.6053e-01, -1.2307e-01, -1.5000e-01,  3.1593e-02,\n",
      "           6.3773e-02,  6.1684e-02, -4.8925e-01,  3.0035e-01, -6.5786e-01,\n",
      "           1.5893e-03,  2.7291e-01, -2.7208e-01,  1.7532e-01,  2.4723e-01,\n",
      "          -2.2300e-02,  3.1567e-01],\n",
      "         [ 8.3030e-02, -4.8451e-01, -2.5794e-01,  1.7203e-01,  4.1603e-01,\n",
      "          -1.5181e-01, -6.6134e-01,  1.7326e-01, -4.3474e-01, -8.3467e-02,\n",
      "          -2.3024e-01,  7.5908e-01,  1.5140e-01,  2.4209e-02, -6.9580e-01,\n",
      "           2.0156e-02,  1.4047e-01,  5.1434e-02, -1.8168e-01,  1.5790e-01,\n",
      "           3.2591e-01,  7.7365e-01],\n",
      "         [-3.2977e-01, -2.4278e-01, -1.2826e-03,  2.2881e-01,  6.7775e-02,\n",
      "          -2.0076e-01, -5.9795e-01,  5.4047e-01, -4.4414e-01, -1.9336e-03,\n",
      "          -2.3488e-02,  9.3834e-01,  2.2692e-01, -3.3894e-01, -7.2269e-01,\n",
      "          -1.8013e-01,  2.9533e-01,  2.4641e-01, -5.0241e-01,  3.5346e-01,\n",
      "           2.9001e-01,  4.4731e-01],\n",
      "         [-5.9188e-01, -5.1829e-03, -2.0198e-01,  7.8232e-01,  4.3110e-02,\n",
      "           7.6984e-02, -2.6681e-01,  2.6851e-01, -4.2194e-01,  1.5173e-01,\n",
      "          -1.9122e-01,  4.8836e-01,  2.5819e-01, -8.7504e-02, -1.1214e+00,\n",
      "           3.4678e-01,  2.7794e-01, -3.2032e-01, -8.0050e-02,  1.4979e-01,\n",
      "          -5.1199e-01,  3.3411e-01],\n",
      "         [-4.1114e-01, -9.4954e-02, -6.5561e-02,  1.1048e-01,  1.3694e-01,\n",
      "          -4.6790e-04, -7.3597e-01,  1.5479e-01, -3.4730e-01,  1.1145e-01,\n",
      "           3.3211e-01,  6.1541e-01, -7.3008e-02, -2.5415e-01, -7.2450e-01,\n",
      "           4.6940e-01, -2.2922e-01, -1.1795e-01, -4.0576e-01,  1.4568e-01,\n",
      "           5.5193e-01,  2.2331e-01],\n",
      "         [ 1.2649e-01, -2.6164e-01, -4.8508e-01,  4.4850e-02,  3.7288e-01,\n",
      "          -2.2832e-01, -8.1733e-01, -1.4714e-02, -7.0939e-02, -3.6004e-02,\n",
      "          -2.7206e-01,  4.0138e-01,  2.4762e-01, -3.2206e-01, -7.9224e-01,\n",
      "          -9.9799e-02,  3.5466e-01, -3.8639e-01, -3.8397e-01,  1.9493e-01,\n",
      "           6.0309e-02,  5.4684e-02],\n",
      "         [ 2.5467e-01, -1.9059e-01, -4.6633e-01,  1.2840e-01, -2.1996e-01,\n",
      "          -6.5820e-02, -8.3684e-01, -1.1341e-01, -3.2607e-01,  1.1661e-01,\n",
      "          -2.4707e-01,  4.8030e-01,  1.0415e-01,  7.7001e-02, -7.0786e-01,\n",
      "          -3.7799e-02, -2.5331e-02, -3.0049e-01, -5.4145e-01,  4.1907e-01,\n",
      "           1.7366e-01,  2.4803e-01],\n",
      "         [-3.0197e-02, -2.5450e-01, -4.5402e-01,  7.5880e-01,  3.8938e-02,\n",
      "          -8.1334e-01, -5.9770e-01,  1.6338e-01, -5.4215e-01, -1.4081e-01,\n",
      "          -3.1473e-02,  3.4499e-01, -1.7267e-01, -2.0361e-01, -3.8612e-01,\n",
      "          -3.2240e-01,  6.4075e-01, -1.9450e-01, -4.1051e-01, -1.3896e-01,\n",
      "          -2.2726e-01,  2.5107e-01],\n",
      "         [ 7.7535e-01, -2.8412e-01, -2.0295e-01,  4.1502e-01,  4.0499e-02,\n",
      "          -3.4672e-01, -6.6843e-01, -1.1600e-01, -2.1543e-01,  1.7554e-01,\n",
      "           3.4662e-01,  1.1802e-01, -2.8318e-01, -1.5394e-01, -1.4306e-01,\n",
      "           1.4524e-02,  1.2690e-01,  1.2438e-02, -7.1608e-01,  1.9928e-01,\n",
      "          -3.4869e-01,  1.1334e-01],\n",
      "         [-1.3222e-02, -7.6553e-01,  4.0990e-02,  3.6912e-01,  3.7088e-01,\n",
      "          -1.3706e-01, -6.3618e-01, -1.6128e-01, -1.4029e-01,  1.4488e-01,\n",
      "           6.6757e-02,  5.6834e-01, -5.7945e-01, -1.3663e-01, -6.3178e-01,\n",
      "           4.3125e-01,  5.1224e-01, -5.3025e-01, -5.2795e-01,  8.8331e-02,\n",
      "          -5.4687e-03, -2.8369e-01],\n",
      "         [ 2.3470e-01, -3.0402e-01, -2.0997e-01,  6.5407e-01,  2.5213e-02,\n",
      "          -4.9707e-01, -7.4575e-01,  2.9772e-01,  1.4859e-02,  1.7082e-01,\n",
      "           9.1732e-02,  2.1871e-01, -2.1425e-01, -2.9667e-01, -7.1428e-01,\n",
      "           3.3051e-01,  2.8373e-01, -1.3558e-01, -3.8165e-01,  5.2330e-01,\n",
      "          -2.3772e-01, -2.0294e-01],\n",
      "         [-3.6405e-02, -2.3601e-01, -3.3921e-02,  7.1703e-01, -4.7717e-02,\n",
      "          -1.0512e-01, -4.4379e-01, -4.3012e-02, -1.0384e-01, -8.4250e-02,\n",
      "           3.5728e-01,  3.5386e-01, -6.3415e-02, -4.1555e-01, -5.2462e-01,\n",
      "           3.2342e-01,  4.6860e-02, -1.1193e-01, -3.1186e-01, -1.4485e-01,\n",
      "           4.4377e-01, -5.9991e-02],\n",
      "         [-1.1136e-01,  3.8369e-02, -2.3213e-01,  4.5032e-01,  1.0567e-02,\n",
      "          -5.6296e-02, -3.8072e-01, -2.7905e-01, -3.6646e-02,  2.7144e-02,\n",
      "          -4.6261e-03,  3.4805e-01, -2.8584e-01,  8.2628e-02, -4.2501e-01,\n",
      "           2.5695e-01,  8.3534e-03, -4.8400e-01, -2.6498e-01,  1.0468e-01,\n",
      "          -1.4517e-01,  2.3763e-02],\n",
      "         [ 8.5162e-02, -5.1963e-01, -2.8877e-01,  8.8457e-01, -4.4346e-02,\n",
      "          -2.5030e-01, -3.1370e-01, -3.6133e-01, -3.1778e-01,  1.7097e-01,\n",
      "          -1.1460e-01,  1.6875e-01,  1.7733e-02,  3.3643e-01, -2.2049e-01,\n",
      "           5.1176e-01,  1.8067e-01, -1.6396e-01, -4.5574e-01,  2.6942e-01,\n",
      "           2.5540e-01, -9.1038e-03],\n",
      "         [ 1.4087e-01,  6.6549e-03, -5.6912e-02,  2.9803e-01, -9.1526e-02,\n",
      "          -3.2840e-01, -3.9829e-01, -1.0766e-01, -3.5903e-01,  2.9722e-01,\n",
      "           1.9544e-02,  4.3054e-02, -1.8816e-02,  1.7098e-01, -3.6793e-01,\n",
      "           3.0361e-01,  4.2388e-01, -1.1674e-01, -1.7817e-01,  3.9957e-01,\n",
      "           3.7622e-01, -5.7233e-02],\n",
      "         [ 2.7822e-01,  2.7789e-02,  1.0253e-02,  3.9309e-01, -7.7440e-02,\n",
      "          -2.8239e-01, -5.7130e-01,  2.1065e-02, -2.0792e-01,  1.1469e-01,\n",
      "           2.9793e-02,  5.9276e-02, -3.6530e-02,  8.7741e-02, -2.7623e-01,\n",
      "           2.1728e-01,  2.8687e-01,  1.8320e-01, -2.9595e-01,  2.8511e-01,\n",
      "           1.8038e-01,  8.5019e-02]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [58]\n",
      "tensor([[[-0.1901,  0.0498, -0.1963,  ..., -0.2477, -0.0398,  0.0656],\n",
      "         [ 0.5907, -0.4730,  0.0684,  ..., -0.4246, -0.1383, -0.5671],\n",
      "         [ 0.2654, -0.7223,  0.3312,  ..., -0.4448, -0.3849,  0.0070],\n",
      "         ...,\n",
      "         [ 0.5143, -0.1560,  0.1298,  ...,  0.0146, -0.1891,  0.0957],\n",
      "         [ 0.5065, -0.0544, -0.2512,  ...,  0.0149,  0.2210, -0.0999],\n",
      "         [ 0.5979, -0.3742, -0.0590,  ..., -0.0281, -0.0027, -0.1298]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [53]\n",
      "tensor([[[ 0.0867,  0.0038,  0.4118,  ..., -0.4011, -0.4509, -0.4728],\n",
      "         [ 0.2459, -0.1362,  0.2974,  ..., -0.5787,  0.2715, -0.2619],\n",
      "         [-0.0812, -0.1105,  0.6951,  ..., -0.1331, -0.0697, -0.3465],\n",
      "         ...,\n",
      "         [-0.4827, -0.1847, -0.1600,  ..., -0.1549, -0.2658, -0.0905],\n",
      "         [-0.2865, -0.5556,  0.1225,  ..., -0.1855, -0.1764, -0.1650],\n",
      "         [ 0.0283, -0.4465,  0.0916,  ...,  0.0076, -0.0685, -0.3324]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [50]\n",
      "tensor([[[ 0.1368, -0.1676,  0.5270,  ...,  0.0520, -0.2174, -0.0478],\n",
      "         [-0.1096,  0.3723,  0.2551,  ..., -0.2036, -0.3552,  0.0795],\n",
      "         [ 0.1484,  0.2112, -0.4174,  ..., -0.1144, -0.3157,  0.1319],\n",
      "         ...,\n",
      "         [-0.1346,  0.0772, -0.1147,  ...,  0.1033, -0.5021, -0.1254],\n",
      "         [-0.0118,  0.1462,  0.1182,  ..., -0.0666, -0.1459, -0.2837],\n",
      "         [-0.0671,  0.0105, -0.0147,  ..., -0.0338, -0.0899, -0.2363]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "each_len_list [35]\n",
      "tensor([[[-1.9563e-01, -1.6825e-01, -3.6238e-01, -9.8019e-02,  8.8888e-02,\n",
      "          -1.2440e-01,  9.0008e-02, -1.8990e-01,  2.3981e-01, -1.9881e-01,\n",
      "          -5.7670e-02,  1.4406e-01, -1.8169e-01,  1.7160e-01,  8.3139e-02,\n",
      "           2.7096e-01,  8.6410e-03, -8.6737e-02, -3.0102e-01,  2.5438e-02,\n",
      "          -1.1919e-01,  3.1434e-02],\n",
      "         [ 2.1523e-02, -5.5455e-01, -1.8546e-02, -1.8992e-01, -3.9421e-01,\n",
      "          -3.4445e-02, -7.3829e-02,  2.6076e-01,  2.1945e-01,  2.4545e-01,\n",
      "           1.1250e-01,  2.8409e-01, -4.5488e-01,  5.8123e-01, -7.0896e-03,\n",
      "           3.5555e-01, -7.3519e-01, -2.2481e-01,  1.0804e-01,  2.0412e-02,\n",
      "          -4.0824e-01, -1.7853e-01],\n",
      "         [ 1.4785e-01, -4.6489e-01, -2.3664e-01,  3.4918e-02, -2.9786e-01,\n",
      "           1.0533e-01, -1.9902e-01,  1.7012e-01,  9.8169e-02,  5.9208e-01,\n",
      "           1.4022e-01,  5.9457e-02, -5.1165e-01,  6.7809e-01,  1.5266e-01,\n",
      "           2.0760e-01, -3.9394e-01,  3.3824e-03, -3.9306e-02, -4.7847e-01,\n",
      "           2.9201e-01,  2.4904e-01],\n",
      "         [ 5.6222e-01, -4.7196e-01, -3.9557e-01,  1.2775e-01, -3.4747e-01,\n",
      "           4.1850e-02,  3.2782e-01, -2.2499e-01,  3.0212e-01, -1.8794e-01,\n",
      "           6.3132e-01,  2.4256e-01, -1.0073e-01,  6.2786e-01, -2.1607e-01,\n",
      "          -1.2656e-01, -3.1254e-01,  1.1257e-01,  1.1354e-02, -3.1352e-01,\n",
      "          -1.6220e-01,  4.3197e-01],\n",
      "         [ 1.9170e-01, -5.3544e-01, -5.5153e-01, -2.4062e-02, -3.2060e-01,\n",
      "           1.9582e-01, -3.6201e-01, -1.6055e-01,  3.5788e-01,  4.0518e-01,\n",
      "           3.5437e-01, -1.4893e-01, -4.2312e-01,  8.2670e-01,  4.3555e-02,\n",
      "          -1.4786e-02, -5.5645e-01, -1.1976e-02,  2.7726e-01, -5.8385e-01,\n",
      "          -3.0493e-01,  5.6461e-01],\n",
      "         [ 1.0383e-01, -8.8430e-01, -2.2204e-01,  2.1673e-01, -1.3145e-01,\n",
      "          -2.2491e-02,  1.4272e-01,  6.3546e-02,  1.7061e-04,  4.6885e-01,\n",
      "           2.4749e-01,  2.4164e-01, -9.4947e-03,  6.4077e-01,  4.0914e-01,\n",
      "          -2.1153e-01, -2.1403e-01, -1.7684e-01, -1.3755e-02, -3.3601e-01,\n",
      "           2.2191e-01,  1.9190e-01],\n",
      "         [-1.1289e-01, -3.4014e-01, -5.2723e-01, -2.5309e-01,  1.0819e-01,\n",
      "           4.9687e-01, -1.2510e-01,  2.3677e-03,  4.1384e-01,  5.4583e-01,\n",
      "           2.2410e-01, -2.7746e-01, -4.7350e-02,  7.1210e-01, -1.3622e-01,\n",
      "          -3.0126e-02, -3.3165e-01, -3.0928e-01,  3.2605e-01, -4.4677e-01,\n",
      "          -9.3561e-02, -8.2539e-02],\n",
      "         [ 4.0301e-01, -6.6116e-01,  2.8106e-02,  1.1826e-01, -2.1315e-01,\n",
      "          -1.0351e-02, -1.4727e-01, -1.9147e-01,  3.2045e-01,  5.8738e-01,\n",
      "           3.3045e-02,  1.5259e-01, -2.8967e-01,  1.5881e-01,  9.2718e-02,\n",
      "           7.7335e-02, -1.7393e-01, -1.5626e-01,  1.2701e-01, -5.1702e-01,\n",
      "           2.6815e-01, -4.3915e-02],\n",
      "         [ 3.0441e-01, -2.5016e-01,  2.2326e-01, -6.5961e-01, -4.2096e-01,\n",
      "           2.5148e-01, -4.0761e-01, -6.1131e-01,  4.7817e-01,  5.8515e-01,\n",
      "           5.4005e-01,  1.1645e-01, -6.9727e-01,  4.5824e-01,  6.1123e-02,\n",
      "           7.0291e-02, -1.3961e-01,  2.1954e-01, -3.4161e-01, -5.9770e-01,\n",
      "          -1.2444e-02,  1.0015e-01],\n",
      "         [ 3.5426e-01, -1.2608e-01, -1.2613e-01, -4.9708e-01,  6.6885e-02,\n",
      "           1.0258e-01, -4.6092e-03,  5.2370e-02,  5.1741e-01,  4.9311e-01,\n",
      "           1.6955e-01, -1.5707e-01, -2.8880e-01,  8.8415e-01,  3.0113e-01,\n",
      "          -5.2072e-02, -5.0790e-02,  8.7860e-02,  7.8258e-02, -5.9905e-01,\n",
      "           3.0854e-01,  2.1976e-01],\n",
      "         [-4.4511e-02, -5.3089e-01,  3.8879e-01, -3.0675e-01,  3.3061e-01,\n",
      "           3.8834e-01, -4.3698e-02,  6.2772e-01,  6.8856e-01,  7.4145e-01,\n",
      "           2.7669e-01,  7.4068e-02, -4.4930e-01,  7.0175e-01,  3.5989e-01,\n",
      "           1.2034e-01, -4.0397e-01,  3.6647e-02, -2.0863e-03, -1.8100e-01,\n",
      "           2.2909e-01,  3.4367e-01],\n",
      "         [ 3.0359e-01, -4.5769e-01, -2.2975e-01, -4.7405e-01, -9.6445e-02,\n",
      "           3.2889e-01, -4.7274e-01, -1.6058e-01,  6.3036e-01,  5.5756e-01,\n",
      "           2.7959e-01,  4.1870e-01, -4.8709e-01,  3.4701e-01,  4.7532e-01,\n",
      "          -2.9403e-01, -2.6194e-01,  1.2116e-01, -2.6683e-01, -3.4455e-01,\n",
      "           8.5140e-03, -2.2736e-01],\n",
      "         [ 3.5550e-01, -2.5436e-01, -3.4733e-01, -7.9026e-01,  3.6752e-01,\n",
      "           5.9228e-01, -1.7686e-01, -4.6185e-01,  7.0873e-01,  7.7913e-02,\n",
      "           9.5816e-02, -1.7561e-01, -2.0040e-01,  4.0224e-01,  3.7234e-01,\n",
      "          -4.5581e-01, -2.7823e-01, -4.0037e-02,  6.8452e-03, -2.6836e-01,\n",
      "           7.0028e-02,  7.5358e-02],\n",
      "         [ 6.7122e-01, -2.6497e-01,  1.7917e-01, -2.2250e-01, -6.5357e-02,\n",
      "           4.3311e-01,  2.0930e-01,  2.5464e-01,  2.8627e-01,  4.2526e-01,\n",
      "           1.2930e-01, -2.2029e-01, -1.0574e-01,  1.6907e-01, -2.3228e-01,\n",
      "          -2.2736e-01,  1.6943e-01,  2.2982e-01, -9.2832e-02, -3.5236e-01,\n",
      "           1.0127e-01,  6.1772e-02],\n",
      "         [ 2.5845e-01, -3.2924e-01,  3.1545e-01,  8.0288e-02, -4.2141e-01,\n",
      "           5.7392e-01, -4.9958e-01,  3.7815e-01, -1.2876e-02, -1.2870e-01,\n",
      "           2.7268e-02,  2.4983e-01, -7.4451e-01,  2.5615e-01,  2.7954e-02,\n",
      "          -2.2057e-02, -1.3952e-01,  3.6195e-01, -8.2146e-02, -6.0208e-01,\n",
      "           7.5717e-02, -2.8669e-01],\n",
      "         [ 1.0105e-01, -8.3412e-02,  4.9079e-02,  1.2081e-01, -4.1037e-01,\n",
      "           5.4603e-01, -2.7572e-01,  2.3155e-01,  4.5031e-01, -1.2091e-01,\n",
      "           5.5218e-01, -1.5752e-01, -7.2091e-01,  3.1244e-01,  2.6503e-01,\n",
      "          -1.0694e-01, -2.2748e-01, -3.2299e-01, -6.2542e-02, -3.5764e-01,\n",
      "           4.2732e-02, -1.0234e-03],\n",
      "         [ 2.6908e-02, -3.0855e-02, -6.1089e-01, -9.5989e-02, -2.2516e-01,\n",
      "           5.1493e-01,  1.4998e-01, -1.4683e-01,  2.6929e-01,  3.5966e-01,\n",
      "          -1.6106e-01, -3.1265e-01, -5.1447e-01,  6.8123e-01,  1.3600e-01,\n",
      "          -1.7403e-02,  1.4612e-01, -9.4483e-02, -2.3258e-01, -5.3777e-01,\n",
      "           6.4994e-02, -1.6926e-01],\n",
      "         [ 1.1567e-02, -2.6265e-01,  4.8957e-02, -2.4974e-01, -2.3972e-01,\n",
      "           1.9208e-01,  4.4554e-02,  3.0769e-01,  5.4096e-01, -3.0423e-02,\n",
      "           1.2579e-01,  7.7156e-02, -1.9818e-03,  4.2459e-01,  1.3813e-01,\n",
      "          -3.1407e-02,  5.9728e-03,  3.2822e-01, -1.7983e-01, -6.9127e-01,\n",
      "           1.3536e-01,  3.0413e-01],\n",
      "         [-3.0217e-02, -2.9749e-01,  1.6309e-01, -2.6483e-01,  3.6879e-02,\n",
      "           1.4414e-01, -2.4758e-02,  5.8874e-01,  1.9594e-01,  3.0930e-01,\n",
      "           2.4919e-02,  2.1912e-03, -8.7537e-02, -7.1753e-02, -1.6881e-01,\n",
      "           1.1728e-01, -4.8360e-01, -7.3166e-02, -1.2841e-01, -3.3267e-01,\n",
      "          -5.1180e-02, -1.6853e-01],\n",
      "         [ 1.2942e-01, -1.9460e-01, -4.6699e-01, -2.2980e-01, -2.3423e-01,\n",
      "           4.1903e-01, -3.5869e-01, -3.4347e-02,  1.1046e-01, -1.0636e-01,\n",
      "           1.6009e-01, -4.2450e-01, -5.0299e-01,  5.6238e-01,  2.1339e-01,\n",
      "          -1.2650e-01, -2.0616e-01,  4.1649e-01, -9.1916e-02, -2.2662e-01,\n",
      "           7.9398e-02, -2.0641e-01],\n",
      "         [ 4.3820e-01, -2.4717e-01,  1.3769e-02, -1.8661e-01, -3.6827e-01,\n",
      "           2.0454e-02, -7.9849e-01,  1.9347e-01,  1.2713e-01,  3.7631e-02,\n",
      "           4.5762e-01,  6.1341e-03, -4.5109e-01,  3.9888e-02,  1.9885e-01,\n",
      "           1.6864e-01, -5.1182e-01,  2.9405e-01,  3.1778e-01, -1.2872e-01,\n",
      "          -3.0183e-01, -2.3605e-01],\n",
      "         [ 1.4168e-01, -4.4743e-01, -3.9645e-01,  4.2268e-02, -1.4292e-01,\n",
      "          -2.4267e-02, -9.1058e-02,  1.0776e-02,  3.5267e-01,  3.6189e-01,\n",
      "           1.4108e-02, -8.9556e-02,  6.2814e-02,  5.5420e-01,  4.9340e-02,\n",
      "          -1.3585e-03, -4.4003e-01,  6.3060e-02, -1.2997e-01, -2.6097e-01,\n",
      "          -6.6664e-02, -1.5420e-01],\n",
      "         [ 3.5392e-01, -6.5544e-01, -2.7010e-01, -3.9854e-01, -4.7388e-01,\n",
      "           2.1519e-01, -1.6977e-01,  7.6283e-02,  1.1582e-01,  4.8990e-01,\n",
      "           3.2221e-01,  6.5023e-01, -1.1857e-01,  3.1976e-01,  8.9142e-02,\n",
      "           8.4265e-02,  7.2151e-02, -2.2277e-01, -2.8592e-02, -2.9273e-01,\n",
      "          -2.2014e-01, -1.2132e-02],\n",
      "         [ 1.7832e-01, -4.2503e-01, -4.2035e-01, -1.9312e-01, -4.7843e-01,\n",
      "           3.8213e-01, -5.8323e-01,  2.4651e-01,  7.2942e-01,  3.4809e-01,\n",
      "           3.9320e-02, -9.5955e-02,  1.8084e-01,  5.8921e-01, -8.6345e-02,\n",
      "           1.2645e-01,  1.5999e-01,  3.6178e-02,  1.8048e-01, -1.0579e-01,\n",
      "          -1.9709e-01, -2.6791e-01],\n",
      "         [ 4.1053e-01, -4.2566e-01, -1.8380e-01, -1.9620e-02, -1.7987e-01,\n",
      "           2.9184e-02, -6.1282e-01, -2.3514e-01,  3.9485e-01,  5.4999e-01,\n",
      "          -2.5392e-01,  1.8230e-01, -2.9843e-01,  9.5293e-01, -2.8542e-01,\n",
      "           2.0862e-01, -5.5496e-01, -3.2462e-01,  1.1057e-01, -2.3706e-02,\n",
      "           3.2166e-01, -2.6064e-01],\n",
      "         [ 4.9788e-01, -3.8356e-01, -2.0258e-01, -4.1050e-01, -1.0323e-01,\n",
      "          -3.0606e-01,  2.1652e-01, -3.5005e-01,  7.1045e-01,  3.7145e-01,\n",
      "          -5.6481e-03, -2.8058e-01, -1.1294e-01,  5.8924e-01,  2.1494e-01,\n",
      "           3.9830e-02, -5.8551e-01, -3.6881e-02, -1.6521e-01,  2.2035e-01,\n",
      "          -1.9801e-01, -3.4167e-01],\n",
      "         [ 2.9389e-01, -1.2171e-01,  2.5908e-01, -6.5620e-01,  2.6739e-02,\n",
      "          -9.6364e-02, -3.2072e-02, -1.0438e-01,  6.1815e-01,  9.3271e-02,\n",
      "          -1.7750e-02,  1.9260e-01, -7.5983e-01,  2.2341e-01,  4.0599e-02,\n",
      "           4.6638e-01, -1.0689e-01,  2.8572e-01, -4.9768e-01, -3.7952e-02,\n",
      "           2.6747e-01,  8.1402e-02],\n",
      "         [ 2.9168e-01, -5.5336e-01,  1.0468e-01, -5.9695e-01, -2.6039e-01,\n",
      "           5.4401e-01, -7.9076e-02, -6.5475e-02,  7.0533e-01,  3.4155e-02,\n",
      "           2.1819e-01, -1.5360e-03,  7.3775e-02,  1.1783e-01,  2.7311e-01,\n",
      "           2.0731e-01, -4.0148e-01, -4.7190e-02, -2.0384e-01, -2.6343e-01,\n",
      "           7.3422e-02,  2.8722e-01],\n",
      "         [ 3.8079e-01, -6.5633e-01,  3.9214e-02, -5.2991e-01, -6.2658e-01,\n",
      "           3.2046e-01, -1.0221e-01, -3.8051e-01,  7.8612e-01,  5.8841e-01,\n",
      "           6.7261e-01,  3.4592e-01, -5.5181e-01,  5.1044e-01,  2.4425e-01,\n",
      "          -4.5302e-02, -2.7087e-01,  8.5096e-02, -4.0743e-01, -6.7243e-01,\n",
      "           1.9444e-01,  9.3940e-02],\n",
      "         [ 4.0608e-01, -3.6932e-01, -4.0635e-01, -4.2088e-01,  6.0098e-02,\n",
      "           1.1582e-01, -2.0101e-01,  1.1635e-02,  8.3148e-01,  4.8616e-01,\n",
      "           1.5152e-01, -6.8900e-02, -9.5783e-02,  7.1603e-01,  4.6021e-01,\n",
      "          -3.0845e-01,  2.1304e-02,  1.8238e-01,  2.3326e-02, -6.3864e-01,\n",
      "           4.1354e-01, -1.7712e-02],\n",
      "         [-3.4709e-02, -6.2322e-01,  8.4549e-02, -3.8478e-01,  2.0225e-01,\n",
      "           1.7284e-01, -1.2572e-02,  6.3232e-01,  8.9613e-01,  7.7551e-01,\n",
      "           1.9001e-01,  1.9575e-01, -2.3641e-01,  6.4941e-01,  3.5344e-01,\n",
      "           1.2846e-01, -6.0537e-01, -1.8759e-01,  1.1726e-02, -1.7174e-01,\n",
      "           1.8775e-01,  4.2364e-01],\n",
      "         [ 3.4189e-01, -3.4042e-01,  3.3844e-02, -7.5444e-01,  1.3005e-01,\n",
      "           3.6009e-01, -2.2113e-01,  1.8244e-01,  6.7725e-01,  7.4855e-01,\n",
      "           2.3916e-01,  5.9420e-01, -2.0550e-01,  1.2706e-01,  4.7479e-01,\n",
      "          -4.8623e-01, -1.2728e-01, -5.0420e-02, -1.1266e-01, -2.2543e-01,\n",
      "           2.6633e-01, -2.3624e-01],\n",
      "         [ 4.0766e-01, -2.2353e-01, -3.2912e-01, -8.9454e-01, -6.8348e-02,\n",
      "           2.7770e-01, -9.2143e-02, -9.7180e-02,  6.6318e-01,  1.7255e-01,\n",
      "           6.2990e-02, -3.5211e-01,  8.5664e-02,  4.7099e-01,  3.9275e-01,\n",
      "          -5.0360e-01, -2.6429e-01, -1.0307e-01,  2.6643e-01, -3.6078e-01,\n",
      "           1.6828e-01,  2.2782e-01],\n",
      "         [ 5.5792e-01, -4.0973e-01,  1.9216e-01,  7.4979e-02, -2.1024e-01,\n",
      "           3.2557e-01,  8.4834e-02,  4.8867e-02,  3.5253e-01,  4.0171e-01,\n",
      "           1.7142e-01,  9.4118e-02,  1.2282e-02,  6.7374e-02,  4.6198e-01,\n",
      "          -2.7677e-02, -4.1860e-01,  3.8186e-02,  1.1141e-01, -2.7618e-02,\n",
      "          -7.7934e-04, -2.0054e-01],\n",
      "         [ 4.4151e-01, -3.4680e-01, -1.2991e-01, -2.0268e-02, -4.6392e-01,\n",
      "          -5.7732e-03, -4.4182e-01,  4.0731e-02,  5.0091e-01,  1.9446e-01,\n",
      "           2.5420e-01, -1.7996e-01,  3.5006e-02,  1.5883e-01,  9.6702e-02,\n",
      "           4.8261e-01, -1.1502e-01,  6.4821e-02,  1.3300e-01, -1.0748e-02,\n",
      "           1.3666e-01,  4.7472e-02],\n",
      "         [ 4.4198e-01, -1.2981e-01, -3.5661e-01, -1.9734e-02, -4.7644e-02,\n",
      "           1.0743e-01, -3.7389e-01,  2.8246e-03,  4.8666e-01,  3.1458e-01,\n",
      "           2.4966e-01, -1.4563e-01, -2.2770e-01,  4.8135e-01,  2.1099e-01,\n",
      "           3.4895e-01, -1.2777e-01,  1.0495e-01,  9.3279e-03, -1.2668e-01,\n",
      "           1.1458e-01, -7.7500e-02],\n",
      "         [ 2.7024e-01, -3.6469e-01,  1.3931e-01, -1.7543e-01, -3.3284e-01,\n",
      "           1.1126e-01, -1.3036e-01,  7.5472e-02,  3.8454e-01,  6.0095e-02,\n",
      "           2.3937e-01,  8.9184e-02, -1.2413e-01,  3.8164e-01,  1.9251e-01,\n",
      "           3.8324e-01, -4.3408e-01, -1.4972e-01, -1.3820e-01,  9.1599e-02,\n",
      "          -5.3415e-02, -2.5114e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [28]\n",
      "tensor([[[ 8.7290e-02,  8.2739e-01, -4.4699e-03,  2.9086e-01,  3.6424e-01,\n",
      "          -4.1247e-01, -1.4672e-01, -1.7884e-01,  3.7599e-01,  1.2235e-01,\n",
      "          -1.4522e-01, -2.6824e-01, -4.1874e-02, -2.2125e-01, -2.0721e-01,\n",
      "          -7.1558e-02, -4.4967e-01, -1.1902e-02, -7.5899e-02, -2.4998e-02,\n",
      "           5.2630e-02, -4.2798e-01],\n",
      "         [-1.9595e-01,  1.8947e-01, -1.0674e-01, -1.3988e-01,  7.5531e-03,\n",
      "          -5.7743e-01,  1.9122e-01, -3.5203e-01, -4.5323e-01,  2.2755e-01,\n",
      "          -1.0648e-02,  2.7283e-01, -1.7128e-01, -1.0500e-01, -2.3492e-02,\n",
      "          -1.1774e-01, -1.2812e-01, -1.4334e-01, -2.0199e-02,  1.8265e-01,\n",
      "          -2.1416e-02, -2.8318e-01],\n",
      "         [-4.3497e-01,  2.2472e-01, -2.7653e-01,  1.7793e-01, -4.4905e-01,\n",
      "          -1.0437e+00,  4.1037e-01,  2.3864e-01,  1.0969e-01,  3.5703e-01,\n",
      "          -2.8886e-01, -1.8062e-01, -5.9144e-01,  4.1530e-03, -2.0428e-01,\n",
      "          -2.1101e-01, -2.3922e-01, -1.6943e-01,  2.9001e-01, -1.2341e-01,\n",
      "          -2.1783e-03,  1.9445e-01],\n",
      "         [-3.0976e-01,  8.8734e-02, -4.6711e-02,  2.5491e-01, -3.6130e-02,\n",
      "          -7.9204e-01, -1.4306e-01,  9.9256e-03, -5.5743e-01,  7.5316e-02,\n",
      "           1.5447e-01,  1.9436e-01, -4.9044e-01, -1.5843e-01, -3.7635e-01,\n",
      "          -3.9522e-01,  3.0383e-04, -5.6214e-02,  3.4012e-01,  1.7876e-01,\n",
      "           5.2166e-02, -1.3813e-01],\n",
      "         [-1.9640e-01, -1.6891e-01, -3.0382e-01, -4.6048e-01, -1.7909e-01,\n",
      "          -5.0562e-01,  3.9111e-02, -5.2775e-01, -3.0476e-01, -1.7801e-01,\n",
      "           3.2652e-01,  1.5466e-01, -5.8331e-01, -2.3255e-01, -3.4419e-01,\n",
      "          -3.2928e-01,  1.0904e-01,  2.5967e-02,  1.2863e-01, -1.7343e-01,\n",
      "          -1.6467e-01, -3.2645e-01],\n",
      "         [-1.0899e-01,  1.5281e-01, -3.4445e-02, -2.4786e-01, -9.1990e-02,\n",
      "          -2.9423e-01,  2.1347e-01, -3.6742e-01, -4.6891e-01,  3.3854e-01,\n",
      "           3.7328e-01, -2.3481e-01, -7.0652e-01,  4.0830e-02, -1.1696e-01,\n",
      "          -5.9533e-02,  3.0181e-01,  3.4646e-02,  3.2687e-02, -2.0340e-01,\n",
      "          -8.6399e-02,  2.7100e-01],\n",
      "         [-1.2637e-01,  1.3850e-01,  4.4635e-02, -2.0763e-01, -2.5973e-01,\n",
      "          -5.0040e-01, -2.6730e-02, -3.6697e-01, -4.4730e-01,  7.7905e-02,\n",
      "           1.6941e-01, -3.3307e-01, -6.8450e-01, -3.8900e-02, -1.0681e-01,\n",
      "          -3.3612e-01, -1.3635e-01, -1.9251e-01,  1.9085e-01, -9.2513e-02,\n",
      "          -8.4809e-02, -4.0726e-01],\n",
      "         [ 3.5975e-01,  1.2774e-01,  5.4534e-01, -3.4597e-01, -1.4448e-01,\n",
      "          -3.5687e-01, -2.8079e-01, -1.2658e-01,  2.3130e-01, -2.4278e-01,\n",
      "           1.7486e-01,  3.0640e-02, -4.3021e-01, -4.1804e-01, -5.1368e-01,\n",
      "          -4.1235e-01,  1.6028e-01,  2.5925e-04, -7.9575e-02,  7.3722e-02,\n",
      "          -1.4872e-01, -4.3476e-01],\n",
      "         [-2.2600e-01,  2.3673e-01,  1.7412e-01, -1.6570e-01, -3.9497e-01,\n",
      "          -2.8012e-01, -1.9762e-01, -1.0237e-01, -2.5409e-01, -1.4358e-01,\n",
      "           2.2530e-01, -1.6122e-01, -2.0657e-01,  1.5029e-01, -4.2360e-02,\n",
      "          -2.0508e-02,  9.2843e-02, -5.1737e-02,  1.6745e-01, -2.8052e-01,\n",
      "          -6.1953e-01, -2.0365e-01],\n",
      "         [ 4.8821e-03,  2.0361e-01,  2.6320e-01,  1.1687e-01, -1.8298e-01,\n",
      "          -4.0891e-01, -7.9542e-02, -2.5333e-01, -4.0115e-01, -1.1280e-01,\n",
      "           7.3341e-02, -1.4524e-01, -4.9790e-01, -1.8888e-01, -4.0434e-01,\n",
      "          -2.8624e-01,  9.0861e-02, -1.0925e-01,  5.7817e-01,  1.9701e-01,\n",
      "          -2.1715e-01, -2.3115e-01],\n",
      "         [-2.9153e-02, -2.2822e-02,  2.2405e-02, -5.8329e-01, -3.8333e-01,\n",
      "           3.0217e-02, -3.4332e-02,  1.3908e-01, -1.5039e-01, -3.1208e-01,\n",
      "          -2.1099e-01, -2.5744e-01, -1.8741e-01, -3.1208e-01, -2.2516e-01,\n",
      "          -2.0304e-01,  2.6103e-01, -3.9959e-02, -4.2493e-02,  1.2731e-01,\n",
      "          -8.3980e-02, -4.5071e-01],\n",
      "         [ 2.8097e-01, -9.1863e-02, -3.7814e-01, -7.8584e-02, -7.0839e-01,\n",
      "          -9.4591e-03, -3.1942e-01, -1.5891e-01, -1.9918e-01, -2.9063e-01,\n",
      "          -1.2960e-02,  4.4859e-01, -1.6033e-01,  5.3336e-02, -1.1613e-01,\n",
      "           1.9372e-01,  5.2645e-02, -1.7018e-02, -2.8357e-02,  8.0847e-02,\n",
      "          -2.8554e-01, -1.4894e-01],\n",
      "         [-5.4300e-02,  2.7669e-01,  1.3416e-01,  3.9825e-01, -3.3692e-02,\n",
      "          -4.4104e-01, -9.3907e-02, -1.7781e-01, -3.1377e-01, -2.3089e-01,\n",
      "           1.2269e-01,  5.1282e-02, -6.5616e-01, -2.5578e-02,  3.4939e-02,\n",
      "          -5.7995e-01, -6.9208e-02, -1.3583e-01,  3.4067e-01,  2.0948e-01,\n",
      "          -1.0926e-01, -7.7646e-02],\n",
      "         [-5.4445e-03,  2.0955e-01,  3.8203e-01, -4.8202e-01, -2.4261e-01,\n",
      "           4.5877e-04, -4.6783e-01, -1.4339e-01, -1.7367e-01, -1.0164e-01,\n",
      "           6.2874e-03, -9.1902e-04, -3.3973e-01, -3.9803e-01, -3.9938e-01,\n",
      "          -2.9890e-01,  1.6765e-01, -3.0038e-02,  2.3705e-01,  3.6694e-02,\n",
      "           3.1368e-01, -1.1024e-02],\n",
      "         [-4.2280e-02,  1.0879e-01, -2.6971e-01, -4.7926e-01, -9.3038e-02,\n",
      "          -3.5651e-01,  4.9039e-02, -1.6174e-01, -2.6760e-01, -3.1137e-02,\n",
      "          -2.5678e-01, -1.2280e-01, -6.6729e-01,  1.0757e-01,  9.7242e-02,\n",
      "          -2.2825e-01, -2.1033e-01,  8.5202e-02,  1.3077e-01, -5.0041e-01,\n",
      "          -1.1778e-01,  8.8030e-02],\n",
      "         [-6.3919e-02,  5.5632e-01,  3.6774e-01,  7.2422e-02, -2.1344e-01,\n",
      "          -8.0884e-01,  1.0546e-01, -6.3664e-01, -7.5107e-01,  2.2459e-02,\n",
      "           7.6792e-02, -3.2978e-01, -4.3252e-01,  1.5562e-01,  5.7277e-02,\n",
      "          -6.3529e-01,  3.1162e-02, -1.6006e-01,  2.4245e-01,  5.8155e-02,\n",
      "          -2.7507e-01, -1.1620e-01],\n",
      "         [-1.7687e-01,  1.5845e-01, -4.5571e-02, -5.2750e-01, -9.8211e-02,\n",
      "          -6.9593e-01, -4.2372e-02, -4.5212e-01, -2.4089e-01,  2.9110e-01,\n",
      "           9.3365e-03, -4.1367e-02, -5.8018e-01, -1.1177e-01, -4.6008e-01,\n",
      "          -1.6574e-01, -1.5198e-01, -2.1813e-01, -2.7455e-01,  2.4434e-02,\n",
      "          -4.7756e-03, -1.1239e-01],\n",
      "         [-3.6196e-01,  2.8963e-01, -2.4128e-01,  2.6751e-01, -2.8757e-01,\n",
      "          -9.2750e-01, -1.6548e-01, -1.3977e-01, -7.0128e-01, -3.4697e-02,\n",
      "           2.1654e-01, -3.6644e-02, -5.6476e-01, -2.0403e-01, -1.8732e-01,\n",
      "          -6.5437e-01, -4.0135e-01, -2.2407e-01, -3.3518e-01, -4.1530e-01,\n",
      "          -1.7638e-01, -1.4059e-01],\n",
      "         [-2.1110e-01,  6.3020e-01,  2.9587e-01,  1.9818e-01, -1.5879e-01,\n",
      "          -6.4263e-01,  3.8926e-02, -5.0150e-01, -2.7186e-01,  2.7649e-01,\n",
      "           4.4271e-01, -5.2432e-02, -3.9974e-01, -5.5871e-01, -1.5300e-01,\n",
      "          -5.2556e-01, -8.3888e-02, -7.1099e-01,  6.1145e-01, -7.3300e-02,\n",
      "          -1.5158e-01,  1.9117e-01],\n",
      "         [-6.7232e-01,  6.1128e-01,  6.6857e-02, -1.8689e-01, -4.6418e-01,\n",
      "          -5.4839e-01,  1.7380e-01, -2.5332e-01, -1.7936e-01, -1.1710e-01,\n",
      "           3.1415e-02,  1.3026e-01, -2.2209e-01, -4.4644e-01, -2.8061e-01,\n",
      "          -7.1177e-01, -1.2721e-01, -9.2485e-02,  1.7422e-01,  2.6844e-01,\n",
      "           9.1546e-02, -6.1521e-02],\n",
      "         [-5.5750e-01,  2.0719e-01,  3.2482e-01,  3.1602e-02, -2.8256e-01,\n",
      "          -2.7097e-01, -1.5933e-01, -1.7691e-01, -3.5443e-01, -9.5413e-02,\n",
      "           1.0182e-01,  1.3845e-01, -4.6747e-01, -3.1639e-01, -1.2139e-01,\n",
      "          -5.6123e-01, -2.4980e-02, -4.2723e-01,  4.4253e-03,  1.2485e-01,\n",
      "          -1.9356e-01, -4.0346e-01],\n",
      "         [-7.0663e-01, -1.2550e-01, -1.0547e-02, -2.3097e-01, -3.2292e-01,\n",
      "          -3.7289e-01,  3.3305e-01,  1.0279e-02, -2.8536e-01, -2.2976e-01,\n",
      "           1.4519e-01,  1.3051e-01,  1.8041e-01, -5.5477e-01, -2.4112e-01,\n",
      "          -8.1108e-02, -1.7748e-01, -4.8465e-01, -2.4788e-01,  2.2320e-01,\n",
      "          -2.9767e-01, -5.2429e-01],\n",
      "         [-5.0757e-01,  4.9831e-01, -4.5521e-02,  1.5098e-01, -6.0831e-01,\n",
      "          -4.2392e-02,  1.0628e-01, -8.5821e-02,  1.8814e-02,  3.2922e-02,\n",
      "           4.2783e-02,  4.5173e-01, -1.2583e-01, -4.0846e-01, -4.1776e-01,\n",
      "          -3.4128e-01, -1.1674e-01, -3.3006e-01,  2.1664e-01, -1.3690e-01,\n",
      "           5.1927e-02, -5.2929e-01],\n",
      "         [-2.5262e-01,  3.0516e-01,  1.7323e-01, -2.5518e-01, -2.0903e-01,\n",
      "           1.8289e-01,  1.3951e-01,  8.3793e-02, -3.2704e-01,  3.0301e-01,\n",
      "          -2.3959e-01, -1.0274e-01, -3.2534e-01,  3.8011e-02, -2.4602e-01,\n",
      "          -3.1522e-01,  2.8378e-01, -3.9785e-02, -1.8988e-02, -4.8997e-01,\n",
      "           1.0819e-01, -4.0184e-01],\n",
      "         [-1.7319e-01,  3.5262e-01,  6.7891e-02, -1.1580e-02, -2.1654e-01,\n",
      "          -5.2267e-01, -1.5824e-01, -1.6386e-01, -8.5092e-01,  3.2176e-01,\n",
      "           4.6899e-02, -4.4750e-01, -5.1363e-01,  2.7196e-02,  2.2596e-02,\n",
      "          -5.2125e-01, -9.5919e-02, -2.6660e-01,  3.1965e-01,  6.6152e-02,\n",
      "          -1.2766e-01, -3.8485e-01],\n",
      "         [ 1.1529e-01, -1.1368e-01,  1.7505e-01,  6.6669e-02, -2.2961e-01,\n",
      "          -4.0636e-01, -6.6647e-01, -2.3981e-01, -2.7847e-01,  4.1965e-01,\n",
      "          -1.2349e-01, -6.7437e-02, -4.6947e-01, -3.0009e-01, -4.6866e-01,\n",
      "          -7.0489e-01,  1.4136e-01,  4.6733e-02, -2.1477e-01, -5.1960e-01,\n",
      "          -3.5570e-01, -2.3216e-01],\n",
      "         [-2.9344e-01,  1.8527e-01, -1.3176e-01,  1.5501e-01, -2.6072e-01,\n",
      "          -5.6304e-01, -1.6565e-01,  1.0292e-01, -3.8819e-01,  3.0922e-01,\n",
      "          -2.7758e-01, -4.5264e-01, -3.7042e-01, -1.3873e-01,  5.2347e-02,\n",
      "          -2.9257e-01, -2.8817e-01, -4.7639e-01, -1.5160e-01, -1.0539e-01,\n",
      "           7.1130e-02, -2.2762e-01],\n",
      "         [-5.2032e-01,  5.0141e-01, -1.5420e-01,  3.4806e-01, -4.4418e-01,\n",
      "          -2.6739e-01, -8.3021e-02,  2.8960e-01, -3.4323e-01, -4.1125e-02,\n",
      "          -2.9367e-01,  6.4470e-02,  5.5430e-02, -6.4601e-01, -2.0762e-01,\n",
      "           9.2521e-03, -4.3267e-01, -3.2990e-01, -2.7206e-01,  1.4358e-01,\n",
      "          -7.1736e-02, -3.9147e-01],\n",
      "         [-4.1435e-01,  1.5197e-01,  2.4405e-01, -8.8976e-03, -2.0487e-01,\n",
      "          -1.4497e-01, -1.1439e-01, -1.1731e-01, -2.4779e-01,  1.2339e-01,\n",
      "          -3.7431e-02,  2.3050e-01, -1.0952e-01, -3.8960e-01, -3.4749e-01,\n",
      "          -4.9456e-01, -2.1950e-01, -3.5064e-01,  8.6387e-02,  5.9567e-02,\n",
      "          -9.0387e-02, -2.6524e-01],\n",
      "         [-3.5995e-01,  2.2377e-01,  2.2048e-01, -1.2404e-01, -1.7217e-01,\n",
      "          -2.5415e-01,  1.2086e-01, -6.0228e-02, -1.3568e-01, -3.4625e-02,\n",
      "          -9.7109e-02,  7.3546e-02, -1.1093e-01, -2.2735e-01, -2.6897e-01,\n",
      "          -2.7533e-01, -7.8846e-02, -9.5398e-02,  2.0140e-03,  1.2870e-01,\n",
      "          -1.1417e-01, -2.4905e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [16]\n",
      "tensor([[[ 0.2886,  0.2561, -0.3325,  0.0356, -0.4718,  0.1067, -0.0464,\n",
      "          -0.3892,  0.1339, -0.1862, -0.6269,  0.1870, -0.0438, -0.1811,\n",
      "          -0.2263, -0.0111, -0.0440,  0.1987,  0.2289,  0.1282,  0.2438,\n",
      "           0.2026],\n",
      "         [ 0.4322,  0.6922, -0.3201, -0.2424,  0.2735, -0.0179, -0.3892,\n",
      "           0.2290, -0.2756, -0.3763, -1.1573,  0.0631, -0.3654, -0.1861,\n",
      "           0.6979, -0.0303,  0.3403,  0.3305,  0.0294,  0.6489, -0.1787,\n",
      "          -0.0825],\n",
      "         [-0.2204,  0.3598, -0.4534, -0.0507, -0.3362,  0.0415, -0.0042,\n",
      "          -0.2192,  0.1878, -0.3317, -0.8471,  0.3972,  0.5678, -0.1384,\n",
      "           0.5883,  0.2060,  0.4782,  0.2063,  0.2126,  0.5080, -0.3121,\n",
      "           0.3458],\n",
      "         [ 0.1948,  0.3252, -0.1250,  0.1251, -0.4036,  0.4626,  0.2378,\n",
      "           0.1392,  0.1637,  0.0655, -0.9576,  0.4516, -0.1889, -0.1681,\n",
      "           0.3493, -0.3981,  0.3679,  0.2966,  0.4876,  0.6112, -0.5776,\n",
      "           0.2584],\n",
      "         [ 0.1091,  0.4607, -0.0630,  0.2458,  0.1758, -0.1121, -0.1421,\n",
      "          -0.1179,  0.2313,  0.0621, -0.5524,  0.4930,  0.1269, -0.2399,\n",
      "           0.0110, -0.2362,  0.1772,  0.4375, -0.0931,  0.4140,  0.0688,\n",
      "           0.3820],\n",
      "         [-0.0855,  0.3988, -0.0907,  0.3981, -0.0049,  0.3322, -0.3364,\n",
      "           0.2017, -0.0590,  0.3906, -0.2631,  0.5097,  0.1669,  0.3335,\n",
      "           0.0056, -0.1133,  0.3512,  0.0891, -0.1961,  0.1676, -0.2122,\n",
      "           0.0902],\n",
      "         [-0.2313,  0.2512, -0.1717, -0.2173, -0.3525,  0.0694, -0.1643,\n",
      "          -0.2003,  0.0604, -0.4041, -0.7502,  0.4891,  0.3702, -0.1899,\n",
      "           0.3427, -0.2695,  0.0199, -0.1382, -0.0448,  0.5382, -0.2768,\n",
      "           0.4544],\n",
      "         [ 0.0193,  0.1254, -0.6389,  0.0986, -0.0330, -0.1440,  0.3596,\n",
      "          -0.1282, -0.0091, -0.2041, -1.0383,  0.5500, -0.0739, -0.3053,\n",
      "           0.3113, -0.2342,  0.5575,  0.3707, -0.3425,  0.0648,  0.3756,\n",
      "           0.1853],\n",
      "         [-0.0123,  0.4605, -0.8094,  0.3096, -0.2867, -0.0668,  0.1137,\n",
      "           0.2760, -0.1026, -0.0377, -0.9411,  0.3055, -0.1202, -0.4402,\n",
      "           0.2897, -0.0801,  0.3098, -0.2121,  0.1532,  0.5174, -0.2353,\n",
      "          -0.0112],\n",
      "         [ 0.4611,  0.3684, -0.1375,  0.4223, -0.2281,  0.1122,  0.0396,\n",
      "           0.3332, -0.2250,  0.1804, -0.8178,  0.9283, -0.0184, -0.8359,\n",
      "           0.4533, -0.4612, -0.0124, -0.3246, -0.1245,  0.1365, -0.0158,\n",
      "           0.2899],\n",
      "         [ 0.2810,  0.4467, -0.1895,  0.3032,  0.0219,  0.2235, -0.1245,\n",
      "          -0.3110, -0.3154, -0.1410, -0.9279,  0.5012, -0.0430, -0.0705,\n",
      "          -0.1040, -0.4521,  0.0577,  0.1666, -0.3573,  0.2927,  0.5025,\n",
      "          -0.1721],\n",
      "         [ 0.7030,  0.3781, -0.3146,  0.6333,  0.1946, -0.1571,  0.2699,\n",
      "          -0.2098, -0.1496, -0.2667, -0.9564, -0.0944,  0.0528,  0.1409,\n",
      "           0.4069, -0.1768,  0.0557, -0.3014, -0.2747,  0.3861, -0.0835,\n",
      "           0.3746],\n",
      "         [ 0.4330,  0.0536, -0.1298,  0.1507,  0.2762, -0.1272, -0.0148,\n",
      "          -0.4085, -0.3771, -0.2878, -0.7796,  0.4407, -0.0439, -0.3149,\n",
      "           0.6583, -0.1552,  0.7503,  0.1053, -0.2188,  0.6534,  0.1251,\n",
      "           0.0846],\n",
      "         [ 0.4888,  0.2633, -0.5460,  0.0905, -0.0028,  0.0052,  0.0803,\n",
      "           0.1107, -0.0447, -0.3605, -0.9106,  0.3107,  0.1129, -0.5170,\n",
      "           0.5660, -0.1381,  0.1001,  0.0484,  0.0289,  0.1126, -0.3726,\n",
      "          -0.0150],\n",
      "         [ 0.3127,  0.6184, -0.3065, -0.0217, -0.2213, -0.1807,  0.1047,\n",
      "           0.1802, -0.2655, -0.2673, -0.8629,  0.0951, -0.2318, -0.1851,\n",
      "           0.4933, -0.2621,  0.3216,  0.0968, -0.5103,  0.3746,  0.0320,\n",
      "           0.1493],\n",
      "         [ 0.3812,  0.4231, -0.0687,  0.4617, -0.2122, -0.1551,  0.0417,\n",
      "           0.7828, -0.0500,  0.0531, -0.9825, -0.0132,  0.2379, -0.2851,\n",
      "           0.5745,  0.0449,  0.1902,  0.4552, -0.1567,  0.2512, -0.2667,\n",
      "           0.5633],\n",
      "         [ 0.3573,  0.7019, -0.2487,  0.2134, -0.0441, -0.1875,  0.3242,\n",
      "          -0.0251, -0.3806, -0.0394, -1.1315, -0.2952,  0.0921, -0.4798,\n",
      "           0.6057, -0.1697,  0.3117,  0.5133, -0.4940,  0.3655, -0.0834,\n",
      "           0.5672],\n",
      "         [ 0.0683,  0.1457, -0.4180,  0.2012,  0.0182, -0.2371,  0.2377,\n",
      "           0.1390, -0.2757, -0.4699, -0.7929,  0.1416, -0.0347, -0.3733,\n",
      "           0.3946, -0.3537,  0.1476,  0.1779,  0.0771,  0.3896, -0.0939,\n",
      "           0.3749]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [29]\n",
      "tensor([[[-2.2377e-01,  3.5462e-01, -8.1677e-02,  1.3352e-01, -5.6149e-01,\n",
      "           7.0009e-01, -3.2966e-01,  5.3697e-01,  2.4386e-01, -4.7790e-01,\n",
      "           2.2902e-01, -2.3918e-01,  1.4727e-01, -3.1927e-01, -3.6147e-01,\n",
      "           3.0256e-02, -2.2092e-01,  3.1722e-02, -3.8351e-01, -1.4071e-01,\n",
      "           3.3610e-01, -3.2200e-01],\n",
      "         [-1.9907e-01, -3.8928e-01,  2.8339e-01,  3.7815e-01, -4.1143e-01,\n",
      "          -7.4490e-02, -1.3855e-01,  1.0469e-01, -5.1371e-02,  2.2734e-01,\n",
      "           1.6433e-01,  5.4757e-01, -1.7773e-01, -4.1104e-02, -2.8881e-01,\n",
      "           3.4992e-01,  1.0842e-01,  6.9089e-02, -4.7483e-01,  4.2067e-01,\n",
      "           1.5954e-01, -7.1662e-02],\n",
      "         [ 1.8583e-01, -1.9338e-01,  2.0213e-01,  6.1911e-01, -5.4100e-01,\n",
      "           2.4830e-01,  2.2578e-01, -3.0912e-01, -2.0799e-01,  2.1023e-01,\n",
      "           3.3209e-01,  1.0102e+00,  1.7095e-01, -2.9674e-01, -1.3210e-02,\n",
      "           5.9499e-01, -1.1853e-01,  9.9614e-02, -7.1661e-02, -3.8138e-02,\n",
      "          -1.0869e-01, -9.5675e-02],\n",
      "         [ 6.2139e-02, -1.6264e-01,  1.5116e-01,  3.8660e-01, -3.1592e-01,\n",
      "           5.3550e-02,  1.3232e-01, -2.1555e-01,  1.7612e-01,  1.2568e-01,\n",
      "           5.7077e-02,  6.3994e-01, -1.8406e-01, -1.1404e-01, -2.4917e-02,\n",
      "           1.4697e-01,  3.5345e-01,  6.8393e-02, -5.5837e-01,  1.8526e-01,\n",
      "           9.9377e-02, -2.3875e-01],\n",
      "         [ 3.5355e-01, -3.7170e-02,  3.4930e-01,  3.1095e-01, -1.6875e-02,\n",
      "           3.8020e-02,  1.2106e-02, -2.4286e-01,  6.8072e-04,  3.4305e-02,\n",
      "           3.6239e-01,  1.1280e+00, -1.9874e-01, -4.1804e-01,  8.8511e-02,\n",
      "           1.1293e-01, -2.9520e-02,  8.5348e-02, -2.7599e-01, -2.0446e-01,\n",
      "           1.7356e-01, -4.5290e-01],\n",
      "         [ 3.9747e-01, -3.0771e-01,  1.7536e-01,  3.4003e-01, -9.7064e-02,\n",
      "           4.2641e-01, -1.8383e-01, -8.3337e-02, -1.2092e-01,  2.2520e-02,\n",
      "           2.0210e-01,  5.1297e-01,  8.5658e-02, -1.8528e-01, -1.0014e-01,\n",
      "           3.7637e-01,  2.0319e-01,  4.4702e-02,  8.5969e-02, -9.7475e-02,\n",
      "          -4.9476e-02, -1.9486e-02],\n",
      "         [ 2.0578e-01, -5.2630e-01,  5.7562e-01,  4.7232e-01, -1.1945e-01,\n",
      "          -2.9038e-01,  1.0223e-01, -3.6111e-01,  1.9152e-01, -1.9486e-01,\n",
      "          -1.6645e-01,  3.3366e-01, -4.5790e-01, -5.6732e-02, -3.0421e-01,\n",
      "           3.6254e-01, -4.7096e-02, -3.2282e-01,  9.8876e-02, -2.9852e-01,\n",
      "           2.3691e-01, -3.6917e-01],\n",
      "         [ 3.7860e-01, -5.5645e-01,  3.9211e-01,  1.4234e-01, -1.2052e-01,\n",
      "          -3.0798e-01,  2.5170e-01, -1.2372e-01,  9.2189e-01, -4.7508e-01,\n",
      "          -7.0180e-02,  5.2523e-01,  1.0397e-01, -2.2981e-01, -2.4473e-01,\n",
      "           1.6495e-01, -2.2612e-01, -5.9829e-02, -5.4538e-02, -4.4046e-01,\n",
      "           2.7341e-01, -3.1884e-01],\n",
      "         [ 1.9243e-01, -1.4879e-01,  5.4203e-01,  6.2728e-01, -3.8139e-01,\n",
      "          -5.5799e-02,  3.4584e-02, -4.0939e-02,  1.2124e-01,  1.1383e-01,\n",
      "           4.5149e-01, -1.0025e-01, -7.5061e-01, -4.9846e-01,  1.4086e-01,\n",
      "           5.0117e-01, -1.4888e-01,  2.2795e-01, -2.4348e-01, -3.1074e-01,\n",
      "           6.5592e-02, -2.2171e-01],\n",
      "         [ 1.1148e-01, -7.8484e-01,  4.3652e-01,  8.5979e-01, -2.4569e-01,\n",
      "           2.1546e-01, -6.3537e-01,  1.2670e-01,  1.1313e-01, -4.2356e-01,\n",
      "           3.7410e-01,  6.6514e-01, -4.2381e-01, -3.4403e-01, -2.3317e-01,\n",
      "           3.0590e-01, -2.4210e-01, -3.9727e-01, -3.6587e-02, -3.7542e-01,\n",
      "           2.8506e-01, -8.1264e-01],\n",
      "         [ 1.2827e-01,  3.5484e-02,  2.5383e-01,  5.5492e-01, -6.0891e-01,\n",
      "           5.0049e-01,  1.8107e-02, -7.3764e-02,  2.4669e-01, -1.8967e-01,\n",
      "           2.9948e-01, -9.5825e-03, -5.0816e-01, -2.9109e-01, -3.2979e-01,\n",
      "           2.6292e-01, -5.9349e-01,  2.2722e-01, -4.5376e-01,  5.5652e-02,\n",
      "           4.6059e-01, -1.6829e-01],\n",
      "         [-2.8463e-01, -6.9031e-01,  4.6928e-01,  7.3300e-01, -2.1049e-01,\n",
      "           5.7773e-01, -4.2329e-01,  2.7390e-01,  1.1983e-02, -1.8742e-01,\n",
      "           3.6722e-01,  5.6029e-01, -3.9147e-01,  1.4744e-03, -5.1703e-01,\n",
      "           7.1000e-03, -1.1819e-01, -3.4183e-01, -8.5999e-02, -7.9358e-02,\n",
      "           2.5204e-01, -6.4231e-01],\n",
      "         [ 2.9268e-02,  8.9450e-02,  4.0039e-01,  5.2653e-01, -5.0933e-01,\n",
      "           3.4526e-01,  1.8670e-01, -1.2731e-01, -2.5315e-03, -2.6195e-01,\n",
      "           3.0953e-01,  2.7379e-02, -1.0693e-01, -1.6201e-01, -2.3548e-01,\n",
      "           3.0053e-01, -5.8630e-01,  2.9165e-01, -3.0361e-01,  3.8817e-01,\n",
      "           4.3266e-01, -1.2204e-01],\n",
      "         [-6.6048e-01, -7.9906e-01,  2.3538e-01,  6.6854e-01, -3.5672e-01,\n",
      "          -4.4971e-01, -3.9175e-01,  7.8893e-02,  4.1958e-01, -4.9203e-01,\n",
      "          -3.7938e-02,  5.7328e-01, -8.0198e-01,  1.3750e-01, -2.8123e-01,\n",
      "           4.4439e-01, -5.4804e-02, -9.8582e-05, -3.7430e-01, -9.4544e-02,\n",
      "          -9.1319e-02, -3.1816e-01],\n",
      "         [-6.5622e-02, -5.5952e-01,  1.7863e-01,  4.9562e-01, -2.4077e-01,\n",
      "          -3.2428e-01, -2.7464e-01, -3.4267e-01,  1.6090e-01, -5.8774e-01,\n",
      "           1.3567e-02,  2.3018e-01, -3.6655e-01, -4.9090e-01, -4.6383e-01,\n",
      "           3.9246e-01,  5.9733e-02,  5.4904e-02, -3.3329e-01, -1.1397e-01,\n",
      "           2.7926e-03, -3.8634e-01],\n",
      "         [ 3.0553e-01, -3.7810e-01,  4.5548e-01,  2.2767e-01, -3.2620e-01,\n",
      "           3.5703e-01,  3.0693e-02, -6.8444e-02,  2.5831e-01,  3.5292e-01,\n",
      "           7.8619e-02, -7.1471e-02, -2.3212e-01, -7.8451e-01, -1.7034e-01,\n",
      "           4.3177e-01, -8.3330e-02,  1.8896e-01, -3.1468e-01,  1.8679e-01,\n",
      "           2.9888e-01,  2.7222e-02],\n",
      "         [ 1.9616e-01, -7.6288e-01,  5.3169e-01,  5.7306e-01,  5.4393e-01,\n",
      "           6.0812e-01, -9.8437e-02,  1.3968e-01,  1.0042e-01, -1.9364e-01,\n",
      "           8.6160e-02,  1.1667e-01, -2.4269e-01, -1.7391e-01, -5.3669e-01,\n",
      "           8.5657e-02,  2.8664e-01, -1.8587e-01,  1.9886e-01, -6.5195e-02,\n",
      "           6.3593e-02, -2.6463e-01],\n",
      "         [ 1.0861e-01, -5.3661e-01,  3.0515e-01,  5.4101e-01,  2.0719e-01,\n",
      "           4.0980e-01,  8.4611e-02, -3.4996e-01,  5.2418e-02,  3.3508e-02,\n",
      "           3.4386e-01, -4.3806e-02, -4.6639e-01, -5.8769e-01, -1.0885e-01,\n",
      "          -3.6814e-02, -6.7512e-02,  2.1059e-01,  1.9516e-01, -2.8271e-01,\n",
      "           2.7211e-01, -2.1647e-01],\n",
      "         [ 1.4298e-01, -1.0012e+00,  5.9484e-02,  3.4692e-01, -1.4822e-01,\n",
      "           1.3058e-01,  2.5115e-01, -2.6104e-01,  4.6599e-01,  4.6677e-02,\n",
      "           5.3829e-01, -1.4709e-01, -4.5580e-02, -2.3317e-01, -4.6807e-02,\n",
      "          -3.2874e-01,  8.0583e-02, -6.8147e-01, -1.6947e-01,  2.0702e-02,\n",
      "           5.3986e-01, -3.1712e-01],\n",
      "         [ 6.9495e-01, -5.0781e-01,  1.7767e-01,  2.7039e-01, -3.0593e-01,\n",
      "           1.5891e-01, -2.1986e-01, -4.9341e-01,  2.2043e-01,  6.1813e-02,\n",
      "           1.9767e-01,  5.0981e-01, -4.2831e-01, -3.3783e-01, -3.7450e-01,\n",
      "           1.6397e-01,  3.8381e-02,  5.2270e-01, -7.1699e-02, -4.2772e-01,\n",
      "           3.7459e-01, -6.5101e-01],\n",
      "         [ 2.0330e-01, -5.6176e-01,  3.8928e-01,  4.6876e-01, -1.9242e-01,\n",
      "          -3.0680e-01, -9.5354e-02, -4.1369e-01,  4.3205e-01, -3.5961e-01,\n",
      "           1.0260e-01,  6.8223e-01, -7.3310e-01,  3.2106e-02, -6.9850e-01,\n",
      "           2.9868e-01,  4.7359e-01, -2.0794e-01, -1.7065e-01, -2.1015e-01,\n",
      "           2.0691e-01, -7.6718e-01],\n",
      "         [ 1.8693e-01, -4.7181e-01,  4.8053e-01,  5.6351e-01, -4.5891e-02,\n",
      "           8.9826e-02,  1.0998e-01, -4.1368e-01,  7.7643e-01, -3.0073e-01,\n",
      "           1.1252e-01,  5.8319e-01,  2.6945e-01,  1.3040e-03, -3.4487e-01,\n",
      "           8.8560e-02, -1.1671e-01,  2.5347e-02, -8.5965e-02, -3.9460e-01,\n",
      "           4.1521e-01, -3.6710e-01],\n",
      "         [ 4.9569e-01, -9.3141e-02,  5.2089e-01, -1.5326e-01, -4.4828e-01,\n",
      "           6.1453e-04,  3.7448e-02, -3.6199e-01,  1.1571e-01, -2.2278e-02,\n",
      "           5.7659e-01,  1.3115e-01, -8.1119e-01, -2.8761e-01, -1.3099e-01,\n",
      "           4.4251e-01, -4.2587e-01,  8.1157e-02, -9.0444e-02,  4.0259e-02,\n",
      "          -8.8423e-02, -3.5008e-01],\n",
      "         [ 2.7174e-01, -6.7770e-01,  8.0971e-01,  5.8910e-01, -7.3477e-03,\n",
      "          -3.8097e-01, -5.2804e-01,  5.2661e-03,  8.9395e-01, -7.5350e-01,\n",
      "           7.0087e-01,  1.0532e+00, -5.2831e-01, -2.0260e-01, -1.1104e+00,\n",
      "           2.1724e-01, -6.5071e-02, -3.3059e-01, -2.8104e-01, -3.5293e-01,\n",
      "          -2.6325e-01, -7.6820e-01],\n",
      "         [ 2.0629e-01, -4.3787e-01,  7.3959e-01,  5.4988e-01, -1.1107e-01,\n",
      "           1.3589e-01, -1.0939e-01, -4.0447e-01,  1.1014e-01, -3.4828e-01,\n",
      "           3.0854e-01,  9.4047e-01, -1.9415e-01,  4.2953e-02, -2.8735e-01,\n",
      "          -2.9875e-01,  1.9525e-01,  5.7341e-01,  1.1510e-01, -3.0667e-01,\n",
      "          -1.0340e-01, -7.6940e-01],\n",
      "         [ 2.9906e-01,  3.5155e-02,  3.3230e-01, -5.1783e-02, -7.3024e-02,\n",
      "           5.6974e-01,  1.4297e-01,  7.3982e-03,  3.0995e-01,  1.4096e-01,\n",
      "           5.7549e-01, -5.4164e-02, -6.0746e-01, -3.3189e-01, -9.2544e-02,\n",
      "           4.6861e-01, -3.2464e-01,  2.2771e-01, -3.7945e-01, -2.6414e-01,\n",
      "           1.4278e-01, -3.7586e-01],\n",
      "         [ 1.5636e-01, -6.6256e-01,  6.2851e-01,  5.4784e-01, -2.3575e-01,\n",
      "           5.6684e-01, -4.1108e-01,  1.5213e-02,  3.6837e-01, -6.2010e-01,\n",
      "           4.8370e-01,  6.9505e-01, -3.2790e-01,  4.7751e-02, -1.5429e-01,\n",
      "           5.5098e-01, -2.1948e-01, -4.0923e-01,  6.5812e-02, -2.9483e-01,\n",
      "           3.5236e-01, -5.7328e-01],\n",
      "         [ 1.2600e-01, -2.1151e-01,  4.2473e-01,  1.5713e-01, -6.0061e-01,\n",
      "           6.8773e-01, -2.4408e-01, -1.6670e-03, -8.6503e-02, -1.8583e-01,\n",
      "           1.1530e-01,  5.3023e-01, -3.3821e-01, -1.4535e-02, -4.2346e-01,\n",
      "           2.5130e-01,  2.4910e-01,  4.2273e-02,  1.8070e-01, -1.7821e-01,\n",
      "           8.0659e-02, -5.8086e-01],\n",
      "         [-1.4630e-01, -1.5573e-01,  4.6241e-01,  3.0727e-01, -6.4495e-01,\n",
      "           6.6292e-01,  1.1636e-01, -2.4456e-01, -8.0903e-02, -3.9314e-01,\n",
      "           2.1144e-01,  4.2102e-01, -2.2533e-01,  6.3753e-01, -2.2089e-01,\n",
      "           3.1108e-01,  2.8813e-01,  7.1811e-02, -9.5399e-02, -3.7625e-01,\n",
      "          -2.1296e-01, -2.1101e-01],\n",
      "         [ 9.3441e-02, -2.2074e-01,  3.8484e-01,  1.2343e-01, -7.7914e-02,\n",
      "           5.8758e-01, -1.4465e-01, -3.9757e-01,  5.0667e-02, -2.1808e-01,\n",
      "           3.3511e-02,  2.9866e-01, -3.8938e-02, -1.2839e-01, -3.2295e-01,\n",
      "           3.9474e-01,  1.4836e-02,  1.8091e-01, -2.8757e-01, -2.7869e-01,\n",
      "           7.5254e-02, -2.2041e-01],\n",
      "         [ 2.5419e-02, -2.7039e-01,  2.3880e-01,  2.6203e-01, -1.8156e-01,\n",
      "           3.4598e-01, -7.9889e-02, -1.6954e-01, -2.5230e-02, -2.6037e-01,\n",
      "           9.1004e-02,  2.7241e-01, -1.8472e-01,  3.2323e-02, -2.3572e-01,\n",
      "           3.0587e-01,  1.4219e-01,  1.1431e-01, -1.0216e-01, -2.5381e-01,\n",
      "          -1.8786e-01, -4.9279e-01]]], grad_fn=<AddBackward0>)\n",
      "each_len_list [59]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-331-ab6b62dec740>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_hidden_state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    514\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2436\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2438\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # 숫자 ->train_epoch\n",
    "    model.train()\n",
    "    \n",
    "    for iteration, batch in enumerate(dataloader_train):\n",
    "        batch_inputs = {k: v for k, v in list (batch[0].items())}\n",
    "        batch_labels = batch[1]\n",
    "\n",
    "        emb = bert(**batch_inputs)\n",
    "        e = nn.Dropout(0.1)\n",
    "        r = e(emb['last_hidden_state'])\n",
    "        \n",
    "        w = nn.Linear(768, 22)\n",
    "        y = w(r)\n",
    "        \n",
    "        print(y)\n",
    "        \n",
    "#         q = nn.Softmax(dim = 2) # A dimension along which Softmax will be computed\n",
    "#         output = y # A dimension along which Softmax will be computed\n",
    "        \n",
    "#         loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (iteration + 1) % 10 == 0:\n",
    "#             print(f'{iteration:3} - loss: {loss.item()}')\n",
    "\n",
    "    # todo 매 에포크가 끝나면 dev 데이터로 성능 비교하기\n",
    "    # Early Stopping 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3158647-85c5-4e88-ae42-6ecd4fddd80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each_len_list [3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-4b4dfd74a24c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#         loss = CELoss(data['input'].view(-1, output.size(-1)), data['label'].view(-1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\github\\JBNU-2022-SPRING\\English world class tagging & Korean_Named Entity Recognition\\Korean_Named Entity Recognition\\ner_collate_fn.py\u001b[0m in \u001b[0;36mcollate_fn_custom\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mpadded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     return {'input': padded_inputs.contiguous(),\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "for epoch in range(5): # 숫자 ->train_epoch\n",
    "    model.train()\n",
    "\n",
    "    for data in dataloader_train:\n",
    "        print(data['input'], data['label'])\n",
    "#         loss = CELoss(data['input'].view(-1, output.size(-1)), data['label'].view(-1))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (iteration + 1) % 10 == 0:\n",
    "#             print(f'{iteration:3} - loss: {loss.item()}')\n",
    "\n",
    "    # todo 매 에포크가 끝나면 dev 데이터로 성능 비교하기\n",
    "    # Early Stopping 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08135a6-7f5d-40c0-a85a-9e2bbe8a01ec",
   "metadata": {},
   "source": [
    "# Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034bec2-7778-4f15-ab31-f05ad7d0a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # 숫자 ->train_epoch\n",
    "    model.train()\n",
    "\n",
    "    for iteration, batch in enumerate(dataloader_train):\n",
    "        batch_inputs = {k: v.cuda(device) for k, v in list(batch[0].items())}\n",
    "        batch_labels = batch[1].cuda(device)\n",
    "\n",
    "        output = model(**batch_inputs)\n",
    "        loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            print(f'{iteration:3} - loss: {loss.item()}')\n",
    "\n",
    "    # todo 매 에포크가 끝나면 dev 데이터로 성능 비교하기\n",
    "    # Early Stopping 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f800c20-2fa6-4426-bf59-39fcd746bcb3",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5fd54-23d8-453a-87b0-abc73950b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "gold_list = []\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, batch in enumerate(dataloader_test):\n",
    "        batch_inputs = {k: v.cuda(device) for k, v in list(batch[0].items())}\n",
    "        batch_labels = batch[1].cuda(device)\n",
    "        \n",
    "        output = model(**batch_inputs)\n",
    "        loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "        \n",
    "        print('loss:', loss.item())\n",
    "        pred_ids = torch.argmax(output, dim=-1)\n",
    "        \n",
    "        for g, p in zip(batch_labels, pred_ids):\n",
    "            gold_mask = g != tag_converter.pad_id\n",
    "            \n",
    "            gold = tag_converter.convert_ids_to_tags(g[gold_mask].tolist())\n",
    "            pred = tag_converter.convert_ids_to_tags(p[gold_mask].tolist())\n",
    "            gold_list.append(gold)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "            print(gold)\n",
    "            print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0aae5-65dd-4eb4-be1a-15b467d07d1c",
   "metadata": {},
   "source": [
    "# Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6662df4d-7fb9-40bf-be33-5ed5b45952e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-e6ef921686ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgold_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpred_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1658\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m         \"\"\"\n\u001b[1;32m-> 1660\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "gold_list = []\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, batch in enumerate(dataloader_test):\n",
    "        batch_inputs = {k: v for k, v in list(batch[0].items())}\n",
    "        batch_labels = batch[1]\n",
    "        \n",
    "        output = model(**batch_inputs)\n",
    "        loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "        \n",
    "        print('loss:', loss.item())\n",
    "        pred_ids = torch.argmax(output, dim=-1)\n",
    "        \n",
    "        for g, p in zip(batch_labels, pred_ids):\n",
    "            gold_mask = g != tag_converter.pad_id\n",
    "            \n",
    "            gold = tag_converter.convert_ids_to_tags(g[gold_mask].tolist())\n",
    "            pred = tag_converter.convert_ids_to_tags(p[gold_mask].tolist())\n",
    "            gold_list.append(gold)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "            print(gold)\n",
    "            print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e813451-0100-4f9b-b49e-fc2bba9226fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list_flat = []\n",
    "pred_list_flat = []\n",
    "for g, p in zip(gold_list, pred_list):\n",
    "    gold_list_flat += g\n",
    "    pred_list_flat += p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedac99-b5f2-416e-9cb0-1cc98f132a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold_list_flat, pred_list_flat, digits=5, labels=list(tag_converter.tag_to_id.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ee831-e9bc-455b-9903-75d1eddc9c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562b268-317e-4f85-a475-51b4c846de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tag_name):\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d5aad-a27e-4ee3-8434-d2f8a6e9caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq):\n",
    "    default = \"O\"\n",
    "\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b792c1-846f-478b-99cb-46453328357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ner_F1(total_answers, total_preds):\n",
    "    num_match = num_preds = num_answers = 0\n",
    "\n",
    "    for answers, preds in zip(total_answers, total_preds):\n",
    "\n",
    "        answer_seg_result = set(get_chunks(answers))\n",
    "        pred_seg_result = set(get_chunks(preds))\n",
    "\n",
    "        num_match += len(answer_seg_result & pred_seg_result)\n",
    "        num_answers += len(answer_seg_result)\n",
    "        num_preds += len(pred_seg_result)\n",
    "\n",
    "    precision = 100.0 * num_match / num_preds\n",
    "    recall = 100.0 * num_match / num_answers\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24c8e9-95b7-4b35-8f63-5859b2dd8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_ner_F1(gold_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ace50-8cc1-48d3-91f2-a0f47fdc96f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
