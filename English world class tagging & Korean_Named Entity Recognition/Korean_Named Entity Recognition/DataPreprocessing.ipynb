{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2075fee-4fe2-485e-aafd-04f84cea7c09",
   "metadata": {},
   "source": [
    "Inference\\\n",
    "https://wikidocs.net/115055\n",
    "\n",
    "https://huggingface.co/docs/transformers/index\n",
    "\n",
    "http://kkma.snu.ac.kr/documents/index.jsp?doc=postag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05f6e2-28e5-4dc3-9cf7-5df657eca8ad",
   "metadata": {},
   "source": [
    "# Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    "# Date; 2. 10. 2022 - 2. 22. 2022\n",
    "# Title: Korean_NER\n",
    "# Professor: Seung-Hoon Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe1e2a7-9c70-4ba1-ae0d-351b644b2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "# from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b31a86-3726-4ff4-952a-241dc6a41593",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_dir = 'C:\\\\Users\\\\LG\\\\Desktop\\\\github\\\\JBNU-2022-SPRING\\\\English world class tagging & Korean_Named Entity Recognition\\\\Ko_En_NER_POStag_data\\Ko_NER_POS'\n",
    "file_name_ko_train = 'train.txt'\n",
    "file_name_ko_test = 'test.txt'\n",
    "file_name_ko_dev = 'dev.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87b4df-03e1-41d4-ac69-b6dd9f693868",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09958c3f-4435-405c-bd9e-2952a750a91d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data_prepro:\n",
    "    \n",
    "    def make_json(file_name):\n",
    "        PATH_data = os.path.join(PATH_dir, file_name)\n",
    "        prepro_file_name = 'prepro_' + file_name.rsplit('.')[0] + '.json' #change file extention from '.txt' to '.json'\n",
    "        PATH_preprodata = os.path.join(PATH_dir, prepro_file_name)\n",
    "        \n",
    "    def read_file(path):\n",
    "        with open(PATH_data, 'r', encoding='UTF8') as f:\n",
    "            rawdata = f.readlines()\n",
    "            \n",
    "        return rawdata\n",
    "        \n",
    "    def Data_NER(path):\n",
    "        \n",
    "        # data preprocessing by using pandas\n",
    "        df0 = pd.DataFrame(rawdata)\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "        \n",
    "        PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "        \n",
    "        # make dataset & tokenizer_tagging [original, token, tokenizer_tagging]\n",
    "        lst=[]\n",
    "        lst_set = []\n",
    "        lst_ent = []\n",
    "        lst_tag = []\n",
    "        for i in range(6945):\n",
    "            if df_final['tag'][i] != None:\n",
    "                morph_to_tokens = tokenizer.tokenize(df_final['Entity'][i])\n",
    "                if len(df_final['Entity'][i]) == len(morph_to_tokens[0]):\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                else:\n",
    "                    if df_final['tag'][i] == 'O':\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                    else:\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        split = df_final['tag'][i].split('-')\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            results = 'I-' + split[-1]\n",
    "                            lst_tag.append(results)\n",
    "\n",
    "            else:\n",
    "                lst_ent = result = ' '.join(str(s) for s in lst_ent)\n",
    "                lst_set.append(lst_ent)\n",
    "                morph_to_tokens = tokenizer.tokenize(lst_ent)\n",
    "                lst_set.append(morph_to_tokens)\n",
    "                lst_set.append(lst_tag)\n",
    "                lst.append(lst_set)\n",
    "                result = 0\n",
    "                lst_ent = []\n",
    "                lst_tok = []\n",
    "                lst_tag = []\n",
    "                lst_set = []\n",
    "            \n",
    "        return lst\n",
    "    \n",
    "    \n",
    "    def rewrite_json():\n",
    "        with open(PATH_preprodata, 'w') as f:\n",
    "            json.dump(lst, f)\n",
    "            \n",
    "        with open(PATH_preprodata, 'r') as f:\n",
    "            preprodata = json.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313e428-0320-4015-b4ea-5cd7e5761fa4",
   "metadata": {},
   "source": [
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0205255f-5b8f-4b02-8110-08b73d7f5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(file_name):\n",
    "    global df_final\n",
    "    PATH_data = os.path.join(PATH_dir, file_name)\n",
    "\n",
    "    #change file extention from '.txt' to '.json'\n",
    "    prepro_file_name = 'prepro_' + file_name.rsplit('.')[0] + '.json'\n",
    "    PATH_preprodata = os.path.join(PATH_dir, prepro_file_name)\n",
    "\n",
    "    with open(PATH_data, 'r', encoding='UTF8') as f:\n",
    "        rawdata = f.readlines()\n",
    "\n",
    "    # data preprocessing by using pandas\n",
    "    df0 = pd.DataFrame(rawdata)\n",
    "    if file_name == 'dev.txt':\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "    else:\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2','1-3'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "   \n",
    "    # upload tokenizer\n",
    "    PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "\n",
    "    # make dataset & tokenizer_tagging [original, token, tokenizer_tagging]\n",
    "    lst=[]\n",
    "    lst_set = []\n",
    "    lst_ent = []\n",
    "    lst_tag = []\n",
    "    for i in range(len(df_final)):\n",
    "        if df_final['tag'][i] != None:\n",
    "            morph_to_tokens = tokenizer.tokenize(df_final['Entity'][i])\n",
    "            if 1 == len(morph_to_tokens):\n",
    "                lst_ent.append(df_final['Entity'][i])\n",
    "                lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "            else:\n",
    "                if df_final['tag'][i] == 'O':\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "                    for i in range(len(morph_to_tokens)-1):\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                else:\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "                    split = df_final['tag'][i].split('-')\n",
    "                    for i in range(len(morph_to_tokens)-1):\n",
    "                        results = 'I-' + split[-1]\n",
    "                        lst_tag.append(results)\n",
    "\n",
    "        else:\n",
    "            lst_ent = result = ' '.join(str(s) for s in lst_ent)\n",
    "            lst_set.append(lst_ent)\n",
    "            morph_to_tokens = tokenizer.tokenize(lst_ent)\n",
    "            lst_set.append(morph_to_tokens)\n",
    "            lst_set.append(lst_tag)\n",
    "            lst.append(lst_set)\n",
    "            result = 0\n",
    "            lst_ent = []\n",
    "            lst_tok = []\n",
    "            lst_tag = []\n",
    "            lst_set = []\n",
    "\n",
    "    with open(PATH_preprodata, 'w') as f:\n",
    "        json.dump(lst, f)\n",
    "\n",
    "    with open(PATH_preprodata, 'r') as f:\n",
    "        preprodata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f142753-7db9-4c30-bd8c-4920c815563e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepro = Data_prepro()\n",
    "\n",
    "train_file = prepro.make_json(file_name_ko_train)\n",
    "test_file = prepro.make_json(file_name_ko_test)\n",
    "dev_file = prepro.make_json(file_name_ko_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb0013-35f5-4b1c-b550-5e029e6f48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_ko_train = 'train.txt'\n",
    "file_name_ko_test = 'test.txt'\n",
    "#file_name_ko_dev = 'dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0484cc4-45db-47d1-bb91-9bbeefe2461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_json(file_name_ko_train)\n",
    "make_json(file_name_ko_test)\n",
    "make_json(file_name_ko_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579ca4a-c538-4afe-bda0-abdb389c0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_json(file_name_ko_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6977e915-2c7f-4721-9e7b-e7e02a7dbb98",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_final' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e39958c076e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmake_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name_ko_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-7181b184ab3f>\u001b[0m in \u001b[0;36mmake_json\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mlst_ent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mlst_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mmorph_to_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Entity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'df_final' referenced before assignment"
     ]
    }
   ],
   "source": [
    "make_json(file_name_ko_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fed6c-cf3f-4aa1-8ff7-36f98000ae4e",
   "metadata": {},
   "source": [
    "# My Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "72b71342-b6db-477b-844a-9a7a1f38b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ' '.join(str(s) for s in lst[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ab3bcb9-8e35-482b-94b9-1f51b507d1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 85])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   101,  10386,   9367,   9246,  10739, 119327,   9792,  73352,   9637,\n",
       "           9783,  75855,   9909,  12605,   9233,   9580,  37819,  22028,   9776,\n",
       "          14867,  24178,  64749, 118965,   9632,   9319,  12508,  17138,   9632,\n",
       "           9666, 118943,   9954,   8857,  12030,  12310,   9202,   9309,  27023,\n",
       "          24891,  52015,  30873,   9460,  29455,  15891,   9233,   9672,  55358,\n",
       "           9109,   8892,  25934,  29805,   8853,  35506,   8888,   9670, 119445,\n",
       "           9954,   9909,  12605,   9233,   9010,   9557,   8888,   9213,  25503,\n",
       "           8843,   9638,   9233,   9580,  37819,  51431,   9476,  29805,   9568,\n",
       "          74322,    117,   9484,  14646,   9233,   9612,  34907,  29805,   9091,\n",
       "         118881,   9056,    119,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(result, return_tensors='pt')\n",
    "print(inputs['input_ids'].size())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c6c45af-b963-4b99-a528-0c307a42237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d1dd2b6-33c5-4b52-9eae-f32d1fd9b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "bert = BertModel.from_pretrained(PRETAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "571dc9bd-5007-4934-a48f-d1c57d585197",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0659fdf3-8d30-49ac-9a7f-d8a9c47ead6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "263270be-90fd-4c74-aaa5-791505c90525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1460, -0.3585, -0.1289,  ..., -0.0559, -0.0949, -0.0022],\n",
       "         [ 0.4797, -0.2805, -0.3411,  ...,  0.5483, -0.2645,  0.1313],\n",
       "         [ 0.2723, -0.2698,  0.2306,  ...,  0.1069,  0.2373, -0.5147],\n",
       "         ...,\n",
       "         [-0.4482, -0.5314, -0.1884,  ..., -0.4645,  0.2049, -0.6246],\n",
       "         [-0.1095, -0.4616, -0.7723,  ..., -0.0311,  0.1242, -0.2750],\n",
       "         [ 0.2146, -0.3461, -0.3170,  ..., -0.1243,  0.1938, -0.1037]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 1.4693e-01, -2.6310e-01,  2.1919e-01, -3.1868e-01, -3.4607e-01,\n",
       "          3.1139e-02,  1.9270e-01, -3.6483e-02, -1.0906e-01, -1.0840e-01,\n",
       "         -1.2835e-01,  1.8402e-02, -8.5716e-02,  3.7373e-02,  1.9826e-01,\n",
       "          1.7237e-01, -1.5535e-01,  1.9489e-01,  1.3541e-01,  6.7832e-02,\n",
       "         -9.8824e-01, -2.3051e-01,  1.3823e-01, -1.9326e-01,  3.8057e-02,\n",
       "          2.1562e-01, -3.1980e-01,  1.1865e-01, -7.8593e-03, -3.1718e-01,\n",
       "         -1.2041e-01, -9.9151e-01,  7.0460e-01,  5.8103e-02,  1.5321e-02,\n",
       "          9.2042e-02, -8.8989e-02,  5.3778e-03,  3.9284e-02, -4.2725e-01,\n",
       "         -1.5986e-01,  2.2240e-01, -2.4434e-01, -2.8918e-02,  1.9769e-01,\n",
       "         -9.4554e-02, -8.6381e-02,  2.5499e-01, -1.9613e-01,  1.0308e-01,\n",
       "         -4.0578e-04, -4.3045e-02,  1.4664e-01, -9.8350e-02,  5.0332e-02,\n",
       "         -9.1949e-03, -6.7344e-02,  3.3386e-02,  1.6006e-02, -2.2808e-01,\n",
       "         -7.2080e-02,  1.3213e-01, -1.3966e-01, -1.0865e-01, -1.4206e-01,\n",
       "         -1.5365e-01,  1.0707e-01, -3.1599e-02, -2.5939e-01, -2.2115e-01,\n",
       "         -1.3523e-01,  1.2763e-01, -1.5096e-01, -1.1791e-01,  2.4059e-01,\n",
       "         -1.6526e-02,  2.0536e-01, -9.6995e-03,  4.6029e-02, -1.5866e-01,\n",
       "          9.2289e-02,  1.7697e-01, -2.0533e-01,  9.1443e-02,  4.4853e-03,\n",
       "          2.8385e-01,  2.3279e-03,  2.1889e-02,  1.8160e-01, -1.7581e-01,\n",
       "          3.2597e-01, -1.8326e-01, -1.2887e-01, -1.5653e-01, -2.4280e-01,\n",
       "         -3.6431e-01,  3.7495e-01, -1.5305e-01, -2.1153e-01,  2.4375e-02,\n",
       "         -1.8934e-01,  7.5625e-02,  2.5226e-02, -1.2992e-01, -1.4468e-01,\n",
       "         -2.0606e-01,  1.7244e-01, -2.3711e-02,  6.6078e-02,  1.4506e-01,\n",
       "          3.6078e-02,  2.3050e-01, -2.0197e-01,  1.3149e-02, -6.3825e-03,\n",
       "         -2.4240e-01, -4.8637e-01,  2.1568e-01,  6.0065e-02, -6.8607e-02,\n",
       "          1.2390e-01,  9.9231e-01,  1.0583e-01, -3.0834e-01,  6.8539e-02,\n",
       "          3.3081e-01, -4.9234e-02,  2.0729e-01,  1.1457e-01, -6.1900e-02,\n",
       "          1.5391e-02,  2.1697e-02, -2.8995e-02,  1.1277e-01,  2.0190e-01,\n",
       "         -5.5545e-02, -3.4264e-01,  3.1838e-01,  8.4347e-02,  3.8805e-02,\n",
       "          1.1332e-01, -1.0354e-01,  1.4298e-01, -1.6942e-01, -1.5901e-01,\n",
       "         -1.8267e-01,  3.4208e-01,  2.0685e-01,  9.8893e-01, -2.0196e-01,\n",
       "         -1.7905e-01,  1.8409e-01,  6.5179e-01,  2.0019e-01, -1.1414e-01,\n",
       "         -2.0307e-01, -1.5110e-01,  2.0425e-01,  3.4023e-01,  9.2527e-02,\n",
       "          1.2283e-01, -1.3888e-01, -7.5849e-02,  1.9802e-02,  1.9577e-03,\n",
       "          3.2889e-02,  1.0331e-01,  3.6561e-01, -2.0905e-01,  2.4278e-01,\n",
       "         -1.5572e-01, -2.2034e-02,  2.4772e-01, -2.0630e-01, -1.1454e-01,\n",
       "          1.2847e-01, -2.8891e-03, -2.6328e-01, -1.9071e-01,  3.2436e-02,\n",
       "          1.3861e-01, -2.2351e-01, -2.2916e-01,  1.3819e-01, -2.5233e-01,\n",
       "         -1.0238e-01,  2.8182e-02, -2.7657e-01, -1.5947e-01,  1.3419e-01,\n",
       "         -1.1584e-01, -1.2569e-01, -1.3627e-01,  2.0020e-01,  3.3605e-01,\n",
       "          3.2599e-01, -1.2194e-01, -1.5394e-02,  4.3718e-02,  1.6780e-01,\n",
       "          1.4121e-01, -6.0054e-03,  1.7816e-01,  1.3975e-01, -3.5615e-01,\n",
       "         -6.9233e-01,  1.7165e-01,  6.0510e-02,  3.4252e-02, -1.2569e-01,\n",
       "          1.3773e-01, -7.0119e-02, -3.6538e-01, -3.2887e-02, -4.2393e-02,\n",
       "          1.2380e-01,  2.7666e-01, -2.3416e-01,  2.6082e-02,  1.1320e-01,\n",
       "          1.4168e-02, -3.9665e-01,  4.9203e-02, -3.0531e-01, -8.1944e-02,\n",
       "          6.5007e-03,  2.6433e-01,  1.6212e-01,  2.5322e-01, -1.3867e-01,\n",
       "         -1.6835e-01, -1.2676e-01,  3.2757e-01,  7.7982e-02,  9.3334e-02,\n",
       "         -2.4770e-01,  8.4753e-02,  8.5025e-02,  1.3041e-01, -2.2299e-01,\n",
       "         -7.9884e-02,  6.9304e-02,  5.7598e-02, -4.3258e-01,  2.1452e-01,\n",
       "         -1.2560e-01,  2.9161e-01,  2.5885e-01, -1.0096e-02,  5.0543e-02,\n",
       "          9.7027e-02,  2.5853e-01,  7.5184e-01,  1.0131e-01,  1.1817e-01,\n",
       "         -9.8672e-01,  1.7666e-01,  1.8912e-01,  2.4781e-02, -2.1166e-01,\n",
       "         -1.3790e-02, -7.4352e-02,  1.4330e-01, -3.3543e-01,  2.9322e-01,\n",
       "         -5.0027e-02, -4.1411e-01, -1.0051e-01,  6.7990e-02, -2.1575e-01,\n",
       "         -1.6461e-01, -3.3135e-01, -1.7748e-01, -2.2912e-01,  6.0106e-02,\n",
       "          1.5314e-01, -3.5391e-02, -7.6115e-02, -3.9231e-01,  2.6883e-02,\n",
       "         -3.1541e-02, -1.2467e-01,  2.0475e-01, -9.8893e-01, -8.9809e-02,\n",
       "         -4.7079e-02, -2.1094e-02,  9.1940e-02,  1.9937e-01,  3.7278e-02,\n",
       "          4.5852e-01, -4.6183e-02,  1.1048e-01, -1.4327e-01, -2.4708e-01,\n",
       "          3.5764e-02,  4.1349e-02,  2.1332e-01,  7.8826e-03, -1.2790e-02,\n",
       "          1.1089e-01, -2.7156e-01,  2.2886e-01, -1.6282e-01,  3.8501e-02,\n",
       "          1.9309e-01, -1.0099e-01,  3.0451e-02,  3.3981e-01,  9.9814e-02,\n",
       "         -1.6793e-03, -1.4246e-01,  3.0146e-01,  2.3181e-01,  3.2465e-01,\n",
       "          2.7855e-02, -1.4165e-01, -2.2235e-01, -1.8820e-01, -2.7089e-02,\n",
       "         -9.0321e-02,  4.4588e-03, -2.6895e-01,  9.8943e-01,  1.0826e-01,\n",
       "         -4.0240e-01, -4.9965e-01, -8.6362e-02,  6.4600e-01, -2.3443e-01,\n",
       "         -8.3883e-01, -3.6995e-02, -3.1009e-01,  1.6604e-02,  2.7262e-01,\n",
       "          1.1750e-01,  1.9552e-01,  3.6144e-02,  1.4971e-02, -2.4862e-01,\n",
       "         -6.8791e-04,  3.1402e-02,  2.8254e-01, -3.0966e-01, -1.3256e-01,\n",
       "          2.3270e-01,  6.8089e-02,  2.0109e-01, -7.9060e-01, -2.8037e-01,\n",
       "          1.6240e-01,  2.6957e-01,  1.3857e-01,  1.5560e-01, -9.9707e-02,\n",
       "         -1.5876e-01,  1.3274e-01, -3.6211e-01, -4.7731e-02, -1.3591e-01,\n",
       "         -2.7612e-01,  7.5003e-02, -1.1832e-01,  3.2880e-01,  3.0323e-01,\n",
       "          2.5450e-01,  6.3310e-02, -1.1880e-01, -2.0417e-01, -1.7507e-01,\n",
       "          3.2257e-01, -9.9131e-01,  1.9731e-01,  2.0358e-01, -2.2101e-02,\n",
       "          2.0526e-01,  6.6916e-02,  1.2792e-02,  1.0579e-01,  6.2567e-02,\n",
       "         -3.0648e-02, -6.4193e-02, -3.5082e-02,  2.9878e-01, -1.3063e-01,\n",
       "          2.4304e-01, -4.0369e-01, -2.3821e-01,  3.4338e-01, -2.1368e-01,\n",
       "          2.4766e-01, -1.1831e-01, -2.8665e-01, -1.1482e-02,  8.8957e-02,\n",
       "         -3.6549e-02, -9.8632e-02,  1.9133e-01,  7.9474e-02, -2.4819e-01,\n",
       "          1.8659e-01, -7.1151e-02, -7.5275e-02,  1.1671e-01, -9.1182e-02,\n",
       "         -6.0430e-02, -3.2654e-01, -1.8344e-01,  1.9715e-01,  1.6995e-01,\n",
       "          2.0425e-01,  7.8690e-02, -1.1674e-01, -1.7233e-01, -1.2206e-02,\n",
       "         -7.0825e-02, -7.7796e-02, -2.8736e-01, -2.1435e-01, -2.1108e-01,\n",
       "         -2.3089e-01,  9.9105e-01,  2.0326e-01,  4.5435e-01, -1.5010e-01,\n",
       "         -1.6414e-01,  1.5419e-01,  1.1146e-02,  2.0237e-01,  2.4927e-01,\n",
       "          5.9066e-02,  3.1640e-02,  2.2357e-01,  2.9825e-01,  5.8030e-02,\n",
       "         -1.6181e-02,  3.5954e-01,  1.9172e-01, -7.7489e-02, -3.0215e-01,\n",
       "         -1.6722e-01, -4.0673e-02,  4.2007e-01, -2.8431e-02,  1.1044e-01,\n",
       "          1.4663e-01, -6.5361e-01,  1.0318e-01, -5.2405e-02,  1.1557e-01,\n",
       "         -1.6369e-01,  2.9291e-01, -5.4168e-02,  6.7312e-02, -1.4204e-01,\n",
       "          6.2252e-02,  9.8897e-01, -1.2238e-01,  1.5489e-01,  2.0937e-01,\n",
       "          2.2082e-01, -1.1154e-01, -3.1513e-02, -2.0587e-01, -2.4609e-02,\n",
       "         -8.9123e-02,  2.4312e-01,  4.8532e-01,  1.7681e-01,  2.4853e-01,\n",
       "         -2.3532e-02, -2.3883e-01, -4.4603e-02,  9.2118e-02,  2.2114e-01,\n",
       "         -1.9008e-01, -2.0059e-01, -3.1436e-01,  1.8539e-01, -2.5959e-02,\n",
       "          1.5445e-01, -1.5384e-01,  2.3519e-01, -1.8109e-01,  2.5812e-01,\n",
       "         -8.4526e-02,  1.6225e-01, -3.4690e-01,  6.8516e-02, -1.9736e-01,\n",
       "         -3.9998e-02, -2.1478e-01,  3.4895e-01,  1.6045e-01,  9.8811e-01,\n",
       "         -2.0725e-01,  3.9897e-01, -1.7009e-01,  7.9227e-02, -2.6912e-01,\n",
       "          1.7889e-01, -2.3940e-01, -1.6147e-02,  2.0038e-01,  2.3444e-01,\n",
       "          7.1832e-02,  1.5625e-01, -1.3383e-01, -8.9325e-01, -9.2578e-02,\n",
       "         -1.9623e-01,  5.0905e-02,  1.9796e-01,  3.5402e-01, -2.3627e-01,\n",
       "          2.8191e-01, -1.2433e-01, -5.3557e-02, -5.6392e-01,  1.8786e-01,\n",
       "         -8.2765e-02,  2.0443e-01,  1.3376e-01, -2.3400e-01, -2.0104e-01,\n",
       "          9.8678e-01,  9.8916e-01, -1.2101e-01,  2.3180e-01,  2.3292e-01,\n",
       "          3.6602e-02, -2.8833e-01, -1.0327e-01, -1.1931e-02,  1.0516e-01,\n",
       "         -1.2729e-02,  1.6951e-01,  3.3186e-01, -4.3002e-02,  2.1089e-02,\n",
       "          5.7910e-02,  2.3547e-01,  1.3753e-01, -1.3606e-01, -1.1691e-01,\n",
       "         -4.0535e-02,  9.6824e-02, -1.6943e-01,  2.8301e-01,  1.6287e-01,\n",
       "         -1.0502e-01, -4.4864e-02, -1.1270e-01, -6.6254e-02, -1.6719e-01,\n",
       "         -1.0672e-01,  2.4516e-01, -9.8709e-01, -2.3033e-02, -2.1856e-02,\n",
       "         -2.4765e-01,  1.1014e-01,  1.2848e-01, -4.6300e-02,  2.4913e-02,\n",
       "         -1.2778e-01, -1.1579e-01,  5.4101e-02,  1.5377e-01,  1.7742e-01,\n",
       "         -1.7090e-01, -5.7894e-02,  1.7556e-01,  8.0542e-02,  1.2773e-01,\n",
       "         -1.9751e-01,  2.6544e-03,  2.6134e-01, -5.9954e-03, -4.0408e-01,\n",
       "          1.7863e-01, -2.7198e-01,  5.7601e-02,  2.4320e-01, -8.8704e-02,\n",
       "         -1.3188e-01, -3.1270e-01,  1.2554e-01, -7.5370e-02,  1.6012e-01,\n",
       "          2.2332e-01, -1.6813e-02,  5.1097e-02,  5.8383e-03, -1.4023e-01,\n",
       "         -4.9060e-02, -2.9063e-01, -4.3001e-02,  3.1051e-01, -2.3742e-02,\n",
       "          9.1723e-02,  8.2537e-02, -7.5777e-02,  6.1166e-02,  1.5366e-01,\n",
       "          1.5466e-01, -7.7783e-02, -2.1415e-02,  1.3472e-01, -1.0047e-02,\n",
       "         -1.6481e-01,  8.1142e-02,  5.9451e-02, -8.0713e-02,  3.9685e-02,\n",
       "          9.9142e-01,  2.0339e-01,  5.8901e-02, -1.3259e-01,  1.3515e-01,\n",
       "         -1.7358e-02,  2.9211e-01,  2.0813e-02,  1.2474e-01,  9.5117e-01,\n",
       "         -1.2376e-01,  1.9682e-01,  3.1307e-01,  8.4840e-02, -3.9524e-02,\n",
       "          9.2618e-02, -2.9688e-01, -1.0909e-01,  1.8697e-01,  2.6133e-01,\n",
       "          1.1106e-01,  1.6252e-01,  6.8055e-02,  9.7241e-02, -4.6672e-02,\n",
       "          2.5566e-01, -1.0788e-01, -3.7905e-01,  3.6324e-01, -2.7368e-01,\n",
       "         -1.2302e-01, -3.2205e-02, -7.3783e-02, -2.1963e-01, -4.1031e-02,\n",
       "         -2.0337e-01,  9.6397e-02, -8.5073e-02,  1.0123e-01, -3.0856e-02,\n",
       "         -8.0471e-02, -1.7894e-01, -1.1523e-01, -1.6469e-01,  3.3053e-03,\n",
       "          2.5085e-01, -3.3351e-01,  6.7399e-02,  4.1329e-01,  6.0443e-02,\n",
       "          5.4544e-02, -8.1754e-02,  9.9721e-03, -6.0719e-01,  2.9776e-01,\n",
       "          1.6095e-01, -2.7972e-01,  1.4470e-01, -1.6334e-01,  3.4730e-01,\n",
       "          8.7671e-03, -1.2850e-01,  2.2160e-01, -9.8677e-01,  2.1285e-02,\n",
       "          2.3954e-01,  3.8324e-02, -3.9046e-03,  1.5183e-01,  2.0265e-01,\n",
       "         -4.5094e-02,  3.1300e-02, -2.7715e-01, -3.5191e-01, -1.6524e-02,\n",
       "         -3.0011e-01,  2.2918e-01,  2.6415e-01,  8.7374e-02, -1.5577e-01,\n",
       "         -1.4365e-01,  1.4241e-01,  3.3795e-01,  1.2900e-01, -1.7934e-01,\n",
       "          1.8974e-01, -8.1449e-02,  1.5656e-01, -1.8571e-01,  1.2323e-01,\n",
       "         -8.6118e-02,  1.4614e-01,  2.7546e-01, -1.5875e-01,  1.5810e-01,\n",
       "         -1.7877e-01,  3.5793e-01, -2.0746e-01,  1.0751e-01,  2.1655e-01,\n",
       "          2.8745e-02,  2.8212e-02, -1.5966e-01,  5.8851e-02,  1.1080e-01,\n",
       "          2.6540e-01,  4.8638e-01,  9.1907e-02, -2.1725e-01, -3.3214e-01,\n",
       "         -1.6402e-01,  1.4048e-01,  4.9535e-03,  6.9155e-02, -4.6030e-02,\n",
       "          1.8070e-01,  4.5719e-06,  8.8719e-03, -4.5350e-02, -1.1412e-02,\n",
       "         -6.0759e-02, -3.3841e-01,  8.0474e-02, -3.6669e-01, -1.4737e-02,\n",
       "         -8.0037e-02,  9.5055e-02, -9.7771e-02,  1.3927e-01,  4.0155e-01,\n",
       "         -1.7556e-01, -2.3331e-01, -1.9772e-01,  4.0255e-02,  2.2133e-01,\n",
       "          1.7423e-01,  1.6680e-01, -9.8435e-02,  2.6950e-01, -2.2400e-01,\n",
       "         -1.7019e-01,  8.5959e-01,  5.6520e-02, -8.5252e-02,  1.6675e-01,\n",
       "          1.8807e-01,  9.0368e-01,  2.9597e-02, -1.8007e-01,  1.5088e-02,\n",
       "         -2.6295e-01,  2.4349e-01,  4.9724e-02]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
