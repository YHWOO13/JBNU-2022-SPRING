{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2075fee-4fe2-485e-aafd-04f84cea7c09",
   "metadata": {},
   "source": [
    "Inference\\\n",
    "https://wikidocs.net/115055\n",
    "\n",
    "https://huggingface.co/docs/transformers/index\n",
    "\n",
    "http://kkma.snu.ac.kr/documents/index.jsp?doc=postag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05f6e2-28e5-4dc3-9cf7-5df657eca8ad",
   "metadata": {},
   "source": [
    "# Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    "# Date; 2. 10. 2022 - 2. 22. 2022\n",
    "# Title: Korean_NER\n",
    "# Professor: Seung-Hoon Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe1e2a7-9c70-4ba1-ae0d-351b644b2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "# from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b31a86-3726-4ff4-952a-241dc6a41593",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_dir = 'C:\\\\Users\\\\LG\\\\Desktop\\\\github\\\\JBNU-2022-SPRING\\\\English world class tagging & Korean_Named Entity Recognition\\\\Ko_En_NER_POStag_data\\Ko_NER_POS'\n",
    "file_name_ko_train = 'train.txt'\n",
    "file_name_ko_test = 'test.txt'\n",
    "file_name_ko_dev = 'dev.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87b4df-03e1-41d4-ac69-b6dd9f693868",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09958c3f-4435-405c-bd9e-2952a750a91d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data_prepro:\n",
    "    \n",
    "    def make_json(file_name):\n",
    "        PATH_data = os.path.join(PATH_dir, file_name)\n",
    "        prepro_file_name = 'prepro_' + file_name.rsplit('.')[0] + '.json' #change file extention from '.txt' to '.json'\n",
    "        PATH_preprodata = os.path.join(PATH_dir, prepro_file_name)\n",
    "        \n",
    "    def read_file(path):\n",
    "        with open(PATH_data, 'r', encoding='UTF8') as f:\n",
    "            rawdata = f.readlines()\n",
    "            \n",
    "        return rawdata\n",
    "        \n",
    "    def Data_NER(path):\n",
    "        \n",
    "        # data preprocessing by using pandas\n",
    "        df0 = pd.DataFrame(rawdata)\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "        \n",
    "        PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "        \n",
    "        # make dataset & tokenizer_tagging [original, token, tokenizer_tagging]\n",
    "        lst=[]\n",
    "        lst_set = []\n",
    "        lst_ent = []\n",
    "        lst_tag = []\n",
    "        for i in range(6945):\n",
    "            if df_final['tag'][i] != None:\n",
    "                morph_to_tokens = tokenizer.tokenize(df_final['Entity'][i])\n",
    "                if len(df_final['Entity'][i]) == len(morph_to_tokens[0]):\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                else:\n",
    "                    if df_final['tag'][i] == 'O':\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                    else:\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        split = df_final['tag'][i].split('-')\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            results = 'I-' + split[-1]\n",
    "                            lst_tag.append(results)\n",
    "\n",
    "            else:\n",
    "                lst_ent = result = ' '.join(str(s) for s in lst_ent)\n",
    "                lst_set.append(lst_ent)\n",
    "                morph_to_tokens = tokenizer.tokenize(lst_ent)\n",
    "                lst_set.append(morph_to_tokens)\n",
    "                lst_set.append(lst_tag)\n",
    "                lst.append(lst_set)\n",
    "                result = 0\n",
    "                lst_ent = []\n",
    "                lst_tok = []\n",
    "                lst_tag = []\n",
    "                lst_set = []\n",
    "            \n",
    "        return lst\n",
    "    \n",
    "    \n",
    "    def rewrite_json():\n",
    "        with open(PATH_preprodata, 'w') as f:\n",
    "            json.dump(lst, f)\n",
    "            \n",
    "        with open(PATH_preprodata, 'r') as f:\n",
    "            preprodata = json.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313e428-0320-4015-b4ea-5cd7e5761fa4",
   "metadata": {},
   "source": [
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0205255f-5b8f-4b02-8110-08b73d7f5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(file_name):\n",
    "    global df_final\n",
    "    PATH_data = os.path.join(PATH_dir, file_name)\n",
    "\n",
    "    #change file extention from '.txt' to '.json'\n",
    "    prepro_file_name = 'prepro_' + file_name.rsplit('.')[0] + '.json'\n",
    "    PATH_preprodata = os.path.join(PATH_dir, prepro_file_name)\n",
    "\n",
    "    with open(PATH_data, 'r', encoding='UTF8') as f:\n",
    "        rawdata = f.readlines()\n",
    "\n",
    "    # data preprocessing by using pandas\n",
    "    df0 = pd.DataFrame(rawdata)\n",
    "    \n",
    "    if file_name == 'dev.txt':\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "    else:\n",
    "        df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2','1-3'])\n",
    "        df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "        df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "        df_final = pd.DataFrame(df_final)\n",
    "        df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "   \n",
    "    # upload tokenizer\n",
    "    PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "\n",
    "    # make dataset & tokenizer_tagging [original, token, tokenizer_tagging]\n",
    "    lst=[]\n",
    "    lst_set = []\n",
    "    lst_ent = []\n",
    "    lst_tag = []\n",
    "    for i in range(len(df_final)):\n",
    "        if df_final['tag'][i] != None:\n",
    "            morph_to_tokens = tokenizer.tokenize(df_final['Entity'][i])\n",
    "            if 1 == len(morph_to_tokens):\n",
    "                lst_ent.append(df_final['Entity'][i])\n",
    "                lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "            else:\n",
    "                if df_final['tag'][i] == 'O':\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "                    for i in range(len(morph_to_tokens)-1):\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                else:\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "                    split = df_final['tag'][i].split('-')\n",
    "                    for i in range(len(morph_to_tokens)-1):\n",
    "                        results = 'I-' + split[-1]\n",
    "                        lst_tag.append(results)\n",
    "\n",
    "        else:\n",
    "            lst_ent = result = ' '.join(str(s) for s in lst_ent)\n",
    "            lst_set.append(lst_ent)\n",
    "            morph_to_tokens = tokenizer.tokenize(lst_ent)\n",
    "            lst_set.append(morph_to_tokens)\n",
    "            lst_set.append(lst_tag)\n",
    "            lst.append(lst_set)\n",
    "            result = 0\n",
    "            lst_ent = []\n",
    "            lst_tok = []\n",
    "            lst_tag = []\n",
    "            lst_set = []\n",
    "\n",
    "    with open(PATH_preprodata, 'w') as f:\n",
    "        json.dump(lst, f)\n",
    "\n",
    "    with open(PATH_preprodata, 'r') as f:\n",
    "        preprodata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5c864e-8004-4976-ae8c-fd0efb5964c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_NER:\n",
    "    \n",
    "    def make_json(file_name):\n",
    "        global df_final\n",
    "        PATH_data = os.path.join(PATH_dir, file_name)\n",
    "\n",
    "        #change file extention from '.txt' to '.json'\n",
    "        prepro_file_name = 'prepro_' + file_name.rsplit('.')[0] + '.json'\n",
    "        PATH_preprodata = os.path.join(PATH_dir, prepro_file_name)\n",
    "\n",
    "        with open(PATH_data, 'r', encoding='UTF8') as f:\n",
    "            rawdata = f.readlines()\n",
    "\n",
    "        # data preprocessing by using pandas\n",
    "        df0 = pd.DataFrame(rawdata)\n",
    "\n",
    "        if file_name == 'dev.txt':\n",
    "            df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2'])\n",
    "            df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "            df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "            df_final = pd.DataFrame(df_final)\n",
    "            df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "        else:\n",
    "            df1 = pd.DataFrame(df0[0].str.split('/').tolist(),columns=['1-1','1-2','1-3'])\n",
    "            df2 = pd.DataFrame(df0[0].str.split('\\t').tolist(),columns=['1','2','3','4'])\n",
    "            df_final = pd.concat([df1['1-1'],df2['4'].str.strip('\\n')],axis = 1)\n",
    "            df_final = pd.DataFrame(df_final)\n",
    "            df_final = df_final.rename(columns={'1-1':'Entity','4':'tag'})\n",
    "\n",
    "        # upload tokenizer\n",
    "        PRETAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "\n",
    "        # make dataset & tokenizer_tagging [original, token, tokenizer_tagging]\n",
    "        lst=[]\n",
    "        lst_set = []\n",
    "        lst_ent = []\n",
    "        lst_tag = []\n",
    "        for i in range(len(df_final)):\n",
    "            if df_final['tag'][i] != None:\n",
    "                morph_to_tokens = tokenizer.tokenize(df_final['Entity'][i])\n",
    "                if 1 == len(morph_to_tokens):\n",
    "                    lst_ent.append(df_final['Entity'][i])\n",
    "                    lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                else:\n",
    "                    if df_final['tag'][i] == 'O':\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            lst_tag.append(df_final['tag'][i])\n",
    "\n",
    "                    else:\n",
    "                        lst_ent.append(df_final['Entity'][i])\n",
    "                        lst_tag.append(df_final['tag'][i])\n",
    "                        split = df_final['tag'][i].split('-')\n",
    "                        for i in range(len(morph_to_tokens)-1):\n",
    "                            results = 'I-' + split[-1]\n",
    "                            lst_tag.append(results)\n",
    "\n",
    "            else:\n",
    "                lst_ent = result = ' '.join(str(s) for s in lst_ent)\n",
    "                lst_set.append(lst_ent)\n",
    "                morph_to_tokens = tokenizer.tokenize(lst_ent)\n",
    "                lst_set.append(morph_to_tokens)\n",
    "                lst_set.append(lst_tag)\n",
    "                lst.append(lst_set)\n",
    "                result = 0\n",
    "                lst_ent = []\n",
    "                lst_tok = []\n",
    "                lst_tag = []\n",
    "                lst_set = []\n",
    "\n",
    "        with open(PATH_preprodata, 'w') as f:\n",
    "            json.dump(lst, f)\n",
    "\n",
    "        with open(PATH_preprodata, 'r') as f:\n",
    "            preprodata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0484cc4-45db-47d1-bb91-9bbeefe2461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_json(file_name_ko_train)\n",
    "make_json(file_name_ko_test)\n",
    "make_json(file_name_ko_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
